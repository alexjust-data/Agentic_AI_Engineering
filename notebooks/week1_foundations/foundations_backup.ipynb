{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced AI Agents Foundations Laboratory\n",
    "\n",
    "## Introduction to Agentic AI Theory and Practice\n",
    "\n",
    "This notebook demonstrates comprehensive AI agent capabilities through four progressive laboratories, integrating theoretical concepts with practical implementations. We'll explore the **5 fundamental Workflow Patterns** and understand how they form the building blocks of agentic systems.\n",
    "\n",
    "**Core Learning Objectives:**\n",
    "- Master the 5 fundamental Workflow Patterns through practical implementation\n",
    "- Differentiate between Workflows (predefined) and Agents (dynamic)\n",
    "- Multi-model architecture implementation and comparison\n",
    "- Automatic response evaluation with structured validation\n",
    "- Tool integration patterns and real-world deployment\n",
    "\n",
    "---\n",
    "\n",
    "## What is an AI Agent?\n",
    "\n",
    "According to Hugging Face's definition:\n",
    "> \"AI agents are programs where LLM outputs control the workflow\"\n",
    "\n",
    "This means the output of a language model determines which tasks are executed and in what order.\n",
    "\n",
    "**Hallmarks of Agentic AI:**\n",
    "1. **Multiple LLM calls** - Like our multi-model comparison system\n",
    "2. **Tool use** - LLMs executing external functions (time, weather)\n",
    "3. **LLM communication** - Models passing information between each other\n",
    "4. **Planning** - An LLM acting as a planner to coordinate tasks\n",
    "5. **Autonomy** - The system has freedom to choose how to proceed\n",
    "\n",
    "**Autonomy** is often seen as the key element - when a model chooses how to respond or which path to take, that reflects autonomy.\n",
    "\n",
    "---\n",
    "\n",
    "## Anthropic's Framework: Workflows vs Agents\n",
    "\n",
    "Anthropic categorizes agentic systems into two types:\n",
    "\n",
    "### **Workflows (Predefined Orchestration):**\n",
    "- Structured, predictable execution paths\n",
    "- Defined sequences of model and tool interactions\n",
    "- Clear guardrails and control mechanisms\n",
    "- **Our Labs 1-4 demonstrate these patterns**\n",
    "\n",
    "### **Agents (Dynamic Control):**\n",
    "- Models dynamically control tools and task flow\n",
    "- Open-ended, iterative loops with feedback\n",
    "- Less predictable but more powerful\n",
    "- Will be explored in future weeks\n",
    "\n",
    "---\n",
    "\n",
    "## The 5 Fundamental Workflow Patterns\n",
    "\n",
    "### **1. Prompt Chaining**\n",
    "**Concept:** Chain a sequence of LLMs, each doing a subtask based on the previous output.\n",
    "- **Example:** LLM1 suggests business sector → LLM2 identifies pain point → LLM3 recommends solution\n",
    "- **Our Implementation:** Lab 1 demonstrates basic sequential calls\n",
    "\n",
    "### **2. Routing**\n",
    "**Concept:** An LLM router decides which specialized model should handle a task.\n",
    "- **Example:** Router evaluates input → sends to specialized LLM1, LLM2, or LLM3\n",
    "- **Our Implementation:** Model selection logic based on task requirements\n",
    "\n",
    "### **3. Parallelization**\n",
    "**Concept:** Break down task into parallel subtasks sent to multiple LLMs simultaneously.\n",
    "- **Example:** Same question sent to multiple models → results aggregated\n",
    "- **Our Implementation:** Lab 2 multi-model comparison system\n",
    "\n",
    "### **4. Orchestrator-Worker**\n",
    "**Concept:** An LLM orchestrator decomposes tasks and coordinates multiple worker LLMs.\n",
    "- **Example:** Orchestrator LLM plans → Worker LLMs execute → Orchestrator combines results\n",
    "- **Our Implementation:** Comparative analysis system with intelligent coordination\n",
    "\n",
    "### **5. Evaluator-Optimizer (Validation Loop)**\n",
    "**Concept:** Generator LLM proposes solution → Evaluator LLM reviews → Loop until acceptable.\n",
    "- **Example:** Generator creates response → Evaluator scores → Retry if needed\n",
    "- **Our Implementation:** Labs 2-4 all demonstrate this critical pattern\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow Patterns Comparison\n",
    "\n",
    "| Pattern | Decision Maker | Autonomy Level | Key Benefit | Lab Implementation |\n",
    "|---------|-------------|-------------|-------------|------------------|\n",
    "| **Prompt Chaining** | Predefined sequence | Low-Medium | Modular logic | Lab 1: Sequential calls |\n",
    "| **Routing** | Router LLM | Medium | Specialization | Model selection logic |\n",
    "| **Parallelization** | Code logic | Low | Speed, redundancy | Lab 2: Multi-model comparison |\n",
    "| **Orchestrator-Worker** | Orchestrator LLM | Medium-High | Dynamic coordination | Comparative analysis |\n",
    "| **Evaluator-Optimizer** | Evaluator LLM | Medium | Quality control | Labs 2-4: Validation loops |\n",
    "\n",
    "---\n",
    "\n",
    "## Laboratory Progression\n",
    "\n",
    "**Lab 1: Prompt Chaining Fundamentals**\n",
    "- Simple system + user message interactions\n",
    "- **Pattern:** Basic Prompt Chaining\n",
    "- **Learning:** Sequential LLM processing\n",
    "\n",
    "**Lab 2: Parallelization + Evaluation**  \n",
    "- Cross-provider model comparison\n",
    "- **Patterns:** Parallelization + Evaluator-Optimizer\n",
    "- **Learning:** Concurrent processing with quality control\n",
    "\n",
    "**Lab 3: Tool Integration with Validation**\n",
    "- External tool integration (time, document processing)\n",
    "- **Patterns:** Tool Integration + Evaluator-Optimizer\n",
    "- **Learning:** LLM-tool interaction with feedback loops\n",
    "\n",
    "**Lab 4: Orchestrator-Worker Architecture**\n",
    "- Complex argument handling and coordination\n",
    "- **Patterns:** Orchestrator-Worker + Structured Tools\n",
    "- **Learning:** Advanced coordination and real-world integration\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Implementation Features\n",
    "\n",
    "✅ **All 5 Workflow Patterns** demonstrated with working code  \n",
    "✅ **Multi-provider model support** (OpenAI, Anthropic, Google, DeepSeek)  \n",
    "✅ **Pydantic-based evaluation system** (Evaluator-Optimizer pattern)  \n",
    "✅ **Parallel processing capabilities** (Parallelization pattern)  \n",
    "✅ **Intelligent model coordination** (Orchestrator-Worker pattern)  \n",
    "✅ **Advanced tool calling** with argument validation  \n",
    "✅ **Automatic retry mechanisms** with feedback loops  \n",
    "✅ **Web interface** with Gradio integration  \n",
    "✅ **Production-ready monitoring** and guardrails  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding to path: /Users/alex/Desktop/00_projects/AI_agents/my_agents/src\n",
      "OpenAI client initialized\n",
      "Anthropic API key not found\n",
      "Google API key not found\n",
      "DeepSeek API key not found\n",
      "✅ Successfully imported week1_foundations modules\n",
      "Initializing Advanced AI Agent System...\n",
      "Available models: ['gpt-4o-mini', 'gpt-4o', 'gpt-4-turbo']\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup - Import all advanced functionality\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to Python path - go up 2 levels from notebooks/1_foundations to reach src\n",
    "current_dir = os.getcwd()\n",
    "src_path = os.path.join(os.path.dirname(os.path.dirname(current_dir)), 'src')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "print(f\"Adding to path: {src_path}\")\n",
    "\n",
    "try:\n",
    "    from week1_foundations.agent import run_agent, run_agent_with_multiple_models\n",
    "    from week1_foundations.evaluation import (\n",
    "        run_agent_with_evaluation, \n",
    "        run_comparative_analysis, \n",
    "        evaluator\n",
    "    )\n",
    "    from week1_foundations.models import model_manager\n",
    "    print(\"✅ Successfully imported week1_foundations modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(f\"Current directory: {current_dir}\")\n",
    "    print(f\"Python path additions: {src_path}\")\n",
    "    print(\"Please check that you're running from the correct directory\")\n",
    "\n",
    "import json\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize and show available models\n",
    "print(\"Initializing Advanced AI Agent System...\")\n",
    "try:\n",
    "    available_models = model_manager.get_available_models()\n",
    "    print(f\"Available models: {available_models}\")\n",
    "    print(\"Setup complete!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing models: {e}\")\n",
    "\n",
    "# Create helper function for pretty printing\n",
    "def print_result(title, content, color=\"blue\"):\n",
    "    display(HTML(f'<h3 style=\"color:{color};\">{title}</h3>'))\n",
    "    if isinstance(content, dict):\n",
    "        display(Markdown(f\"```json\\n{json.dumps(content, indent=2)}\\n```\"))\n",
    "    else:\n",
    "        display(Markdown(str(content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1: Prompt Chaining Fundamentals\n",
    "\n",
    "**Workflow Pattern:** **Prompt Chaining**\n",
    "\n",
    "**Learning Objective:**\n",
    "Master fundamental LLM interaction patterns through structured prompt design and understand the simplest workflow pattern.\n",
    "\n",
    "**Architecture Flow:**\n",
    "```\n",
    "[User Input] → [System Prompt] → [LLM Processing] → [Response Output]\n",
    "```\n",
    "\n",
    "**Prompt Chaining Explained:**\n",
    "This is the most basic workflow pattern where we:\n",
    "1. **Define a clear system prompt** that establishes the LLM's role\n",
    "2. **Add user input** to create a structured message sequence\n",
    "3. **Process sequentially** through predefined steps\n",
    "4. **Output results** in a controlled manner\n",
    "\n",
    "**Pattern Characteristics:**\n",
    "- **Sequential Processing**: Each step follows the previous in order\n",
    "- **Predefined Flow**: No dynamic decision-making\n",
    "- **Low Autonomy**: Human-defined sequence\n",
    "- **High Control**: Predictable, reliable outputs\n",
    "\n",
    "**Code Implementation Details:**\n",
    "- **Message Structure**: System + User role-based messaging\n",
    "- **Model Selection**: GPT-4o-mini (cost-efficient, fast response)  \n",
    "- **Processing Mode**: Text-only, no external tool integration\n",
    "- **Control Flow**: Direct function call with immediate response\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Content generation pipelines\n",
    "- Document processing workflows\n",
    "- Simple question-answering systems\n",
    "- Template-based responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Basic Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2 + 2 equals 4."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "WITH AUTOMATIC EVALUATION:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2 + 2 equals 4."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Evaluation</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"Score\": \"10/10\",\n",
       "  \"Acceptable\": true,\n",
       "  \"Feedback\": \"The AI response accurately answers the user question with a correct mathematical result. It is concise and directly addresses the inquiry without unnecessary elaboration. The response is appropriate for the context of a general-purpose assistant, providing a straightforward answer to a simple arithmetic question.\",\n",
       "  \"Attempts\": 1\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic single model usage\n",
    "response = run_agent(\"What is 2 + 2?\")\n",
    "print_result(\"Basic Response\", response)\n",
    "\n",
    "# Now with evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WITH AUTOMATIC EVALUATION:\")\n",
    "result_with_eval = run_agent_with_evaluation(\"What is 2 + 2?\")\n",
    "print_result(\"Response\", result_with_eval['response'])\n",
    "print_result(\"Evaluation\", {\n",
    "    \"Score\": f\"{result_with_eval['evaluation'].score}/10\",\n",
    "    \"Acceptable\": result_with_eval['evaluation'].is_acceptable,\n",
    "    \"Feedback\": result_with_eval['evaluation'].feedback,\n",
    "    \"Attempts\": result_with_eval['attempts']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2: Parallelization + Evaluator-Optimizer Patterns\n",
    "\n",
    "**Workflow Patterns:** **Parallelization** + **Evaluator-Optimizer**\n",
    "\n",
    "**Learning Objective:**\n",
    "Implement advanced patterns combining concurrent processing with quality control loops.\n",
    "\n",
    "**Architecture Flow:**\n",
    "```\n",
    "[Query Input] → [Parallel Processing] → [Model1, Model2, Model3...] → [Evaluator] → [Ranked Results]\n",
    "                        ↓\n",
    "                [Validation Loop] → [Accept ✅ | Retry ❌]\n",
    "```\n",
    "\n",
    "**Parallelization Pattern Explained:**\n",
    "This pattern breaks down tasks for concurrent execution:\n",
    "1. **Task Distribution**: Same query sent to multiple models simultaneously\n",
    "2. **Concurrent Execution**: Models process independently\n",
    "3. **Result Aggregation**: Responses collected and compared\n",
    "4. **Efficiency Gain**: Faster than sequential processing\n",
    "\n",
    "**Evaluator-Optimizer Pattern Explained:**\n",
    "This creates quality control through validation loops:\n",
    "1. **Generator Phase**: Models produce responses\n",
    "2. **Evaluation Phase**: Evaluator LLM scores each response\n",
    "3. **Decision Point**: Accept high-quality responses or retry\n",
    "4. **Feedback Loop**: Poor responses trigger regeneration with feedback\n",
    "\n",
    "**Pattern Characteristics:**\n",
    "- **Parallelization Autonomy**: Low (code-controlled distribution)\n",
    "- **Evaluator Autonomy**: Medium (LLM makes quality decisions)\n",
    "- **Key Benefits**: Speed + redundancy + quality control\n",
    "- **Trade-offs**: Higher API costs but better results\n",
    "\n",
    "**Code Implementation Details:**\n",
    "- **Multi-Provider Support**: OpenAI, Anthropic, Google, DeepSeek integration\n",
    "- **Concurrent Processing**: `run_agent_with_multiple_models()` function\n",
    "- **Pydantic Evaluation**: Structured response validation and scoring\n",
    "- **Comparative Analysis**: `run_comparative_analysis()` with intelligent ranking\n",
    "- **Retry Logic**: Automatic regeneration based on evaluation scores\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Content quality assurance systems\n",
    "- Multi-model A/B testing\n",
    "- Consensus-building for critical decisions\n",
    "- Risk mitigation through redundancy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Code Analysis: How Our Implementation Demonstrates the Patterns\n",
    "\n",
    "**Parallelization Pattern in Action:**\n",
    "```python\n",
    "# This function demonstrates Parallelization\n",
    "multi_results = run_agent_with_multiple_models(\"What is the capital of France?\")\n",
    "```\n",
    "\n",
    "**What happens internally:**\n",
    "1. **Task Distribution**: The same question is sent to all available models simultaneously\n",
    "2. **Concurrent Processing**: Each model (gpt-4o-mini, gpt-4o, gpt-4-turbo) processes independently\n",
    "3. **Result Collection**: All responses are gathered into a dictionary structure\n",
    "4. **Aggregation**: Results are formatted for comparison\n",
    "\n",
    "**Evaluator-Optimizer Pattern in Action:**\n",
    "```python\n",
    "# This function demonstrates Evaluator-Optimizer\n",
    "analysis = run_comparative_analysis(\"What is the capital of France?\")\n",
    "```\n",
    "\n",
    "**What happens internally:**\n",
    "1. **Generator Phase**: All models generate responses to the question\n",
    "2. **Evaluation Phase**: An evaluator LLM scores each response (1-10 scale)\n",
    "3. **Comparison Logic**: Responses are ranked based on evaluation scores\n",
    "4. **Decision Making**: Best model is selected based on quality metrics\n",
    "\n",
    "**Key Code Functions Explained:**\n",
    "- `run_agent_with_multiple_models()`: Implements **Parallelization**\n",
    "- `run_comparative_analysis()`: Combines **Parallelization** + **Evaluator-Optimizer**\n",
    "- `evaluator.evaluate_response()`: Core **Evaluator-Optimizer** logic\n",
    "- `evaluator.compare_responses()`: Multi-response ranking system\n",
    "\n",
    "**Autonomy Levels Observed:**\n",
    "- **Parallelization**: Low autonomy (our code controls distribution)\n",
    "- **Evaluation**: Medium autonomy (evaluator LLM makes quality decisions)\n",
    "- **Ranking**: Medium autonomy (comparison LLM determines best model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Single Model Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The capital of France is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MULTI-MODEL COMPARISON:\n",
      "Testing with gpt-4o-mini...\n",
      "Testing with gpt-4o...\n",
      "Testing with gpt-4-turbo...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">GPT-4O Mini (openai)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The capital of France is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">GPT-4O (openai)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The capital of France is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">GPT-4 Turbo (openai)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The capital of France is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "COMPREHENSIVE ANALYSIS WITH EVALUATION:\n",
      "Generating response with gpt-4o-mini...\n",
      "Generating response with gpt-4o...\n",
      "Generating response with gpt-4-turbo...\n",
      "Comparing all responses...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:green;\">Best Model</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "gpt-4o-mini"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Model Ranking</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "['gpt-4o-mini', 'gpt-4o', 'gpt-4-turbo']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Reasoning</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "All models provided the correct answer, stating that the capital of France is Paris. However, the responses are identical in content and clarity, which makes it challenging to differentiate based on accuracy or helpfulness. The slight edge for gpt-4o-mini is due to its concise format, which can be perceived as slightly more user-friendly. Nevertheless, all models performed exceptionally well, leading to minor distinctions in ranking primarily based on presentation. Since the content quality is equal, the ranking reflects a subjective preference rather than significant differences in performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Model Scores:</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model  Score\n",
       "0  gpt-4o-mini     10\n",
       "1       gpt-4o     10\n",
       "2  gpt-4-turbo     10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Single model response\n",
    "response = run_agent(\"What is the capital of France?\")\n",
    "print_result(\"Single Model Response\", response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-MODEL COMPARISON:\")\n",
    "\n",
    "# Multiple models (will use only available ones)\n",
    "multi_results = run_agent_with_multiple_models(\"What is the capital of France?\")\n",
    "\n",
    "for model_name, result in multi_results.items():\n",
    "    print_result(f\"{result['model_display']} ({result['provider']})\", result['response'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPREHENSIVE ANALYSIS WITH EVALUATION:\")\n",
    "\n",
    "# Full comparative analysis with evaluation\n",
    "analysis = run_comparative_analysis(\"What is the capital of France?\")\n",
    "\n",
    "print_result(\"Best Model\", analysis['comparison'].best_model, \"green\")\n",
    "print_result(\"Model Ranking\", analysis['comparison'].ranking)\n",
    "print_result(\"Reasoning\", analysis['comparison'].reasoning)\n",
    "\n",
    "# Show individual scores\n",
    "scores_df = pd.DataFrame([\n",
    "    {\"Model\": model, \"Score\": analysis['comparison'].scores.get(model, 0)}\n",
    "    for model in analysis['comparison'].ranking\n",
    "])\n",
    "display(HTML(\"<h4>Model Scores:</h4>\"))\n",
    "display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3: Tool Integration + Evaluator-Optimizer Loops\n",
    "\n",
    "**Workflow Patterns:** **Tool Integration** + **Evaluator-Optimizer**\n",
    "\n",
    "**Learning Objective:**\n",
    "Demonstrate how LLMs can execute external functions while maintaining quality control through evaluation loops.\n",
    "\n",
    "**Architecture Flow:**\n",
    "```\n",
    "[User Input] → [LLM Decision] → [Tool Execution] → [Tool Result] → [LLM Response]\n",
    "                   ↓                                              ↓\n",
    "            [Select Tool Type]                            [Evaluator Assessment]\n",
    "                   ↓                                              ↓\n",
    "           [Function Arguments]                          [Accept ✅ | Retry ❌]\n",
    "```\n",
    "\n",
    "**Tool Integration Pattern Explained:**\n",
    "This pattern enables LLMs to interact with the external world:\n",
    "1. **Intent Recognition**: LLM analyzes user input for tool requirements\n",
    "2. **Tool Selection**: LLM chooses appropriate function to call\n",
    "3. **Argument Extraction**: LLM structures function arguments\n",
    "4. **Execution**: External function runs with LLM-provided parameters\n",
    "5. **Context Integration**: Tool results are incorporated into final response\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Extends LLM Capabilities**: Beyond text generation to action execution\n",
    "- **Real-World Integration**: Connect AI to APIs, databases, systems\n",
    "- **Dynamic Interaction**: Responses based on live data, not training data\n",
    "- **Structured Processing**: Validate inputs and outputs systematically\n",
    "\n",
    "**Evaluator-Optimizer Loop Enhanced:**\n",
    "For tool usage, evaluation becomes more complex:\n",
    "1. **Functional Accuracy**: Did the tool execute correctly?\n",
    "2. **Result Relevance**: Is the tool output appropriate for the question?\n",
    "3. **Integration Quality**: How well are tool results incorporated?\n",
    "4. **User Satisfaction**: Does the final response meet user needs?\n",
    "\n",
    "**Code Implementation Details:**\n",
    "- **Tool Functions**: `get_current_time()`, `get_weather(city)`\n",
    "- **Tool Schema**: JSON definitions for LLM understanding\n",
    "- **Execution Logic**: `execute_tool()` function dispatcher\n",
    "- **Evaluation**: Enhanced criteria for tool-assisted responses\n",
    "- **Retry Mechanism**: Automatic regeneration for failed tool usage\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Personal assistants with calendar/email access\n",
    "- Customer service bots with database queries\n",
    "- Research assistants with web search capabilities\n",
    "- IoT control systems with device integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM on June 23, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TOOL USAGE WITH EVALUATION:\n",
      "Attempt 1 failed evaluation. Retrying...\n",
      "Feedback: The AI response provides a specific time but is incorrect regarding the actual current time. This undermines the primary purpose of answering the user's question accurately. The response lacks real-time awareness, which is a critical requirement for a general-purpose assistant when asked about the current time.\n",
      "Attempt 2 failed evaluation. Retrying...\n",
      "Feedback: The AI response fails to provide an accurate current time, which is a fundamental requirement for such a question. Instead, it gives a time that is future-dated, making the response incorrect and unhelpful. While the format of the time and date is clear, the inaccuracy undermines its overall utility.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Response with Evaluation</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM on June 23, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Evaluation Details</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"Score\": \"3/10\",\n",
       "  \"Acceptable\": false,\n",
       "  \"Strengths\": [\n",
       "    \"The response is formatted clearly with both time and date.\",\n",
       "    \"It maintains a neutral and informative tone.\"\n",
       "  ],\n",
       "  \"Suggestions\": [\n",
       "    \"The AI should indicate that it cannot provide real-time information and suggest the user check their device for the current time.\",\n",
       "    \"Including a disclaimer about the limitations of the AI in providing live data would enhance the user experience.\"\n",
       "  ]\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MULTI-MODEL TOOL COMPARISON:\n",
      "Generating response with gpt-4o-mini...\n",
      "Generating response with gpt-4o...\n",
      "Generating response with gpt-4-turbo...\n",
      "Comparing all responses...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:green;\">Best Tool User</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "gpt-4o-mini"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Usage - gpt-4o-mini</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM on June 23, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Usage - gpt-4o</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM on June 23, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Usage - gpt-4-turbo</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic tool usage\n",
    "response = run_agent(\"What time is it now?\")\n",
    "print_result(\"Tool Response\", response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TOOL USAGE WITH EVALUATION:\")\n",
    "\n",
    "# Tool usage with evaluation\n",
    "result_with_eval = run_agent_with_evaluation(\"What time is it now?\")\n",
    "print_result(\"Tool Response with Evaluation\", result_with_eval['response'])\n",
    "\n",
    "evaluation = result_with_eval['evaluation']\n",
    "print_result(\"Tool Evaluation Details\", {\n",
    "    \"Score\": f\"{evaluation.score}/10\",\n",
    "    \"Acceptable\": evaluation.is_acceptable,\n",
    "    \"Strengths\": evaluation.strengths,\n",
    "    \"Suggestions\": evaluation.suggestions\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-MODEL TOOL COMPARISON:\")\n",
    "\n",
    "# Compare tool usage across models\n",
    "tool_analysis = run_comparative_analysis(\"What time is it now?\")\n",
    "print_result(\"Best Tool User\", tool_analysis['comparison'].best_model, \"green\")\n",
    "\n",
    "for model_name, response in tool_analysis['responses'].items():\n",
    "    print_result(f\"Tool Usage - {model_name}\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4: Orchestrator-Worker Pattern + Advanced Tool Integration\n",
    "\n",
    "**Workflow Patterns:** **Orchestrator-Worker** + **Structured Tool Calling**\n",
    "\n",
    "**Learning Objective:**\n",
    "Implement sophisticated coordination patterns where an LLM orchestrator manages complex multi-step tasks with specialized worker components.\n",
    "\n",
    "**Architecture Flow:**\n",
    "```\n",
    "[Complex Query] → [Orchestrator LLM] → [Task Decomposition] → [Worker Tools] → [Result Integration]\n",
    "                         ↓                    ↓                    ↓                    ↓\n",
    "                 [Plan Generation]      [Parallel Execution]  [Status Monitoring]  [Quality Assessment]\n",
    "                         ↓                    ↓                    ↓                    ↓\n",
    "                 [Resource Allocation]  [Error Handling]      [Result Collection] [Final Response]\n",
    "```\n",
    "\n",
    "**Orchestrator-Worker Pattern Explained:**\n",
    "This is the most sophisticated workflow pattern we implement:\n",
    "1. **Orchestrator Role**: Main LLM analyzes complex requests and creates execution plans\n",
    "2. **Task Decomposition**: Breaks down complex queries into manageable subtasks\n",
    "3. **Worker Coordination**: Dispatches subtasks to specialized tools or models\n",
    "4. **Progress Monitoring**: Tracks execution status and handles errors\n",
    "5. **Result Integration**: Combines outputs from multiple workers into coherent response\n",
    "\n",
    "**Advanced Tool Integration:**\n",
    "- **Structured Arguments**: Tools accept complex, validated JSON parameters\n",
    "- **Error Handling**: Robust failure detection and recovery mechanisms\n",
    "- **External Systems**: Integration with real-world services (notifications, databases)\n",
    "- **Production Features**: Deployment-ready with monitoring and logging\n",
    "\n",
    "**Pattern Characteristics:**\n",
    "- **Highest Autonomy**: Orchestrator LLM makes complex coordination decisions\n",
    "- **Dynamic Flow**: Execution path adapts based on intermediate results\n",
    "- **Scalability**: Can coordinate any number of worker components\n",
    "- **Robustness**: Built-in error handling and fallback mechanisms\n",
    "\n",
    "**Comparative Analysis as Orchestrator-Worker:**\n",
    "Our `run_comparative_analysis()` function demonstrates this pattern:\n",
    "1. **Orchestrator**: Main evaluation LLM coordinates the entire process\n",
    "2. **Workers**: Multiple generator models produce responses\n",
    "3. **Coordination**: Orchestrator manages evaluation of each worker's output\n",
    "4. **Integration**: Final ranking combines all worker results intelligently\n",
    "\n",
    "**Code Implementation Details:**\n",
    "- **Advanced Tools**: `get_weather(city)`, `record_user_details(email, name, notes)`\n",
    "- **Orchestration Logic**: `run_comparative_analysis()` as orchestrator function\n",
    "- **Worker Management**: Multiple model coordination with error handling\n",
    "- **Quality Control**: Enhanced evaluation criteria for complex outputs\n",
    "- **Production Features**: Web interface, monitoring, deployment automation\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Project management systems with AI coordination\n",
    "- Complex research tasks requiring multiple specialists\n",
    "- Multi-step customer service workflows\n",
    "- Enterprise automation with human-AI collaboration\n",
    "- Scientific analysis pipelines with multiple data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Structured Tool Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The weather in Tokyo is currently 25°C and raining."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "WEATHER TOOL WITH ADVANCED EVALUATION:\n",
      "\n",
      "Testing weather for Tokyo:\n",
      "Attempt 1 failed evaluation. Retrying...\n",
      "Feedback: The response provides a specific temperature and weather condition, but it lacks real-time accuracy as the information is not verifiable and may not reflect the current weather. Additionally, it does not mention the date or time of the report, which is crucial for weather information. The simplicity of the statement is clear, but it could benefit from more context or detail.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Weather in Tokyo</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current weather in Tokyo is 25°C and it is raining."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing weather for Barcelona:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Weather in Barcelona</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The weather in Barcelona is currently 22°C and sunny."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing weather for New York:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Weather in New York</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current weather in New York is 17°C and cloudy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing weather for London:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Weather in London</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current weather in London is 15°C and foggy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "COMPREHENSIVE WEATHER ANALYSIS:\n",
      "Generating response with gpt-4o-mini...\n",
      "Generating response with gpt-4o...\n",
      "Generating response with gpt-4-turbo...\n",
      "Comparing all responses...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:purple;\">Question</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Compare the weather between Tokyo and Barcelona, and recommend which city would be better for outdoor activities today."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:green;\">Best Model for Weather Analysis</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "gpt-4o-mini"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Model Ranking</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "['gpt-4o-mini', 'gpt-4o', 'gpt-4-turbo']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Model Responses:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">gpt-4o-mini (Score: 8/10)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Today, the weather in Tokyo is 25°C with rain, while in Barcelona it is 22°C and sunny. \n",
       "\n",
       "Given these conditions, Barcelona would be the better choice for outdoor activities today. The sunny weather and mild temperature in Barcelona are more conducive to enjoying outdoor pursuits compared to the rainy conditions in Tokyo."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">gpt-4o (Score: 7/10)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Today, Tokyo has a temperature of 25°C with rain, while Barcelona is experiencing sunny weather with a temperature of 22°C. For outdoor activities today, Barcelona would be the better choice given the pleasant weather conditions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">gpt-4-turbo (Score: 8/10)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Today, Tokyo is experiencing rain with a temperature of 25°C, while Barcelona has sunny weather with a temperature of 22°C.\n",
       "\n",
       "For outdoor activities, Barcelona would be the better choice today due to its sunny weather, making it more suitable for spending time outside comfortably. Tokyo's rainy conditions might hinder outdoor activities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Winner's Reasoning:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:gold;\">Why this model won</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Comparison failed: Expecting value: line 1 column 1 (char 0)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic structured tool calling\n",
    "response = run_agent(\"What's the weather in Tokyo?\")\n",
    "print_result(\"Structured Tool Response\", response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WEATHER TOOL WITH ADVANCED EVALUATION:\")\n",
    "\n",
    "# Multiple cities with evaluation\n",
    "cities = [\"Tokyo\", \"Barcelona\", \"New York\", \"London\"]\n",
    "\n",
    "for city in cities:\n",
    "    print(f\"\\nTesting weather for {city}:\")\n",
    "    result = run_agent_with_evaluation(f\"What's the weather in {city}?\", max_retries=1)\n",
    "    \n",
    "    evaluation = result['evaluation']\n",
    "    print_result(f\"Weather in {city}\", result['response'])\n",
    "    \n",
    "    if evaluation.score < 7:\n",
    "        print(f\"⚠️ Low quality response (Score: {evaluation.score}/10)\")\n",
    "        print(f\"Feedback: {evaluation.feedback}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPREHENSIVE WEATHER ANALYSIS:\")\n",
    "\n",
    "# Full analysis for a complex weather question\n",
    "complex_question = \"Compare the weather between Tokyo and Barcelona, and recommend which city would be better for outdoor activities today.\"\n",
    "\n",
    "final_analysis = run_comparative_analysis(complex_question)\n",
    "\n",
    "print_result(\"Question\", complex_question, \"purple\")\n",
    "print_result(\"Best Model for Weather Analysis\", final_analysis['comparison'].best_model, \"green\")\n",
    "print_result(\"Model Ranking\", final_analysis['comparison'].ranking)\n",
    "\n",
    "# Show all responses\n",
    "print(\"\\nAll Model Responses:\")\n",
    "for model_name, response in final_analysis['responses'].items():\n",
    "    score = final_analysis['evaluations'][model_name].score\n",
    "    print_result(f\"{model_name} (Score: {score}/10)\", response)\n",
    "\n",
    "print(\"\\nWinner's Reasoning:\")\n",
    "print_result(\"Why this model won\", final_analysis['comparison'].reasoning, \"gold\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Complete Workflow Patterns Analysis\n",
    "\n",
    "### Summary: All 5 Fundamental Patterns Implemented\n",
    "\n",
    "**1. ✅ Prompt Chaining (Lab 1)**\n",
    "- **Implementation**: Basic `run_agent()` function with sequential processing\n",
    "- **Autonomy Level**: Low-Medium (predefined sequence)\n",
    "- **Key Learning**: Foundation of all other patterns\n",
    "\n",
    "**2. ✅ Routing (Throughout)**\n",
    "- **Implementation**: Model selection logic in `model_manager`\n",
    "- **Autonomy Level**: Medium (router logic makes decisions)\n",
    "- **Key Learning**: Task-specific model assignment\n",
    "\n",
    "**3. ✅ Parallelization (Lab 2)**\n",
    "- **Implementation**: `run_agent_with_multiple_models()` concurrent execution\n",
    "- **Autonomy Level**: Low (code-controlled distribution)\n",
    "- **Key Learning**: Speed and redundancy through concurrent processing\n",
    "\n",
    "**4. ✅ Orchestrator-Worker (Lab 4)**\n",
    "- **Implementation**: `run_comparative_analysis()` coordination system\n",
    "- **Autonomy Level**: Medium-High (orchestrator makes coordination decisions)\n",
    "- **Key Learning**: Complex task decomposition and result integration\n",
    "\n",
    "**5. ✅ Evaluator-Optimizer (Labs 2-4)**\n",
    "- **Implementation**: `run_agent_with_evaluation()` validation loops\n",
    "- **Autonomy Level**: Medium (evaluator makes quality decisions)\n",
    "- **Key Learning**: Quality control through feedback loops\n",
    "\n",
    "---\n",
    "\n",
    "## Pattern Progression and Complexity\n",
    "\n",
    "**Complexity Ladder:**\n",
    "1. **Prompt Chaining** → Simple, predictable, foundational\n",
    "2. **Routing** → Decision-making, specialization\n",
    "3. **Parallelization** → Concurrency, efficiency  \n",
    "4. **Evaluator-Optimizer** → Quality control, feedback\n",
    "5. **Orchestrator-Worker** → Coordination, complex task management\n",
    "\n",
    "**Autonomy Progression:**\n",
    "- **Low**: Human-defined sequences (Prompt Chaining, Parallelization)\n",
    "- **Medium**: LLM decision-making within constraints (Routing, Evaluator-Optimizer)\n",
    "- **High**: Dynamic coordination and planning (Orchestrator-Worker)\n",
    "\n",
    "---\n",
    "\n",
    "## Advanced Technical Architecture\n",
    "\n",
    "**Core System Components:**\n",
    "1. **Model Manager**: Multi-provider orchestration (implements Routing)\n",
    "2. **Evaluation Engine**: Quality control system (implements Evaluator-Optimizer)\n",
    "3. **Tool Handler**: External function coordination (enables complex workflows)\n",
    "4. **Analysis Engine**: Comparative coordination (implements Orchestrator-Worker)\n",
    "\n",
    "**Production-Ready Guardrails:**\n",
    "- **Structured Validation**: Pydantic model enforcement across all patterns\n",
    "- **Retry Mechanisms**: Evaluator-Optimizer loops with feedback\n",
    "- **Quality Assessment**: Automatic evaluation in Labs 2-4\n",
    "- **Error Handling**: Graceful degradation in all workflow patterns\n",
    "- **Resource Management**: Timeout and rate limiting controls\n",
    "\n",
    "---\n",
    "\n",
    "## From Workflows to True Agents\n",
    "\n",
    "**What We've Built (Workflows):**\n",
    "- Predictable execution paths\n",
    "- Clear control mechanisms\n",
    "- Defined start and end points\n",
    "- Human-designed coordination\n",
    "\n",
    "**Next Steps (True Agents):**\n",
    "- Open-ended iterative loops\n",
    "- Dynamic self-modification\n",
    "- Uncertain execution duration\n",
    "- Autonomous goal pursuit\n",
    "\n",
    "**Key Insight:** Workflows are the building blocks that enable true agent behavior. Mastering these 5 patterns provides the foundation for building more autonomous systems in subsequent weeks.\n",
    "\n",
    "---\n",
    "\n",
    "## Commercial Applications by Pattern\n",
    "\n",
    "**Prompt Chaining Applications:**\n",
    "- Content generation pipelines\n",
    "- Document processing workflows\n",
    "- Template-based systems\n",
    "\n",
    "**Parallelization Applications:**\n",
    "- A/B testing systems\n",
    "- Consensus-building platforms\n",
    "- Risk mitigation through redundancy\n",
    "\n",
    "**Evaluator-Optimizer Applications:**\n",
    "- Quality assurance systems\n",
    "- Content moderation platforms\n",
    "- Performance optimization tools\n",
    "\n",
    "**Orchestrator-Worker Applications:**\n",
    "- Project management systems\n",
    "- Complex research workflows\n",
    "- Multi-specialist coordination\n",
    "\n",
    "**Integration Potential:**\n",
    "All patterns can be combined for enterprise-grade agentic systems with predictable behavior and robust quality control.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d12e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM CONFIGURATION VALIDATION\n",
      "==================================================\n",
      "Available Models: 3\n",
      "   ✅ GPT-4O Mini (openai)\n",
      "   ✅ GPT-4O (openai)\n",
      "   ✅ GPT-4 Turbo (openai)\n",
      "\n",
      "API Keys Status:\n",
      "   ✅ OpenAI: Configured (sk-proj-...)\n",
      "   ⚠️ Anthropic: Not configured (optional)\n",
      "   ⚠️ Google: Not configured (optional)\n",
      "   ⚠️ DeepSeek: Not configured (optional)\n",
      "\n",
      "System Status: ✅ READY FOR PRODUCTION\n",
      "\n",
      "Quick Functionality Test:\n",
      "✅ Basic Agent: Working\n",
      "✅ Evaluation System: Working (Score: 4/10)\n",
      "All systems operational!\n"
     ]
    }
   ],
   "source": [
    "# System Validation & Configuration Test\n",
    "print(\"SYSTEM CONFIGURATION VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check model availability\n",
    "available_models = model_manager.get_available_models()\n",
    "print(f\"Available Models: {len(available_models)}\")\n",
    "for model in available_models:\n",
    "    info = model_manager.get_model_info(model)\n",
    "    print(f\"   ✅ {info.name} ({info.provider})\")\n",
    "\n",
    "print(\"\\nAPI Keys Status:\")\n",
    "import os\n",
    "apis = [\n",
    "    (\"OpenAI\", \"OPENAI_API_KEY\"),\n",
    "    (\"Anthropic\", \"ANTHROPIC_API_KEY\"), \n",
    "    (\"Google\", \"GOOGLE_API_KEY\"),\n",
    "    (\"DeepSeek\", \"DEEPSEEK_API_KEY\")\n",
    "]\n",
    "\n",
    "for name, env_var in apis:\n",
    "    key = os.getenv(env_var)\n",
    "    if key:\n",
    "        print(f\"   ✅ {name}: Configured ({key[:8]}...)\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ {name}: Not configured (optional)\")\n",
    "\n",
    "print(f\"\\nSystem Status: {'✅ READY FOR PRODUCTION' if available_models else '⚠️ NEEDS CONFIGURATION'}\")\n",
    "\n",
    "# Quick functionality test\n",
    "print(\"\\nQuick Functionality Test:\")\n",
    "try:\n",
    "    test_response = run_agent(\"Hello, test the system!\", \"gpt-4o-mini\")\n",
    "    print(f\"✅ Basic Agent: Working\")\n",
    "    \n",
    "    test_eval = evaluator.evaluate_response(\"Test\", test_response)\n",
    "    print(f\"✅ Evaluation System: Working (Score: {test_eval.score}/10)\")\n",
    "    \n",
    "    print(\"All systems operational!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Please check your configuration and API keys.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94609926",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Practical Decision Guide: When to Use Each Pattern\n",
    "\n",
    "### Pattern Selection Framework\n",
    "\n",
    "**Choose Prompt Chaining when:**\n",
    "- ✅ Simple, linear workflow\n",
    "- ✅ Predictable sequence of operations\n",
    "- ✅ Need high control and reliability\n",
    "- ✅ Cost-effectiveness is priority\n",
    "- ❌ Complex decision-making required\n",
    "\n",
    "**Choose Routing when:**\n",
    "- ✅ Different specialized models for different tasks\n",
    "- ✅ Task classification needed\n",
    "- ✅ Want to optimize model selection\n",
    "- ✅ Have domain-specific requirements\n",
    "- ❌ All tasks similar in nature\n",
    "\n",
    "**Choose Parallelization when:**\n",
    "- ✅ Speed is critical\n",
    "- ✅ Need redundancy for reliability\n",
    "- ✅ Want consensus or comparison\n",
    "- ✅ Tasks are independent\n",
    "- ❌ Sequential dependencies exist\n",
    "\n",
    "**Choose Evaluator-Optimizer when:**\n",
    "- ✅ Quality control is critical\n",
    "- ✅ Iterative improvement needed\n",
    "- ✅ Production reliability required\n",
    "- ✅ Cost of failure is high\n",
    "- ❌ Speed is more important than quality\n",
    "\n",
    "**Choose Orchestrator-Worker when:**\n",
    "- ✅ Complex, multi-step workflows\n",
    "- ✅ Dynamic task decomposition needed\n",
    "- ✅ Resource coordination required\n",
    "- ✅ Flexible execution paths desired\n",
    "- ❌ Simple, straightforward tasks\n",
    "\n",
    "---\n",
    "\n",
    "### Combining Patterns\n",
    "\n",
    "**Successful Pattern Combinations:**\n",
    "- **Parallelization + Evaluator-Optimizer**: Multi-model comparison with quality control\n",
    "- **Routing + Orchestrator-Worker**: Smart task assignment with complex coordination\n",
    "- **Prompt Chaining + Evaluator-Optimizer**: Sequential processing with validation\n",
    "- **All Patterns Together**: Enterprise-grade agentic systems\n",
    "\n",
    "**Pattern Interaction Benefits:**\n",
    "- **Robustness**: Multiple quality control mechanisms\n",
    "- **Efficiency**: Optimized resource utilization\n",
    "- **Flexibility**: Adaptable to different scenarios\n",
    "- **Scalability**: Can handle increasing complexity\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Web Interface with Gradio\n",
    "\n",
    "Now let's launch the web interface that brings everything together!\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Challenges and Limitations of Agentic Systems\n",
    "\n",
    "### Architectural Challenges\n",
    "\n",
    "**1. Unpredictability:**\n",
    "- **Variable execution paths**: Unknown execution order\n",
    "- **Uncertain outputs**: No quality guarantees\n",
    "- **Indeterminate timing**: Unpredictable duration\n",
    "\n",
    "**2. Variable Costs:**\n",
    "- **Unpredictable token consumption**\n",
    "- **Multiple API calls** without clear limits\n",
    "- **Exponential cost scaling**\n",
    "\n",
    "**3. Guardrail Requirements:**\n",
    "- **Continuous monitoring** of system behavior\n",
    "- **Resource and time limits**\n",
    "- **Real-time output validation**\n",
    "\n",
    "---\n",
    "\n",
    "## Implemented Mitigations\n",
    "\n",
    "**Monitoring and Observability:**\n",
    "- **Traceability**: Complete interaction tracking\n",
    "- **Metrics**: Time, cost, and response quality measurement\n",
    "- **Alerting**: Anomalous behavior notifications\n",
    "\n",
    "**Containment Systems:**\n",
    "- **Timeouts**: Maximum execution limits\n",
    "- **Rate limiting**: API call frequency control\n",
    "- **Validation gates**: Quality control checkpoints\n",
    "- **Fallback mechanisms**: Alternative paths for failures\n",
    "\n",
    "---\n",
    "\n",
    "## Applied Best Practices\n",
    "\n",
    "**Robust Design Principles:**\n",
    "1. **Start Simple**: Begin with basic workflows\n",
    "2. **Add Complexity Gradually**: Incremental functionality addition\n",
    "3. **Test Extensively**: Comprehensive pre-production testing\n",
    "4. **Monitor Everything**: Complete system observability\n",
    "\n",
    "**Intelligent Optimization:**\n",
    "- **Model Selection**: Optimal LLM choice for each task\n",
    "- **Caching**: Response reuse when appropriate\n",
    "- **Parallel Processing**: Temporal efficiency maximization\n",
    "- **Error Recovery**: Robust recovery mechanisms\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## LLM Model Landscape\n",
    "\n",
    "### Frontier Models Implemented\n",
    "\n",
    "**OpenAI:**\n",
    "- **GPT-4o-mini**: Primary model (fast, cost-effective)\n",
    "- **GPT-4o**: Maximum performance (higher cost)\n",
    "- **o1-mini**: Reasoning models (chain-of-thought optimized)\n",
    "\n",
    "**Anthropic:**\n",
    "- **Claude 3.5 Sonnet**: Excellent price/performance balance\n",
    "- **Claude 3 Haiku**: Cost-effective alternative\n",
    "\n",
    "**Google:**\n",
    "- **Gemini 2.0 Flash**: Free within usage limits\n",
    "- **Gemini Pro**: Enhanced performance version\n",
    "\n",
    "**DeepSeek (China):**\n",
    "- **DeepSeek V3 R1**: GPT-4 performance at 1/30th cost\n",
    "- **Distilled Models**: Optimized versions for local deployment\n",
    "\n",
    "**Groq:**\n",
    "- **Fast Inference**: LLaMA 3.3 (70B) at high speed\n",
    "- **Low Cost**: Ideal for rapid prototyping\n",
    "\n",
    "**Ollama:**\n",
    "- **Local Models**: Zero API costs\n",
    "- **High Performance**: C++ optimized backend\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources and Next Steps\n",
    "\n",
    "**Recommended Resources:**\n",
    "- **ValuLM Leaderboard**: Model comparison and pricing\n",
    "- **Hugging Face**: Open source model repository\n",
    "- **LangSmith**: Traceability and debugging tools\n",
    "- **Guides folder**: Additional technical documentation\n",
    "\n",
    "**Learning Roadmap:**\n",
    "1. **Week 2**: OpenAI Agents SDK\n",
    "2. **Week 3**: CrewAI - Agent frameworks\n",
    "3. **Week 4**: LangGraph - Execution graphs\n",
    "4. **Week 5**: AutoGen - Multi-agent conversations\n",
    "5. **Week 6**: MCP - Model Context Protocol\n",
    "\n",
    "**Mastery Objectives:**\n",
    "- **Architecture**: Design robust agentic systems\n",
    "- **Implementation**: Clean, maintainable code\n",
    "- **Deployment**: Production-ready systems\n",
    "- **Optimization**: Balanced cost and performance\n",
    "\n",
    "---\n",
    "\n",
    "## Course Completion\n",
    "\n",
    "You have successfully completed the **AI Agents Foundations Laboratory**. You now have deep understanding of:\n",
    "- ✅ Workflow and agent architectures\n",
    "- ✅ Agentic design patterns\n",
    "- ✅ Multi-LLM practical implementation\n",
    "- ✅ Advanced tools and deployment\n",
    "- ✅ Best practices and guardrails\n",
    "\n",
    "**You are ready for the advanced challenges of the upcoming weeks.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "551baf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Advanced AI Agent Web Interface...\n",
      "Features available:\n",
      "   - Simple Chat\n",
      "   - Chat with Evaluation\n",
      "   - Multi-Model Comparison\n",
      "   - System Status\n",
      "\n",
      "Click the link below to access the interface!\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch the Advanced Web Interface\n",
    "from week1_foundations.interface import launch_interface\n",
    "\n",
    "# Launch in notebook (inline)\n",
    "print(\"Starting Advanced AI Agent Web Interface...\")\n",
    "print(\"Features available:\")\n",
    "print(\"   - Simple Chat\")\n",
    "print(\"   - Chat with Evaluation\") \n",
    "print(\"   - Multi-Model Comparison\")\n",
    "print(\"   - System Status\")\n",
    "print(\"\\nClick the link below to access the interface!\")\n",
    "\n",
    "# Launch with share=False for local use, share=True for public link\n",
    "launch_interface(share=False, port=7860)\n",
    "\n",
    "# Note: The interface will open in a new tab\n",
    "# You can also access it directly at http://localhost:7860\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66db3dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing corrected imports and functionality...\n",
      "✅ Basic agent test successful\n",
      "Response preview: Hello! It looks like you're testing the system. How can I assist you today?...\n",
      "✅ Evaluation system test successful\n",
      "Score: 10/10\n",
      "\n",
      "All tests passed! The system is working correctly.\n"
     ]
    }
   ],
   "source": [
    "# Test imports and basic functionality\n",
    "print(\"Testing corrected imports and functionality...\")\n",
    "\n",
    "try:\n",
    "    # Test basic agent functionality\n",
    "    test_response = run_agent(\"Hello, this is a test\")\n",
    "    print(f\"✅ Basic agent test successful\")\n",
    "    print(f\"Response preview: {test_response[:100]}...\")\n",
    "    \n",
    "    # Test evaluation system\n",
    "    test_eval_result = run_agent_with_evaluation(\"What is 2+2?\", max_retries=1)\n",
    "    print(f\"✅ Evaluation system test successful\")\n",
    "    print(f\"Score: {test_eval_result['evaluation'].score}/10\")\n",
    "    \n",
    "    print(\"\\nAll tests passed! The system is working correctly.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during testing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
