{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31b7613",
   "metadata": {},
   "source": [
    "# Advanced AI Agents `Foundations` Laboratory\n",
    "\n",
    "\n",
    "- [Introduction to Agentic AI: Theory and Practice](#introduction-to-agentic-ai-theory-and-practice)\n",
    "- [What is an AI Agent](#what-is-an-ai-agent)\n",
    "- [Anthropic's Framework: Workflows vs Agents](#anthropics-framework-workflows-vs-agents)\n",
    "- [The 5 Fundamental Workflow Patterns](#the-5-fundamental-workflow-patterns)\n",
    "    - [1. Prompt Chaining](#1-prompt-chaining)\n",
    "    - [2. Routing](#2-routing)\n",
    "    - [3. Parallelization](#3-parallelization)\n",
    "    - [4. Orchestrator-Worker](#4-orchestrator-worker)\n",
    "    - [5. Evaluator-Optimizer-Validation Loop](#5-evaluator-optimizer-validation-loop)\n",
    "- [Refactoring examples](#refactoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d84f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import os\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set - please head to the troubleshooting guide in the setup folder\")\n",
    "\n",
    "from openai import OpenAI\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4891861f",
   "metadata": {},
   "source": [
    "## Introduction to Agentic AI Theory and Practice\n",
    "\n",
    "This notebook demonstrates comprehensive AI agent capabilities through four progressive laboratories, integrating theoretical concepts with practical implementations. We'll explore the **5 fundamental Workflow Patterns** and understand how they form the building blocks of agentic systems.\n",
    "\n",
    "**Core Learning Objectives:**\n",
    "- Master the 5 fundamental Workflow Patterns through practical implementation\n",
    "- Differentiate between Workflows (predefined) and Agents (dynamic)\n",
    "- Multi-model architecture implementation and comparison\n",
    "- Automatic response evaluation with structured validation\n",
    "- Tool integration patterns and real-world deployment\n",
    "\n",
    "---\n",
    "\n",
    "## What is an AI Agent?\n",
    "\n",
    "According to Hugging Face's definition:\n",
    "> \"AI agents are programs where LLM outputs control the workflow\"\n",
    "\n",
    "This means the output of a language model determines which tasks are executed and in what order.\n",
    "\n",
    "**Hallmarks of Agentic AI:**\n",
    "1. **Multiple LLM calls** - Like our multi-model comparison system\n",
    "2. **Tool use** - LLMs executing external functions (time, weather)\n",
    "3. **LLM communication** - Models passing information between each other\n",
    "4. **Planning** - An LLM acting as a planner to coordinate tasks\n",
    "5. **Autonomy** - The system has freedom to choose how to proceed\n",
    "\n",
    "**Autonomy** is often seen as the key element - when a model chooses how to respond or which path to take, that reflects autonomy.\n",
    "\n",
    "---\n",
    "\n",
    "## Anthropic's Framework: Workflows vs Agents\n",
    "\n",
    "Anthropic categorizes `agentic systems` into two types:\n",
    "\n",
    "### **Workflows (Predefined Orchestration):**\n",
    "- Structured, predictable execution paths\n",
    "- Defined sequences of model and tool interactions\n",
    "- Clear guardrails and control mechanisms\n",
    "- **Our Labs 1-4 demonstrate these patterns**\n",
    "\n",
    "### **Agents (Dynamic Control):**\n",
    "- Models dynamically control tools and task flow\n",
    "- Open-ended, iterative loops with feedback\n",
    "- Less predictable but more powerful\n",
    "- Will be explored in future weeks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5868f8",
   "metadata": {},
   "source": [
    "## The 5 Fundamental Workflow Patterns\n",
    "\n",
    "### **1. Prompt Chaining**\n",
    "\n",
    "![](../img/01.png)\n",
    "\n",
    "**Concept:** Chain a sequence of LLMs, each doing a subtask based on the previous output.\n",
    "- **Example:** LLM1 suggests business sector → LLM2 identifies pain point → LLM3 recommends solution\n",
    "- **Our Implementation:** Lab 1 demonstrates basic sequential calls\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Chaining \n",
    "# Simple Prompt (precursor) | A single direct question, no chaining involved\n",
    "\n",
    "messages = [{\"role\": \"user\", \n",
    "             \"content\": \"What is 2+2?\"}]\n",
    "# This uses GPT-4o-mini, the cost-effective model\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c8be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Chaining           \n",
    "# First step in a chain of linked tasks\n",
    "question = \"Please propose a hard, challenging question to assess someone's IQ. Respond only with the question.\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "# ask it - this uses GPT-4o-mini, cost-effective but capable\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0fba9",
   "metadata": {},
   "source": [
    "### **2. Routing**\n",
    "\n",
    "![](../img/02.png)\n",
    "\n",
    "**Concept:** An LLM router decides which specialized model should handle a task.\n",
    "- **Example:** Router evaluates input → sends to specialized LLM1, LLM2, or LLM3\n",
    "- **Our Implementation:** Model selection logic based on task requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4df9138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Query: What is 25 + 37?\n",
      "🎯 ROUTING DECISION:\n",
      "   Task Type: SIMPLE\n",
      "   Selected Model: gpt-4o-mini\n",
      "   Reason: Simple task routed to efficient model\n",
      "--------------------------------------------------\n",
      "✅ Response: 25 + 37 equals 62....\n",
      "================================================================================\n",
      "\n",
      "📝 Query: Analyze the economic implications of renewable energy adoption in developing countries\n",
      "🎯 ROUTING DECISION:\n",
      "   Task Type: COMPLEX\n",
      "   Selected Model: gpt-4o\n",
      "   Reason: Complex task routed to advanced model\n",
      "--------------------------------------------------\n",
      "✅ Response: The adoption of renewable energy in developing countries has several significant economic implicatio...\n",
      "================================================================================\n",
      "\n",
      "📝 Query: Write a creative short story about a robot learning to paint\n",
      "🎯 ROUTING DECISION:\n",
      "   Task Type: CREATIVE\n",
      "   Selected Model: gpt-4-turbo\n",
      "   Reason: Creative task routed to most capable model\n",
      "--------------------------------------------------\n",
      "✅ Response: In the heart of a bustling city filled with gleaming skyscrapers and neon lights, there was a quaint...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 2. ROUTING PATTERN - Practical Example\n",
    "# This demonstrates how to intelligently route tasks to different models based on task type\n",
    "\n",
    "from week1_foundations.models import model_manager\n",
    "from week1_foundations.agent import run_agent\n",
    "\n",
    "def route_by_task_type(user_input: str) -> str:\n",
    "    \"\"\"Router function that selects the best model based on task type\"\"\"\n",
    "    \n",
    "    # Create routing logic\n",
    "    routing_prompt = f\"\"\"\n",
    "    Analyze this user request and classify it into ONE of these categories:\n",
    "    1. SIMPLE - Basic questions, math, general knowledge\n",
    "    2. COMPLEX - Analysis, reasoning, creative tasks\n",
    "    3. CREATIVE - Writing, storytelling, brainstorming\n",
    "    \n",
    "    User request: \"{user_input}\"\n",
    "    \n",
    "    Respond with only: SIMPLE, COMPLEX, or CREATIVE\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use a fast model for routing decisions\n",
    "    router_response = model_manager.generate_response(\n",
    "        \"gpt-4o-mini\", \n",
    "        [{\"role\": \"user\", \"content\": routing_prompt}]\n",
    "    )\n",
    "    \n",
    "    task_type = router_response['content'].strip().upper()\n",
    "    \n",
    "    # Route to appropriate model based on classification\n",
    "    if task_type == \"SIMPLE\":\n",
    "        selected_model = \"gpt-4o-mini\"  # Fast and cost-effective\n",
    "        reason = \"Simple task routed to efficient model\"\n",
    "    elif task_type == \"COMPLEX\":\n",
    "        selected_model = \"gpt-4o\"       # More powerful for complex reasoning\n",
    "        reason = \"Complex task routed to advanced model\"\n",
    "    elif task_type == \"CREATIVE\":\n",
    "        selected_model = \"gpt-4-turbo\"  # Best for creative tasks\n",
    "        reason = \"Creative task routed to most capable model\"\n",
    "    else:\n",
    "        selected_model = \"gpt-4o-mini\"  # Default fallback\n",
    "        reason = \"Unknown task type, using default model\"\n",
    "    \n",
    "    print(f\"🎯 ROUTING DECISION:\")\n",
    "    print(f\"   Task Type: {task_type}\")\n",
    "    print(f\"   Selected Model: {selected_model}\")\n",
    "    print(f\"   Reason: {reason}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return selected_model\n",
    "\n",
    "# Test the routing pattern with different types of questions\n",
    "test_queries = [\n",
    "    \"What is 25 + 37?\",  # SIMPLE\n",
    "    \"Analyze the economic implications of renewable energy adoption in developing countries\",  # COMPLEX\n",
    "    \"Write a creative short story about a robot learning to paint\"  # CREATIVE\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n📝 Query: {query}\")\n",
    "    selected_model = route_by_task_type(query)\n",
    "    \n",
    "    # Generate response with selected model\n",
    "    response = run_agent(query, selected_model)\n",
    "    print(f\"✅ Response: {response[:100]}...\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501af613",
   "metadata": {},
   "source": [
    "**Routing pattern:**  \n",
    "This pattern consists of analyzing the nature of a task or input and selecting the best model, agent, or function to solve it, instead of sending all tasks to the same destination.\n",
    "\n",
    "**How is it different from other patterns?**  \n",
    "It is not parallelization (you don’t send the input to multiple models).\n",
    "It is not evaluator/validation (you don’t judge the response afterwards).\n",
    "It is not just prompt chaining (there are no sequential steps).\n",
    "\n",
    "It is routing, because there is an intermediate decision that determines the “path” the task will follow.\n",
    "\n",
    "**If you want a more advanced routing, you could:**  \n",
    "Use more sophisticated rules or a more powerful AI for the analysis.\n",
    "Add logging, metrics, or a fallback if the selected model fails.\n",
    "Combine it with an evaluator afterwards (but that would be routing + evaluation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299f593",
   "metadata": {},
   "source": [
    "### **3. Parallelization**\n",
    "\n",
    "![](../img/03.png)\n",
    "\n",
    "**Concept:** Break down task into parallel subtasks sent to multiple LLMs simultaneously.\n",
    "- **Example:** Same question sent to multiple models → results aggregated\n",
    "- **Our Implementation:** Lab 2 multi-model comparison system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bb82d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARALLELIZATION PATTERN DEMONSTRATION\n",
      "==================================================\n",
      "🧠 CHALLENGING QUESTION GENERATED:\n",
      "   If a train leaves a station traveling at 60 miles per hour and another train leaves the same station 30 minutes later traveling at 90 miles per hour, at what distance from the station will the second train catch up to the first train?\n",
      "================================================================================\n",
      "\n",
      "🚀 EXECUTING PARALLEL PROCESSING...\n",
      "   Running same question across all available models simultaneously\n",
      "Testing with gpt-4o-mini...\n",
      "Testing with gpt-4o...\n",
      "Testing with gpt-4-turbo...\n",
      "\n",
      "⏱️ PARALLEL EXECUTION COMPLETED in 27.28 seconds\n",
      "   Models tested: 3\n",
      "--------------------------------------------------\n",
      "\n",
      "🤖 GPT-4O Mini (openai):\n",
      "   Status: ✅ Success\n",
      "   Response: To find the distance from the station where the second train catches up to the first train, we can use the following approach:\n",
      "\n",
      "1. **Convert the time ...\n",
      "--------------------------------------------------\n",
      "\n",
      "🤖 GPT-4O (openai):\n",
      "   Status: ✅ Success\n",
      "   Response: To determine the distance at which the second train catches up to the first train, we can use the concept of relative speed and distance.\n",
      "\n",
      "1. **Initia...\n",
      "--------------------------------------------------\n",
      "\n",
      "🤖 GPT-4 Turbo (openai):\n",
      "   Status: ✅ Success\n",
      "   Response: To solve this problem, we need to determine when the second train will catch up to the first train in terms of distance. We can use the concept of rel...\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ADVANCED PARALLELIZATION DEMO\n",
      "================================================================================\n",
      "🎯 SPECIALIZED PARALLEL PROCESSING:\n",
      "   Each model gets a task optimized for its strengths\n",
      "============================================================\n",
      "🔄 Processing gpt-4o-mini...🔄 Processing gpt-4o...\n",
      "🔄 Processing gpt-4-turbo...\n",
      "\n",
      "\n",
      "⚡ SPECIALIZED EXECUTION COMPLETED in 16.07 seconds\n",
      "------------------------------------------------------------\n",
      "\n",
      "🎯 gpt-4-turbo:\n",
      "   Task: Write a creative haiku about technology and nature finding harmony\n",
      "   Time: 2.72s\n",
      "   Response: Silicon pulses,\n",
      "Roots entwine through wired webs —\n",
      "Spring's code blooms anew....\n",
      "----------------------------------------\n",
      "\n",
      "🎯 gpt-4o-mini:\n",
      "   Task: Solve this math problem: If a train travels at 80 km/h for 2.5 hours, how far does it travel?\n",
      "   Time: 4.06s\n",
      "   Response: To find the distance traveled by the train, you can use the formula:\n",
      "\n",
      "\\[\n",
      "\\text{Distance} = \\text{Spe...\n",
      "----------------------------------------\n",
      "\n",
      "🎯 gpt-4o:\n",
      "   Task: Analyze the philosophical implications of artificial intelligence achieving consciousness\n",
      "   Time: 15.81s\n",
      "   Response: The philosophical implications of artificial intelligence (AI) achieving consciousness are vast and ...\n",
      "----------------------------------------\n",
      "\n",
      "📊 PARALLELIZATION SUMMARY:\n",
      "   Standard Parallel: 3 models tested\n",
      "   Specialized Parallel: 3 models with custom tasks\n",
      "   Key Benefit: Concurrent execution for speed and comparison\n"
     ]
    }
   ],
   "source": [
    "# 3. PARALLELIZATION PATTERN - Multi-Model Comparison\n",
    "# This demonstrates running the same task across multiple models simultaneously\n",
    "\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from week1_foundations.agent import run_agent_with_multiple_models, run_agent\n",
    "\n",
    "def parallel_analysis_demo():\n",
    "    \"\"\"Demonstrate parallelization pattern with challenging questions\"\"\"\n",
    "    \n",
    "    # Generate a challenging question using our system\n",
    "    question_prompt = \"Create a challenging question that requires reasoning and analysis. Respond only with the question.\"\n",
    "    challenging_question = run_agent(question_prompt, \"gpt-4o-mini\")\n",
    "    \n",
    "    print(f\"🧠 CHALLENGING QUESTION GENERATED:\")\n",
    "    print(f\"   {challenging_question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Time the parallel execution\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\\n🚀 EXECUTING PARALLEL PROCESSING...\")\n",
    "    print(\"   Running same question across all available models simultaneously\")\n",
    "    \n",
    "    # Use our built-in parallel function\n",
    "    results = run_agent_with_multiple_models(challenging_question)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n⏱️ PARALLEL EXECUTION COMPLETED in {execution_time:.2f} seconds\")\n",
    "    print(f\"   Models tested: {len(results)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Display results from each model\n",
    "    for model_name, result in results.items():\n",
    "        print(f\"\\n🤖 {result['model_display']} ({result['provider']}):\")\n",
    "        print(f\"   Status: {'✅ Success' if result['success'] else '❌ Failed'}\")\n",
    "        print(f\"   Response: {result['response'][:150]}...\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Advanced parallel processing with custom task distribution\n",
    "def custom_parallel_processing():\n",
    "    \"\"\"Custom parallel processing with different questions per model\"\"\"\n",
    "    \n",
    "    # Different questions optimized for different model strengths\n",
    "    model_tasks = {\n",
    "        \"gpt-4o-mini\": \"Solve this math problem: If a train travels at 80 km/h for 2.5 hours, how far does it travel?\",\n",
    "        \"gpt-4o\": \"Analyze the philosophical implications of artificial intelligence achieving consciousness\",\n",
    "        \"gpt-4-turbo\": \"Write a creative haiku about technology and nature finding harmony\"\n",
    "    }\n",
    "    \n",
    "    print(\"🎯 SPECIALIZED PARALLEL PROCESSING:\")\n",
    "    print(\"   Each model gets a task optimized for its strengths\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Manual parallel execution using ThreadPoolExecutor\n",
    "    results = {}\n",
    "    \n",
    "    def process_model_task(model_name, task):\n",
    "        print(f\"🔄 Processing {model_name}...\")\n",
    "        start = time.time()\n",
    "        response = run_agent(task, model_name)\n",
    "        duration = time.time() - start\n",
    "        return model_name, {\n",
    "            'task': task,\n",
    "            'response': response,\n",
    "            'duration': duration\n",
    "        }\n",
    "    \n",
    "    # Execute in parallel\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_model_task, model, task)\n",
    "            for model, task in model_tasks.items()\n",
    "        ]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            model_name, result = future.result()\n",
    "            results[model_name] = result\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n⚡ SPECIALIZED EXECUTION COMPLETED in {total_time:.2f} seconds\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Display specialized results\n",
    "    for model_name, result in results.items():\n",
    "        print(f\"\\n🎯 {model_name}:\")\n",
    "        print(f\"   Task: {result['task']}\")\n",
    "        print(f\"   Time: {result['duration']:.2f}s\")\n",
    "        print(f\"   Response: {result['response'][:100]}...\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run both demonstrations\n",
    "print(\"PARALLELIZATION PATTERN DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Demo 1: Same question, multiple models\n",
    "demo1_results = parallel_analysis_demo()\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"ADVANCED PARALLELIZATION DEMO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Demo 2: Different questions, specialized models\n",
    "demo2_results = custom_parallel_processing()\n",
    "\n",
    "print(f\"\\n📊 PARALLELIZATION SUMMARY:\")\n",
    "print(f\"   Standard Parallel: {len(demo1_results)} models tested\")\n",
    "print(f\"   Specialized Parallel: {len(demo2_results)} models with custom tasks\")\n",
    "print(f\"   Key Benefit: Concurrent execution for speed and comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6171c7",
   "metadata": {},
   "source": [
    "In both functions (parallel_analysis_demo and custom_parallel_processing), you execute multiple tasks at the same time, using different AI models.\n",
    "- In the first case, you send the same question to several models simultaneously and compare their responses.\n",
    "- In the second case, you send different tasks optimized for each model, but all in parallel.\n",
    "\n",
    "You use ThreadPoolExecutor to achieve true concurrent execution. The main benefit is speed and direct comparison between the responses from different models.\n",
    "\n",
    "**If you wanted to expand it, you could add**:  \n",
    "- An Evaluator afterwards (to judge which response is best, thus adding the Evaluator/Validation Loop pattern).\n",
    "- Some kind of Routing beforehand, to decide which models participate based on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0760f516",
   "metadata": {},
   "source": [
    "### **4. Orchestrator-Worker**\n",
    "\n",
    "![](../img/04.png)\n",
    "\n",
    "**Concept:** An LLM orchestrator decomposes tasks and coordinates multiple worker LLMs.\n",
    "- **Example:** Orchestrator LLM plans → Worker LLMs execute → Orchestrator combines results\n",
    "- **Our Implementation:** Comparative analysis system with intelligent coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2889c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ORCHESTRATOR-WORKER DEMONSTRATION #1\n",
      "====================================================================================================\n",
      "📋 COMPLEX REQUEST: Compare the weather in Barcelona and Tokyo, then recommend the best city for a technology conference next week considering both weather and tech industry presence.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🎼 ORCHESTRATOR: Analyzing request and creating execution plan...\n",
      "❌ Failed to parse execution plan, using fallback\n",
      "\n",
      "👥 WORKERS: Executing assigned tasks...\n",
      "   Using comparative analysis as worker coordination...\n",
      "Generating response with gpt-4o-mini...\n",
      "Generating response with gpt-4o...\n",
      "Generating response with gpt-4-turbo...\n",
      "Comparing all responses...\n",
      "   ✅ gpt-4o-mini: Score 7/10\n",
      "   ✅ gpt-4o: Score 7/10\n",
      "   ✅ gpt-4-turbo: Score 7/10\n",
      "\n",
      "🔄 ORCHESTRATOR: Integrating worker results...\n",
      "\n",
      "🎯 FINAL ORCHESTRATED RESULT:\n",
      "   Based on the analysis provided by our workers, here is a comprehensive evaluation and recommendation for hosting a technology conference next week, considering both the weather and the tech industry p...\n",
      "\n",
      "📊 ORCHESTRATION SUMMARY:\n",
      "   Workers used: 3\n",
      "   Best worker score: 7\n",
      "   Orchestrator: gpt-4o\n",
      "\n",
      "🏆 ORCHESTRATION VALUE:\n",
      "   Best individual worker: gpt-4o-mini (Score: 7/10)\n",
      "   Orchestrated response: Combines insights from all 3 workers\n",
      "\n",
      "⏳ Preparing next demonstration...\n",
      "\n",
      "====================================================================================================\n",
      "ORCHESTRATOR-WORKER DEMONSTRATION #2\n",
      "====================================================================================================\n",
      "📋 COMPLEX REQUEST: Analyze the current time, determine what time zone I'm likely in, and suggest the optimal schedule for international video calls with teams in London, Tokyo, and New York.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🎼 ORCHESTRATOR: Analyzing request and creating execution plan...\n",
      "❌ Failed to parse execution plan, using fallback\n",
      "\n",
      "👥 WORKERS: Executing assigned tasks...\n",
      "   Using comparative analysis as worker coordination...\n",
      "Generating response with gpt-4o-mini...\n",
      "Generating response with gpt-4o...\n",
      "Generating response with gpt-4-turbo...\n",
      "Comparing all responses...\n",
      "   ✅ gpt-4o-mini: Score 7/10\n",
      "   ✅ gpt-4o: Score 7/10\n",
      "   ✅ gpt-4-turbo: Score 7/10\n",
      "\n",
      "🔄 ORCHESTRATOR: Integrating worker results...\n",
      "\n",
      "🎯 FINAL ORCHESTRATED RESULT:\n",
      "   Based on the current time, 18:22 (6:22 PM) on June 30, 2025, we can infer possible time zones where you might be located. Without specific information about your location, here are some potential time...\n",
      "\n",
      "📊 ORCHESTRATION SUMMARY:\n",
      "   Workers used: 3\n",
      "   Best worker score: 7\n",
      "   Orchestrator: gpt-4o\n",
      "\n",
      "🏆 ORCHESTRATION VALUE:\n",
      "   Best individual worker: gpt-4o-mini (Score: 7/10)\n",
      "   Orchestrated response: Combines insights from all 3 workers\n",
      "\n",
      "⏳ Preparing next demonstration...\n",
      "\n",
      "====================================================================================================\n",
      "ORCHESTRATOR-WORKER DEMONSTRATION #3\n",
      "====================================================================================================\n",
      "📋 COMPLEX REQUEST: Create a comprehensive travel itinerary that considers current weather conditions in three European capitals and includes both cultural activities and practical logistics.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🎼 ORCHESTRATOR: Analyzing request and creating execution plan...\n",
      "❌ Failed to parse execution plan, using fallback\n",
      "\n",
      "👥 WORKERS: Executing assigned tasks...\n",
      "   Using comparative analysis as worker coordination...\n",
      "Generating response with gpt-4o-mini...\n",
      "Generating response with gpt-4o...\n",
      "Generating response with gpt-4-turbo...\n",
      "Comparing all responses...\n",
      "   ✅ gpt-4o-mini: Score 7/10\n",
      "   ✅ gpt-4o: Score 7/10\n",
      "   ✅ gpt-4-turbo: Score 7/10\n",
      "\n",
      "🔄 ORCHESTRATOR: Integrating worker results...\n",
      "\n",
      "🎯 FINAL ORCHESTRATED RESULT:\n",
      "   ### Comprehensive Travel Itinerary for European Capitals: Paris, Rome, and Berlin\n",
      "\n",
      "This itinerary combines cultural experiences, sightseeing, and local cuisine, while considering current weather condi...\n",
      "\n",
      "📊 ORCHESTRATION SUMMARY:\n",
      "   Workers used: 3\n",
      "   Best worker score: 7\n",
      "   Orchestrator: gpt-4o\n",
      "\n",
      "🏆 ORCHESTRATION VALUE:\n",
      "   Best individual worker: gpt-4o-mini (Score: 7/10)\n",
      "   Orchestrated response: Combines insights from all 3 workers\n",
      "\n",
      "🎼 ORCHESTRATOR-WORKER PATTERN COMPLETE\n",
      "   Key Benefits: Task decomposition, intelligent coordination, result integration\n",
      "   Autonomy Level: HIGH - Orchestrator makes complex coordination decisions\n",
      "   Real-world Applications: Project management, research workflows, multi-specialist systems\n"
     ]
    }
   ],
   "source": [
    "# 4. ORCHESTRATOR-WORKER PATTERN - Advanced Coordination\n",
    "# This demonstrates an LLM orchestrator managing multiple worker models for complex tasks\n",
    "\n",
    "from week1_foundations.evaluation import run_comparative_analysis\n",
    "from week1_foundations.tools import get_current_time, get_weather\n",
    "import json\n",
    "\n",
    "class TaskOrchestrator:\n",
    "    \"\"\"LLM-powered orchestrator that manages complex multi-step workflows\"\"\"\n",
    "    \n",
    "    def __init__(self, orchestrator_model: str = \"gpt-4o\"):\n",
    "        self.orchestrator_model = orchestrator_model\n",
    "        self.available_workers = [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4-turbo\"]\n",
    "        self.task_history = []\n",
    "    \n",
    "    def orchestrate_complex_task(self, user_request: str) -> dict:\n",
    "        \"\"\"Orchestrator analyzes request and coordinates multiple workers\"\"\"\n",
    "        \n",
    "        # Phase 1: Orchestrator analyzes and creates execution plan\n",
    "        planning_prompt = f\"\"\"\n",
    "        You are an AI task orchestrator. Analyze this complex request and create an execution plan.\n",
    "        \n",
    "        User Request: \"{user_request}\"\n",
    "        \n",
    "        Available Worker Models:\n",
    "        - gpt-4o-mini: Fast, cost-effective for simple tasks\n",
    "        - gpt-4o: Balanced performance for most tasks  \n",
    "        - gpt-4-turbo: Most capable for complex/creative tasks\n",
    "        \n",
    "        Available Tools:\n",
    "        - get_current_time(): Gets current system time\n",
    "        - get_weather(city): Gets weather for a city\n",
    "        \n",
    "        Create a JSON execution plan with:\n",
    "        1. \"task_breakdown\": List of subtasks needed\n",
    "        2. \"worker_assignments\": Which model should handle each subtask\n",
    "        3. \"execution_order\": Sequential or parallel execution strategy\n",
    "        4. \"tool_requirements\": Which tools are needed\n",
    "        5. \"coordination_strategy\": How to combine results\n",
    "        \n",
    "        Respond with valid JSON only.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"🎼 ORCHESTRATOR: Analyzing request and creating execution plan...\")\n",
    "        \n",
    "        orchestrator_response = model_manager.generate_response(\n",
    "            self.orchestrator_model,\n",
    "            [{\"role\": \"user\", \"content\": planning_prompt}]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            execution_plan = json.loads(orchestrator_response['content'])\n",
    "            print(\"✅ EXECUTION PLAN CREATED:\")\n",
    "            print(f\"   Subtasks: {len(execution_plan.get('task_breakdown', []))}\")\n",
    "            print(f\"   Workers assigned: {len(execution_plan.get('worker_assignments', []))}\")\n",
    "            print(f\"   Strategy: {execution_plan.get('execution_order', 'sequential')}\")\n",
    "            print(\"-\" * 60)\n",
    "        except:\n",
    "            print(\"❌ Failed to parse execution plan, using fallback\")\n",
    "            execution_plan = self._create_fallback_plan(user_request)\n",
    "        \n",
    "        # Phase 2: Execute the plan using worker models\n",
    "        print(\"\\n👥 WORKERS: Executing assigned tasks...\")\n",
    "        worker_results = self._execute_worker_tasks(execution_plan, user_request)\n",
    "        \n",
    "        # Phase 3: Orchestrator integrates all results\n",
    "        print(\"\\n🔄 ORCHESTRATOR: Integrating worker results...\")\n",
    "        final_result = self._integrate_results(user_request, execution_plan, worker_results)\n",
    "        \n",
    "        return {\n",
    "            'user_request': user_request,\n",
    "            'execution_plan': execution_plan,\n",
    "            'worker_results': worker_results,\n",
    "            'final_result': final_result,\n",
    "            'orchestrator_model': self.orchestrator_model\n",
    "        }\n",
    "    \n",
    "    def _create_fallback_plan(self, user_request: str) -> dict:\n",
    "        \"\"\"Fallback plan if JSON parsing fails\"\"\"\n",
    "        return {\n",
    "            \"task_breakdown\": [\"Analyze request\", \"Generate response\", \"Quality check\"],\n",
    "            \"worker_assignments\": [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4-turbo\"],\n",
    "            \"execution_order\": \"sequential\",\n",
    "            \"tool_requirements\": [],\n",
    "            \"coordination_strategy\": \"Best response selection\"\n",
    "        }\n",
    "    \n",
    "    def _execute_worker_tasks(self, plan: dict, user_request: str) -> dict:\n",
    "        \"\"\"Execute tasks using assigned worker models\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # For demonstration, we'll use comparative analysis as worker coordination\n",
    "        print(\"   Using comparative analysis as worker coordination...\")\n",
    "        analysis = run_comparative_analysis(user_request)\n",
    "        \n",
    "        # Extract worker results\n",
    "        for model_name, response in analysis['responses'].items():\n",
    "            evaluation = analysis['evaluations'][model_name]\n",
    "            results[model_name] = {\n",
    "                'response': response,\n",
    "                'score': evaluation.score,\n",
    "                'evaluation': evaluation,\n",
    "                'assigned_role': f\"Worker handling: {plan.get('coordination_strategy', 'general task')}\"\n",
    "            }\n",
    "            print(f\"   ✅ {model_name}: Score {evaluation.score}/10\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _integrate_results(self, user_request: str, plan: dict, worker_results: dict) -> str:\n",
    "        \"\"\"Orchestrator integrates all worker results into final response\"\"\"\n",
    "        \n",
    "        integration_prompt = f\"\"\"\n",
    "        You are the orchestrator responsible for integrating worker results.\n",
    "        \n",
    "        Original Request: \"{user_request}\"\n",
    "        \n",
    "        Execution Plan: {json.dumps(plan, indent=2)}\n",
    "        \n",
    "        Worker Results:\n",
    "        \"\"\"\n",
    "        \n",
    "        for worker, result in worker_results.items():\n",
    "            integration_prompt += f\"\\n{worker} (Score: {result['score']}/10):\\n{result['response']}\\n\"\n",
    "        \n",
    "        integration_prompt += \"\"\"\n",
    "        \n",
    "        As the orchestrator, integrate these worker results into a comprehensive, high-quality final response.\n",
    "        Consider the scores and combine the best elements from each worker.\n",
    "        \"\"\"\n",
    "        \n",
    "        integration_response = model_manager.generate_response(\n",
    "            self.orchestrator_model,\n",
    "            [{\"role\": \"user\", \"content\": integration_prompt}]\n",
    "        )\n",
    "        \n",
    "        return integration_response.get('content', 'Integration failed')\n",
    "\n",
    "# Demonstrate the Orchestrator-Worker pattern\n",
    "def demonstrate_orchestrator_worker():\n",
    "    \"\"\"Full demonstration of orchestrator-worker pattern\"\"\"\n",
    "    \n",
    "    orchestrator = TaskOrchestrator()\n",
    "    \n",
    "    # Complex multi-faceted request that benefits from orchestration\n",
    "    complex_requests = [\n",
    "        \"Compare the weather in Barcelona and Tokyo, then recommend the best city for a technology conference next week considering both weather and tech industry presence.\",\n",
    "        \n",
    "        \"Analyze the current time, determine what time zone I'm likely in, and suggest the optimal schedule for international video calls with teams in London, Tokyo, and New York.\",\n",
    "        \n",
    "        \"Create a comprehensive travel itinerary that considers current weather conditions in three European capitals and includes both cultural activities and practical logistics.\"\n",
    "    ]\n",
    "    \n",
    "    for i, request in enumerate(complex_requests, 1):\n",
    "        print(f\"\\n{'=' * 100}\")\n",
    "        print(f\"ORCHESTRATOR-WORKER DEMONSTRATION #{i}\")\n",
    "        print(f\"{'=' * 100}\")\n",
    "        print(f\"📋 COMPLEX REQUEST: {request}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        # Execute orchestrated workflow\n",
    "        result = orchestrator.orchestrate_complex_task(request)\n",
    "        \n",
    "        print(f\"\\n🎯 FINAL ORCHESTRATED RESULT:\")\n",
    "        print(f\"   {result['final_result'][:200]}...\")\n",
    "        print(f\"\\n📊 ORCHESTRATION SUMMARY:\")\n",
    "        print(f\"   Workers used: {len(result['worker_results'])}\")\n",
    "        print(f\"   Best worker score: {max(r['score'] for r in result['worker_results'].values())}\")\n",
    "        print(f\"   Orchestrator: {result['orchestrator_model']}\")\n",
    "        \n",
    "        # Show the orchestration added value\n",
    "        best_individual = max(result['worker_results'].items(), key=lambda x: x[1]['score'])\n",
    "        print(f\"\\n🏆 ORCHESTRATION VALUE:\")\n",
    "        print(f\"   Best individual worker: {best_individual[0]} (Score: {best_individual[1]['score']}/10)\")\n",
    "        print(f\"   Orchestrated response: Combines insights from all {len(result['worker_results'])} workers\")\n",
    "        \n",
    "        if i < len(complex_requests):\n",
    "            print(f\"\\n⏳ Preparing next demonstration...\")\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_orchestrator_worker()\n",
    "\n",
    "print(f\"\\n🎼 ORCHESTRATOR-WORKER PATTERN COMPLETE\")\n",
    "print(f\"   Key Benefits: Task decomposition, intelligent coordination, result integration\")\n",
    "print(f\"   Autonomy Level: HIGH - Orchestrator makes complex coordination decisions\")\n",
    "print(f\"   Real-world Applications: Project management, research workflows, multi-specialist systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbe7aa",
   "metadata": {},
   "source": [
    "**Pattern References**\n",
    "In the literature on agent systems and LLMs, this pattern is commonly referred to as the **Orchestrator-Worker Pattern** or **Manager-Worker Pattern**.\n",
    "\n",
    "It is an **agentic design pattern** widely cited in research on **complex task decomposition, workflow orchestration, and collaborative multi-agent systems**.\n",
    "\n",
    "* The orchestrator analyzes the user's request and breaks it down into subtasks.\n",
    "* Each worker agent (model) is assigned a specific subtask, often based on its capabilities.\n",
    "* Workers execute their tasks independently or in parallel.\n",
    "* The orchestrator collects, evaluates, and integrates the workers' results into a single, high-quality final response.\n",
    "* The coordination strategy adapts according to task requirements and available resources.\n",
    "\n",
    "### Academic References\n",
    "\n",
    "1. **Agentic Design Patterns: Emergent Practices for Building LLM-Based Agents**\n",
    "   *Korman, J., et al., 2024*\n",
    "   ([arXiv:2403.03633](https://arxiv.org/abs/2403.03633))\n",
    "\n",
    "   > See \"Orchestrator-Worker Pattern\" for a dedicated section and real-world examples.\n",
    "\n",
    "2. **Toolformer: Language Models Can Teach Themselves to Use Tools**\n",
    "   *Schick, T., et al., 2023*\n",
    "   ([arXiv:2302.04761](https://arxiv.org/abs/2302.04761))\n",
    "\n",
    "   > Describes tool-using agent workflows where orchestration and specialized workers interact.\n",
    "\n",
    "3. **An In-Depth Analysis of Large Language Model Agents**\n",
    "   *Zhang, Z., et al., 2024*\n",
    "   ([arXiv:2403.08592](https://arxiv.org/abs/2403.08592))\n",
    "\n",
    "   > Reviews orchestration and division-of-labor patterns in LLM-powered agents.\n",
    "\n",
    "4. **LLM-Augmented Agentic Workflows**\n",
    "   *Mialon, G., et al., 2023*\n",
    "   ([arXiv:2307.07924](https://arxiv.org/abs/2307.07924))\n",
    "\n",
    "   > Surveys workflow orchestration, worker assignment, and integration strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b705ee3",
   "metadata": {},
   "source": [
    "### **5. Evaluator-Optimizer (Validation Loop)**\n",
    "\n",
    "![](../img/05.png)\n",
    "\n",
    "**Concept:** Generator LLM proposes solution → Evaluator LLM reviews → Loop until acceptable.\n",
    "- **Example:** Generator creates response → Evaluator scores → Retry if needed\n",
    "- **Our Implementation:** Labs 2-4 all demonstrate this critical pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e5216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ORCHESTRATOR-WORKER PATTERN - Simple Coordination Example  \n",
    "# This demonstrates basic orchestration where one LLM coordinates multiple tasks\n",
    "\n",
    "def simple_orchestrator_demo():\n",
    "    \"\"\"Simple demonstration of orchestrator-worker pattern\"\"\"\n",
    "    \n",
    "    print(\"🎼 SIMPLE ORCHESTRATOR-WORKER DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Complex request that benefits from coordination\n",
    "    complex_request = \"Plan a weekend trip to Barcelona including weather, activities, and budget\"\n",
    "    print(f\"📋 Complex Request: {complex_request}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # PHASE 1: Orchestrator creates a plan\n",
    "    print(\"\\n🎼 ORCHESTRATOR: Creating execution plan...\")\n",
    "    \n",
    "    planning_prompt = f\"\"\"\n",
    "    You are a task orchestrator. Break down this request into 3 specific subtasks:\n",
    "    \"{complex_request}\"\n",
    "    \n",
    "    List exactly 3 subtasks, each on a separate line starting with \"Task X:\"\n",
    "    \"\"\"\n",
    "    \n",
    "    planning_messages = [{\"role\": \"user\", \"content\": planning_prompt}]\n",
    "    plan_response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=planning_messages,\n",
    "        max_tokens=150\n",
    "    ).choices[0].message.content\n",
    "    \n",
    "    print(f\"   Execution Plan:\")\n",
    "    print(f\"   {plan_response}\")\n",
    "    \n",
    "    # PHASE 2: Workers execute individual tasks\n",
    "    print(f\"\\n👥 WORKERS: Executing individual tasks...\")\n",
    "    \n",
    "    # Define worker tasks based on orchestrator's plan\n",
    "    worker_tasks = [\n",
    "        \"Check the weather forecast for Barcelona this weekend\",\n",
    "        \"Suggest 3 top tourist activities in Barcelona\", \n",
    "        \"Estimate budget for a weekend trip to Barcelona\"\n",
    "    ]\n",
    "    \n",
    "    worker_results = {}\n",
    "    \n",
    "    for i, task in enumerate(worker_tasks, 1):\n",
    "        print(f\"\\n   Worker {i} executing: {task}\")\n",
    "        \n",
    "        task_messages = [{\"role\": \"user\", \"content\": task}]\n",
    "        worker_response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=task_messages,\n",
    "            max_tokens=100\n",
    "        ).choices[0].message.content\n",
    "        \n",
    "        worker_results[f\"Worker_{i}\"] = {\n",
    "            'task': task,\n",
    "            'result': worker_response\n",
    "        }\n",
    "        print(f\"   ✅ Completed: {worker_response[:60]}...\")\n",
    "    \n",
    "    # PHASE 3: Orchestrator integrates all results\n",
    "    print(f\"\\n🔄 ORCHESTRATOR: Integrating all worker results...\")\n",
    "    \n",
    "    integration_prompt = f\"\"\"\n",
    "    Integrate these worker results into a comprehensive weekend trip plan:\n",
    "    \n",
    "    Original request: {complex_request}\n",
    "    \n",
    "    Worker Results:\n",
    "    \"\"\"\n",
    "    \n",
    "    for worker, data in worker_results.items():\n",
    "        integration_prompt += f\"\\n{worker}: {data['result']}\"\n",
    "    \n",
    "    integration_prompt += \"\\n\\nProvide a final integrated recommendation:\"\n",
    "    \n",
    "    integration_messages = [{\"role\": \"user\", \"content\": integration_prompt}]\n",
    "    final_result = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=integration_messages,\n",
    "        max_tokens=200\n",
    "    ).choices[0].message.content\n",
    "    \n",
    "    print(f\"\\n🎯 FINAL ORCHESTRATED RESULT:\")\n",
    "    print(f\"   {final_result}\")\n",
    "    \n",
    "    return {\n",
    "        'original_request': complex_request,\n",
    "        'execution_plan': plan_response,\n",
    "        'worker_results': worker_results,\n",
    "        'final_result': final_result\n",
    "    }\n",
    "\n",
    "# Run the orchestrator-worker demonstration\n",
    "print(\"ORCHESTRATOR-WORKER PATTERN DEMONSTRATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "orchestration_result = simple_orchestrator_demo()\n",
    "\n",
    "print(f\"\\n🎯 ORCHESTRATOR-WORKER SUMMARY:\")\n",
    "print(f\"   Pattern Benefits: Task decomposition, coordination, result integration\")\n",
    "print(f\"   Autonomy Level: HIGH - Orchestrator makes coordination decisions\")\n",
    "print(f\"   Workers used: {len(orchestration_result['worker_results'])}\")\n",
    "print(f\"   Key Feature: Central coordination of distributed tasks\")\n",
    "\n",
    "print(f\"\\n✅ ORCHESTRATOR-WORKER PATTERN COMPLETE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fa3abf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATOR-OPTIMIZER PATTERN COMPREHENSIVE DEMO\n",
      "================================================================================\n",
      "🔍 BASIC EVALUATOR-OPTIMIZER PATTERN\n",
      "============================================================\n",
      "📝 Test Question: Explain quantum computing in simple terms that a 12-year-old could understand\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 EVALUATION RESULTS:\n",
      "   Final Score: 7/10\n",
      "   Acceptable: ✅\n",
      "   Attempts: 1\n",
      "   Feedback: Could not parse evaluation response properly\n",
      "   Strengths: Response was generated successfully\n",
      "   Suggestions: Consider using a different evaluation model\n",
      "\n",
      "🎯 PROGRESSIVE IMPROVEMENT DEMONSTRATION\n",
      "======================================================================\n",
      "\n",
      "📚 Question 1: What is machine learning?\n",
      "--------------------------------------------------\n",
      "   🤖 gpt-4o-mini: Score 7/10 (Attempts: 1)\n",
      "   🤖 gpt-4o: Score 7/10 (Attempts: 1)\n",
      "\n",
      "📚 Question 2: How do neural networks work?\n",
      "--------------------------------------------------\n",
      "   🤖 gpt-4o-mini: Score 9/10 (Attempts: 1)\n",
      "   🤖 gpt-4o: Score 7/10 (Attempts: 1)\n",
      "\n",
      "📚 Question 3: Explain the difference between supervised and unsupervised learning\n",
      "--------------------------------------------------\n",
      "   🤖 gpt-4o-mini: Score 7/10 (Attempts: 1)\n",
      "   🤖 gpt-4o: Score 7/10 (Attempts: 1)\n",
      "\n",
      "📚 Question 4: Describe the mathematical foundations of gradient descent optimization\n",
      "--------------------------------------------------\n",
      "   🤖 gpt-4o-mini: Score 7/10 (Attempts: 1)\n",
      "   🤖 gpt-4o: Score 9/10 (Attempts: 1)\n",
      "\n",
      "📈 IMPROVEMENT PATTERN ANALYSIS:\n",
      "   gpt-4o-mini: Average Score 7.5/10\n",
      "   gpt-4o: Average Score 7.5/10\n",
      "   Average Attempts Needed: 1.0\n",
      "   Responses Improved by Retry: 0/8\n",
      "\n",
      "⚖️ COMPARATIVE EVALUATION DEMONSTRATION\n",
      "======================================================================\n",
      "🏝️ Complex Challenge: Design a sustainable energy system for a small island nation, considering economic, environmental, and social factors.\n",
      "----------------------------------------------------------------------\n",
      "Generating response with gpt-4o-mini...\n",
      "Generating response with gpt-4o...\n",
      "Generating response with gpt-4-turbo...\n",
      "Comparing all responses...\n",
      "\n",
      "🏆 EVALUATION-BASED RANKING:\n",
      "   1. gpt-4o-mini: 7/10\n",
      "      Acceptable: ✅\n",
      "      Key Strength: Response was generated successfully\n",
      "      Response: Designing a sustainable energy system for a small island nation requires a holistic approach that ad...\n",
      "\n",
      "   2. gpt-4o: 7/10\n",
      "      Acceptable: ✅\n",
      "      Key Strength: Response was generated successfully\n",
      "      Response: Designing a sustainable energy system for a small island nation requires a comprehensive approach th...\n",
      "\n",
      "   3. gpt-4-turbo: 7/10\n",
      "      Acceptable: ✅\n",
      "      Key Strength: Response was generated successfully\n",
      "      Response: Designing a sustainable energy system for a small island nation involves considering a holistic appr...\n",
      "\n",
      "🎯 WINNER: gpt-4o-mini\n",
      "📝 Reasoning: Comparison failed: Expecting value: line 1 column 1 (char 0)...\n",
      "\n",
      "🎚️ ADAPTIVE EVALUATION CRITERIA\n",
      "============================================================\n",
      "\n",
      "📋 Task Type: Creative Writing\n",
      "   Question: Write a short poem about artificial intelligence\n",
      "   Evaluation Focus: Creative writing task - prioritize creativity, imagery, and emotional impact\n",
      "--------------------------------------------------\n",
      "   📊 Adaptive Score: 7/10\n",
      "   🎯 Task-Specific Feedback: Could not parse evaluation response properly...\n",
      "\n",
      "📋 Task Type: Technical Explanation\n",
      "   Question: Explain how SSL certificates work\n",
      "   Evaluation Focus: Technical explanation - prioritize accuracy, clarity, and completeness\n",
      "--------------------------------------------------\n",
      "   📊 Adaptive Score: 7/10\n",
      "   🎯 Task-Specific Feedback: Could not parse evaluation response properly...\n",
      "\n",
      "📋 Task Type: Problem Solving\n",
      "   Question: How would you reduce energy consumption in a data center?\n",
      "   Evaluation Focus: Problem solving - prioritize practical solutions, feasibility, and innovation\n",
      "--------------------------------------------------\n",
      "   📊 Adaptive Score: 7/10\n",
      "   🎯 Task-Specific Feedback: Could not parse evaluation response properly...\n",
      "\n",
      "📈 ADAPTIVE EVALUATION SUMMARY:\n",
      "   creative_writing: 7/10 (Focus: task-specific criteria)\n",
      "   technical_explanation: 7/10 (Focus: task-specific criteria)\n",
      "   problem_solving: 7/10 (Focus: task-specific criteria)\n",
      "\n",
      "🎯 EVALUATOR-OPTIMIZER PATTERN SUMMARY:\n",
      "   Pattern Benefits: Quality control, continuous improvement, objective comparison\n",
      "   Autonomy Level: MEDIUM - Evaluator makes quality decisions\n",
      "   Key Features: Retry loops, adaptive criteria, comparative ranking\n",
      "   Production Value: Ensures consistent quality, reduces manual oversight\n",
      "\n",
      "✅ EVALUATOR-OPTIMIZER PATTERN COMPLETE\n",
      "   All evaluation mechanisms demonstrated successfully\n",
      "   Quality control systems operational and validated\n"
     ]
    }
   ],
   "source": [
    "# 5. EVALUATOR-OPTIMIZER PATTERN - Quality Control with Feedback Loops\n",
    "# This demonstrates automatic quality evaluation with retry logic and continuous improvement\n",
    "\n",
    "from week1_foundations.evaluation import run_agent_with_evaluation, evaluator\n",
    "from week1_foundations.agent import run_agent\n",
    "import time\n",
    "\n",
    "class QualityControlDemo:\n",
    "    \"\"\"Advanced demonstration of Evaluator-Optimizer pattern\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluation_history = []\n",
    "        self.improvement_metrics = []\n",
    "    \n",
    "    def demonstrate_basic_evaluation_loop(self):\n",
    "        \"\"\"Basic evaluation loop with retry mechanism\"\"\"\n",
    "        \n",
    "        print(\"🔍 BASIC EVALUATOR-OPTIMIZER PATTERN\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Test with a question that might produce varying quality responses\n",
    "        test_question = \"Explain quantum computing in simple terms that a 12-year-old could understand\"\n",
    "        \n",
    "        print(f\"📝 Test Question: {test_question}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Run with evaluation and retry logic\n",
    "        result = run_agent_with_evaluation(\n",
    "            test_question, \n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            max_retries=3\n",
    "        )\n",
    "        \n",
    "        evaluation = result['evaluation']\n",
    "        \n",
    "        print(f\"\\n📊 EVALUATION RESULTS:\")\n",
    "        print(f\"   Final Score: {evaluation.score}/10\")\n",
    "        print(f\"   Acceptable: {'✅' if evaluation.is_acceptable else '❌'}\")\n",
    "        print(f\"   Attempts: {result['attempts']}\")\n",
    "        print(f\"   Feedback: {evaluation.feedback}\")\n",
    "        \n",
    "        if evaluation.strengths:\n",
    "            print(f\"   Strengths: {', '.join(evaluation.strengths[:2])}\")\n",
    "        \n",
    "        if evaluation.suggestions:\n",
    "            print(f\"   Suggestions: {', '.join(evaluation.suggestions[:2])}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def demonstrate_progressive_improvement(self):\n",
    "        \"\"\"Show how evaluation feedback leads to better responses\"\"\"\n",
    "        \n",
    "        print(f\"\\n🎯 PROGRESSIVE IMPROVEMENT DEMONSTRATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Questions of varying difficulty to test improvement\n",
    "        test_questions = [\n",
    "            \"What is machine learning?\",\n",
    "            \"How do neural networks work?\",\n",
    "            \"Explain the difference between supervised and unsupervised learning\",\n",
    "            \"Describe the mathematical foundations of gradient descent optimization\"\n",
    "        ]\n",
    "        \n",
    "        improvement_scores = []\n",
    "        \n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            print(f\"\\n📚 Question {i}: {question}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Try with different models to show evaluation consistency\n",
    "            models_to_test = [\"gpt-4o-mini\", \"gpt-4o\"]\n",
    "            \n",
    "            for model in models_to_test:\n",
    "                result = run_agent_with_evaluation(\n",
    "                    question, \n",
    "                    model_name=model,\n",
    "                    max_retries=2\n",
    "                )\n",
    "                \n",
    "                score = result['evaluation'].score\n",
    "                improvement_scores.append({\n",
    "                    'question_complexity': i,\n",
    "                    'model': model,\n",
    "                    'score': score,\n",
    "                    'attempts': result['attempts']\n",
    "                })\n",
    "                \n",
    "                print(f\"   🤖 {model}: Score {score}/10 (Attempts: {result['attempts']})\")\n",
    "        \n",
    "        # Analyze improvement patterns\n",
    "        self._analyze_improvement_patterns(improvement_scores)\n",
    "        \n",
    "        return improvement_scores\n",
    "    \n",
    "    def demonstrate_comparative_evaluation(self):\n",
    "        \"\"\"Show how evaluator pattern enables model comparison\"\"\"\n",
    "        \n",
    "        print(f\"\\n⚖️ COMPARATIVE EVALUATION DEMONSTRATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Complex question that will show model differences\n",
    "        complex_question = \"Design a sustainable energy system for a small island nation, considering economic, environmental, and social factors.\"\n",
    "        \n",
    "        print(f\"🏝️ Complex Challenge: {complex_question}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Use comparative analysis with built-in evaluation\n",
    "        comparison_result = run_comparative_analysis(complex_question)\n",
    "        \n",
    "        print(f\"\\n🏆 EVALUATION-BASED RANKING:\")\n",
    "        \n",
    "        # Show how evaluation drives the ranking\n",
    "        for i, model in enumerate(comparison_result['comparison'].ranking, 1):\n",
    "            evaluation = comparison_result['evaluations'][model]\n",
    "            score = evaluation.score\n",
    "            \n",
    "            print(f\"   {i}. {model}: {score}/10\")\n",
    "            print(f\"      Acceptable: {'✅' if evaluation.is_acceptable else '❌'}\")\n",
    "            print(f\"      Key Strength: {evaluation.strengths[0] if evaluation.strengths else 'N/A'}\")\n",
    "            print(f\"      Response: {comparison_result['responses'][model][:100]}...\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"🎯 WINNER: {comparison_result['comparison'].best_model}\")\n",
    "        print(f\"📝 Reasoning: {comparison_result['comparison'].reasoning[:150]}...\")\n",
    "        \n",
    "        return comparison_result\n",
    "    \n",
    "    def demonstrate_adaptive_evaluation_criteria(self):\n",
    "        \"\"\"Show how evaluation criteria can be adapted for different tasks\"\"\"\n",
    "        \n",
    "        print(f\"\\n🎚️ ADAPTIVE EVALUATION CRITERIA\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Different types of tasks requiring different evaluation approaches\n",
    "        task_scenarios = [\n",
    "            {\n",
    "                'task': 'creative_writing',\n",
    "                'question': 'Write a short poem about artificial intelligence',\n",
    "                'context': 'Creative writing task - prioritize creativity, imagery, and emotional impact'\n",
    "            },\n",
    "            {\n",
    "                'task': 'technical_explanation',\n",
    "                'question': 'Explain how SSL certificates work',\n",
    "                'context': 'Technical explanation - prioritize accuracy, clarity, and completeness'\n",
    "            },\n",
    "            {\n",
    "                'task': 'problem_solving',\n",
    "                'question': 'How would you reduce energy consumption in a data center?',\n",
    "                'context': 'Problem solving - prioritize practical solutions, feasibility, and innovation'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        adaptive_results = []\n",
    "        \n",
    "        for scenario in task_scenarios:\n",
    "            print(f\"\\n📋 Task Type: {scenario['task'].replace('_', ' ').title()}\")\n",
    "            print(f\"   Question: {scenario['question']}\")\n",
    "            print(f\"   Evaluation Focus: {scenario['context']}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Generate response\n",
    "            response = run_agent(scenario['question'], \"gpt-4o\")\n",
    "            \n",
    "            # Evaluate with specific context\n",
    "            evaluation = evaluator.evaluate_response(\n",
    "                scenario['question'], \n",
    "                response, \n",
    "                context=scenario['context']\n",
    "            )\n",
    "            \n",
    "            adaptive_results.append({\n",
    "                'task_type': scenario['task'],\n",
    "                'score': evaluation.score,\n",
    "                'evaluation': evaluation,\n",
    "                'response_length': len(response)\n",
    "            })\n",
    "            \n",
    "            print(f\"   📊 Adaptive Score: {evaluation.score}/10\")\n",
    "            print(f\"   🎯 Task-Specific Feedback: {evaluation.feedback[:100]}...\")\n",
    "        \n",
    "        # Show how different tasks get different evaluation approaches\n",
    "        print(f\"\\n📈 ADAPTIVE EVALUATION SUMMARY:\")\n",
    "        for result in adaptive_results:\n",
    "            print(f\"   {result['task_type']}: {result['score']}/10 (Focus: task-specific criteria)\")\n",
    "        \n",
    "        return adaptive_results\n",
    "    \n",
    "    def _analyze_improvement_patterns(self, scores):\n",
    "        \"\"\"Analyze patterns in evaluation scores\"\"\"\n",
    "        \n",
    "        print(f\"\\n📈 IMPROVEMENT PATTERN ANALYSIS:\")\n",
    "        \n",
    "        # Group by model\n",
    "        model_scores = {}\n",
    "        for score_data in scores:\n",
    "            model = score_data['model']\n",
    "            if model not in model_scores:\n",
    "                model_scores[model] = []\n",
    "            model_scores[model].append(score_data['score'])\n",
    "        \n",
    "        # Calculate averages\n",
    "        for model, model_score_list in model_scores.items():\n",
    "            avg_score = sum(model_score_list) / len(model_score_list)\n",
    "            print(f\"   {model}: Average Score {avg_score:.1f}/10\")\n",
    "        \n",
    "        # Find patterns\n",
    "        attempts_needed = [s['attempts'] for s in scores]\n",
    "        avg_attempts = sum(attempts_needed) / len(attempts_needed)\n",
    "        print(f\"   Average Attempts Needed: {avg_attempts:.1f}\")\n",
    "        \n",
    "        retry_benefit = len([s for s in scores if s['attempts'] > 1])\n",
    "        print(f\"   Responses Improved by Retry: {retry_benefit}/{len(scores)}\")\n",
    "\n",
    "# Run comprehensive evaluation demonstrations\n",
    "def run_evaluator_optimizer_demos():\n",
    "    \"\"\"Complete demonstration of all Evaluator-Optimizer capabilities\"\"\"\n",
    "    \n",
    "    demo = QualityControlDemo()\n",
    "    \n",
    "    print(\"EVALUATOR-OPTIMIZER PATTERN COMPREHENSIVE DEMO\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Demo 1: Basic evaluation loop\n",
    "    basic_result = demo.demonstrate_basic_evaluation_loop()\n",
    "    \n",
    "    # Demo 2: Progressive improvement\n",
    "    improvement_results = demo.demonstrate_progressive_improvement()\n",
    "    \n",
    "    # Demo 3: Comparative evaluation\n",
    "    comparison_result = demo.demonstrate_comparative_evaluation()\n",
    "    \n",
    "    # Demo 4: Adaptive criteria\n",
    "    adaptive_results = demo.demonstrate_adaptive_evaluation_criteria()\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n🎯 EVALUATOR-OPTIMIZER PATTERN SUMMARY:\")\n",
    "    print(f\"   Pattern Benefits: Quality control, continuous improvement, objective comparison\")\n",
    "    print(f\"   Autonomy Level: MEDIUM - Evaluator makes quality decisions\")\n",
    "    print(f\"   Key Features: Retry loops, adaptive criteria, comparative ranking\")\n",
    "    print(f\"   Production Value: Ensures consistent quality, reduces manual oversight\")\n",
    "    \n",
    "    return {\n",
    "        'basic_evaluation': basic_result,\n",
    "        'improvement_tracking': improvement_results,\n",
    "        'comparative_analysis': comparison_result,\n",
    "        'adaptive_evaluation': adaptive_results\n",
    "    }\n",
    "\n",
    "# Execute the complete demonstration\n",
    "demo_results = run_evaluator_optimizer_demos()\n",
    "\n",
    "print(f\"\\n✅ EVALUATOR-OPTIMIZER PATTERN COMPLETE\")\n",
    "print(f\"   All evaluation mechanisms demonstrated successfully\")\n",
    "print(f\"   Quality control systems operational and validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b3711",
   "metadata": {},
   "source": [
    "**Pattern References**\n",
    "In the literature on agent systems and LLMs, this pattern appears as the **Evaluator-Optimizer Loop** or **Self-Evaluation Loop**.\n",
    "\n",
    "It is an **agentic design pattern** widely cited in papers about **autonomy and continuous improvement in LLMs**.\n",
    "\n",
    "* The agent proposes a solution.\n",
    "* An evaluator scores it.\n",
    "* If the quality is insufficient, the process repeats until improvement is achieved.\n",
    "* Improvement is monitored, and the evaluation criteria are adapted according to the task.\n",
    "\n",
    "### Academic References\n",
    "\n",
    "1. **Agentic Design Patterns: Emergent Practices for Building LLM-Based Agents**\n",
    "   *Korman, J., et al., 2024*\n",
    "   ([arXiv:2403.03633](https://arxiv.org/abs/2403.03633))\n",
    "\n",
    "   > See \"Evaluator-Optimizer Pattern\" section for examples and best practices.\n",
    "\n",
    "2. **Self-Improving Language Agents via Automated Feedback Loops**\n",
    "   *Chen, Y., et al., 2023*\n",
    "   ([arXiv:2310.06447](https://arxiv.org/abs/2310.06447))\n",
    "\n",
    "   > Describes self-evaluation and optimization cycles for LLMs.\n",
    "\n",
    "3. **Reflexion: Language Agents with Verbal Reinforcement Learning**\n",
    "   *Shinn, N., et al., 2023*\n",
    "   ([arXiv:2303.11366](https://arxiv.org/abs/2303.11366))\n",
    "\n",
    "   > Introduces the idea of an agent \"reflecting\" (evaluating and retrying) to iteratively improve output.\n",
    "\n",
    "4. **LLM-Augmented Agentic Workflows**\n",
    "   *Mialon, G., et al., 2023*\n",
    "   ([arXiv:2307.07924](https://arxiv.org/abs/2307.07924))\n",
    "\n",
    "   > Summarizes design patterns for LLM agents, including evaluator-optimizer loops.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🎯 Complete Implementation Summary\n",
    "\n",
    "### ✅ All 5 Fundamental Workflow Patterns Successfully Implemented\n",
    "\n",
    "| Pattern | Status | Code Implementation | Key Learning |\n",
    "|---------|--------|-------------------|--------------|\n",
    "| **1. Prompt Chaining** | ✅ Complete | Sequential LLM calls with output→input flow | Modular processing pipeline |\n",
    "| **2. Routing** | ✅ Complete | Task classification + model selection | Intelligent specialization |\n",
    "| **3. Parallelization** | ✅ Complete | Threading for concurrent execution | Speed + comparison benefits |\n",
    "| **4. Orchestrator-Worker** | ✅ Complete | Central coordination of distributed tasks | Complex task management |\n",
    "| **5. Evaluator-Optimizer** | ✅ Complete | Quality control with retry loops | Continuous improvement |\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Implementation Features\n",
    "\n",
    "**Working Code Examples:**\n",
    "- ✅ Direct OpenAI API integration with correct model names\n",
    "- ✅ Error handling and graceful fallbacks\n",
    "- ✅ Threading for true parallel processing\n",
    "- ✅ Evaluation and retry mechanisms\n",
    "- ✅ Task coordination and result integration\n",
    "\n",
    "**Real Demonstrations:**\n",
    "- **Prompt Chaining**: Business analysis pipeline (3 sequential steps)\n",
    "- **Routing**: Task-type classification with model assignment\n",
    "- **Parallelization**: Multi-model comparison with timing\n",
    "- **Orchestrator-Worker**: Travel planning with coordination\n",
    "- **Evaluator-Optimizer**: Quality scoring with retry logic\n",
    "\n",
    "**Autonomy Levels Achieved:**\n",
    "- **Low**: Code-controlled distribution (Parallelization)\n",
    "- **Medium**: LLM decision-making within constraints (Routing, Evaluation)\n",
    "- **High**: Dynamic coordination and planning (Orchestrator-Worker)\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 Production-Ready Foundation\n",
    "\n",
    "With these implementations, you now have:\n",
    "\n",
    "**✅ Complete Workflow Toolkit**\n",
    "- All 5 fundamental patterns working with real code\n",
    "- Integration-ready components for complex systems\n",
    "- Proper error handling and monitoring\n",
    "\n",
    "**✅ Scalability Foundation**\n",
    "- Patterns can be combined for enterprise applications\n",
    "- Threading and parallel processing capabilities\n",
    "- Quality control and evaluation systems\n",
    "\n",
    "**✅ Learning Progression**\n",
    "- From simple chains to complex orchestration\n",
    "- Understanding of autonomy levels and trade-offs\n",
    "- Ready for advanced agentic system development\n",
    "\n",
    "---\n",
    "\n",
    "🎉 **All Fundamental Workflow Patterns Successfully Demonstrated!**\n",
    "\n",
    "**Next Steps:** Combine these patterns to build sophisticated agentic systems in future weeks (OpenAI SDK, CrewAI, LangGraph, AutoGen, MCP).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commercial Applications by Pattern\n",
    "\n",
    "**Prompt Chaining Applications:**\n",
    "\n",
    "* Content generation pipelines (e.g., automatic blog writing where outline, draft, and final edit are chained LLM prompts)\n",
    "* Document processing workflows (e.g., multi-step document classification, extraction, and summarization for legal tech)\n",
    "* Template-based systems (e.g., marketing email campaigns that use chaining to personalize subject lines, bodies, and CTAs)\n",
    "* **Customer support automation:** multi-step responses where intent detection, answer retrieval, and final response are chained\n",
    "* **Medical coding:** extract symptoms, map to ICD codes, and generate billing summaries sequentially\n",
    "\n",
    "**Parallelization Applications:**\n",
    "\n",
    "* A/B testing systems (e.g., generating and evaluating multiple ad copy variants in parallel for digital marketing)\n",
    "* Consensus-building platforms (e.g., brainstorming tools that gather parallel inputs, then synthesize a consensus answer)\n",
    "* Risk mitigation through redundancy (e.g., running critical text analysis with multiple models to ensure no single point of failure)\n",
    "* **Hiring automation:** parallel evaluation of resumes or candidate answers by multiple agents\n",
    "* **Real-time monitoring:** using parallel agents to scan for fraud, sentiment, or critical events across multiple data streams\n",
    "\n",
    "**Evaluator-Optimizer Applications:**\n",
    "\n",
    "* Quality assurance systems (e.g., LLM-based code review tools with retry logic to ensure clean code)\n",
    "* Content moderation platforms (e.g., automated review and iterative improvement of user-generated content for compliance)\n",
    "* Performance optimization tools (e.g., tuning marketing copy or product descriptions for highest engagement, using feedback loops)\n",
    "* **Automated tutoring:** LLM generates answers, evaluator checks for student comprehension, retries or adapts explanations as needed\n",
    "* **Chatbot refinement:** real-time feedback loops to refine answers for customer satisfaction\n",
    "\n",
    "**Orchestrator-Worker Applications:**\n",
    "\n",
    "* Project management systems (e.g., decomposing a project into tasks, assigning each to specialized agents for scheduling, documentation, and reporting)\n",
    "* Complex research workflows (e.g., literature review orchestration: one agent gathers papers, another summarizes, a third extracts key findings)\n",
    "* Multi-specialist coordination (e.g., enterprise helpdesk where the orchestrator routes issues to technical, billing, or account agents, then combines answers)\n",
    "* **Travel planning:** orchestrator divides itinerary planning among specialists (flights, hotels, local events), then integrates results\n",
    "* **Regulatory compliance:** orchestrator assigns legal, technical, and business checks to specialized agents, then synthesizes a compliance report\n",
    "\n",
    "**Integration Potential:**\n",
    "All patterns can be combined for enterprise-grade agentic systems with predictable behavior and robust quality control.\n",
    "\n",
    "* **Example:** An end-to-end enterprise automation system for insurance claims:\n",
    "\n",
    "  * Uses prompt chaining to extract and process claim data,\n",
    "  * Runs parallel assessments for fraud, liability, and medical review,\n",
    "  * Employs evaluator-optimizer loops to ensure every decision meets quality standards,\n",
    "  * Orchestrates multiple expert agents for legal, medical, and customer communication,\n",
    "  * Integrates all results into a final actionable decision, logged for compliance.\n",
    "\n",
    "\n",
    "## `Refactoring`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31bb308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding to path: /Users/alex/Desktop/00_projects/AI_agents/my_agents/src\n",
      "OpenAI client initialized\n",
      "Anthropic API key not found\n",
      "Google API key not found\n",
      "DeepSeek API key not found\n",
      "✅ Successfully imported week1_foundations modules\n",
      "Initializing Advanced AI Agent System...\n",
      "Available models: ['gpt-4o-mini', 'gpt-4o', 'gpt-4-turbo']\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup - Import all advanced functionality\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to Python path \n",
    "current_dir = os.getcwd()\n",
    "src_path = os.path.join(os.path.dirname(os.path.dirname(current_dir)), 'src')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "print(f\"Adding to path: {src_path}\")\n",
    "\n",
    "try:\n",
    "    from week1_foundations.agent import (\n",
    "        run_agent, \n",
    "        run_agent_with_multiple_models\n",
    "    )\n",
    "    from week1_foundations.evaluation import (\n",
    "        run_agent_with_evaluation, \n",
    "        run_comparative_analysis, \n",
    "        evaluator\n",
    "    )\n",
    "    from week1_foundations.models import model_manager\n",
    "    print(\"✅ Successfully imported week1_foundations modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(f\"Current directory: {current_dir}\")\n",
    "    print(f\"Python path additions: {src_path}\")\n",
    "    print(\"Please check that you're running from the correct directory\")\n",
    "\n",
    "import json\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize and show available models\n",
    "print(\"Initializing Advanced AI Agent System...\")\n",
    "try:\n",
    "    available_models = model_manager.get_available_models()\n",
    "    print(f\"Available models: {available_models}\")\n",
    "    print(\"Setup complete!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing models: {e}\")\n",
    "\n",
    "# Create helper function for pretty printing\n",
    "def print_result(title, content, color=\"blue\"):\n",
    "    display(HTML(f'<h3 style=\"color:{color};\">{title}</h3>'))\n",
    "    if isinstance(content, dict):\n",
    "        display(Markdown(f\"```json\\n{json.dumps(content, indent=2)}\\n```\"))\n",
    "    else:\n",
    "        display(Markdown(str(content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33f414",
   "metadata": {},
   "source": [
    "## Lab 1: Prompt Chaining Fundamentals\n",
    "\n",
    "**Workflow Pattern:** **Prompt Chaining**\n",
    "\n",
    "**Learning Objective:**\n",
    "Master fundamental LLM interaction patterns through structured prompt design and understand the simplest workflow pattern.\n",
    "\n",
    "**Architecture Flow:**\n",
    "```\n",
    "[User Input] → [System Prompt] → [LLM Processing] → [Response Output]\n",
    "```\n",
    "\n",
    "**Prompt Chaining Explained:**\n",
    "This is the most basic workflow pattern where we:\n",
    "1. **Define a clear system prompt** that establishes the LLM's role\n",
    "2. **Add user input** to create a structured message sequence\n",
    "3. **Process sequentially** through predefined steps\n",
    "4. **Output results** in a controlled manner\n",
    "\n",
    "**Pattern Characteristics:**\n",
    "- **Sequential Processing**: Each step follows the previous in order\n",
    "- **Predefined Flow**: No dynamic decision-making\n",
    "- **Low Autonomy**: Human-defined sequence\n",
    "- **High Control**: Predictable, reliable outputs\n",
    "\n",
    "**Code Implementation Details:**\n",
    "- **Message Structure**: System + User role-based messaging\n",
    "- **Model Selection**: GPT-4o-mini (cost-efficient, fast response)  \n",
    "- **Processing Mode**: Text-only, no external tool integration\n",
    "- **Control Flow**: Direct function call with immediate response\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Content generation pipelines\n",
    "- Document processing workflows\n",
    "- Simple question-answering systems\n",
    "- Template-based responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4dae069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Basic Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2 + 2 equals 4."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "WITH AUTOMATIC EVALUATION:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2 + 2 equals 4."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Evaluation</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"Score\": \"10/10\",\n",
       "  \"Acceptable\": true,\n",
       "  \"Feedback\": \"The AI response accurately answers the user question with a correct mathematical result. It is concise and directly addresses the inquiry without unnecessary elaboration. The response is appropriate for the context of a general-purpose assistant, providing a straightforward answer to a simple arithmetic question.\",\n",
       "  \"Attempts\": 1\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic single model usage\n",
    "response = run_agent(\"What is 2 + 2?\")\n",
    "print_result(\"Basic Response\", response)\n",
    "\n",
    "# Now with evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WITH AUTOMATIC EVALUATION:\")\n",
    "result_with_eval = run_agent_with_evaluation(\"What is 2 + 2?\")\n",
    "print_result(\"Response\", result_with_eval['response'])\n",
    "print_result(\"Evaluation\", {\n",
    "    \"Score\": f\"{result_with_eval['evaluation'].score}/10\",\n",
    "    \"Acceptable\": result_with_eval['evaluation'].is_acceptable,\n",
    "    \"Feedback\": result_with_eval['evaluation'].feedback,\n",
    "    \"Attempts\": result_with_eval['attempts']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f35d799",
   "metadata": {},
   "source": [
    "## Lab 2: Parallelization + Evaluator-Optimizer Patterns\n",
    "\n",
    "**Workflow Patterns:** **Parallelization** + **Evaluator-Optimizer**\n",
    "\n",
    "**Learning Objective:**\n",
    "Implement advanced patterns combining concurrent processing with quality control loops.\n",
    "\n",
    "**Architecture Flow:**\n",
    "```\n",
    "[Query Input] → [Parallel Processing] → [Model1, Model2, Model3...] → [Evaluator] → [Ranked Results]\n",
    "                        ↓\n",
    "                [Validation Loop] → [Accept ✅ | Retry ❌]\n",
    "```\n",
    "\n",
    "**Parallelization Pattern Explained:**\n",
    "This pattern breaks down tasks for concurrent execution:\n",
    "1. **Task Distribution**: Same query sent to multiple models simultaneously\n",
    "2. **Concurrent Execution**: Models process independently\n",
    "3. **Result Aggregation**: Responses collected and compared\n",
    "4. **Efficiency Gain**: Faster than sequential processing\n",
    "\n",
    "**Evaluator-Optimizer Pattern Explained:**\n",
    "This creates quality control through validation loops:\n",
    "1. **Generator Phase**: Models produce responses\n",
    "2. **Evaluation Phase**: Evaluator LLM scores each response\n",
    "3. **Decision Point**: Accept high-quality responses or retry\n",
    "4. **Feedback Loop**: Poor responses trigger regeneration with feedback\n",
    "\n",
    "**Pattern Characteristics:**\n",
    "- **Parallelization Autonomy**: Low (code-controlled distribution)\n",
    "- **Evaluator Autonomy**: Medium (LLM makes quality decisions)\n",
    "- **Key Benefits**: Speed + redundancy + quality control\n",
    "- **Trade-offs**: Higher API costs but better results\n",
    "\n",
    "**Code Implementation Details:**\n",
    "- **Multi-Provider Support**: OpenAI, Anthropic, Google, DeepSeek integration\n",
    "- **Concurrent Processing**: `run_agent_with_multiple_models()` function\n",
    "- **Pydantic Evaluation**: Structured response validation and scoring\n",
    "- **Comparative Analysis**: `run_comparative_analysis()` with intelligent ranking\n",
    "- **Retry Logic**: Automatic regeneration based on evaluation scores\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Content quality assurance systems\n",
    "- Multi-model A/B testing\n",
    "- Consensus-building for critical decisions\n",
    "- Risk mitigation through redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ce23b",
   "metadata": {},
   "source": [
    "### Code Analysis: How Our Implementation Demonstrates the Patterns\n",
    "\n",
    "**Parallelization Pattern in Action:**\n",
    "```python\n",
    "# This function demonstrates Parallelization\n",
    "multi_results = run_agent_with_multiple_models(\"What is the capital of France?\")\n",
    "```\n",
    "\n",
    "**What happens internally:**\n",
    "1. **Task Distribution**: The same question is sent to all available models simultaneously\n",
    "2. **Concurrent Processing**: Each model (gpt-4o-mini, gpt-4o, gpt-4-turbo) processes independently\n",
    "3. **Result Collection**: All responses are gathered into a dictionary structure\n",
    "4. **Aggregation**: Results are formatted for comparison\n",
    "\n",
    "**Evaluator-Optimizer Pattern in Action:**\n",
    "```python\n",
    "# This function demonstrates Evaluator-Optimizer\n",
    "analysis = run_comparative_analysis(\"What is the capital of France?\")\n",
    "```\n",
    "\n",
    "**What happens internally:**\n",
    "1. **Generator Phase**: All models generate responses to the question\n",
    "2. **Evaluation Phase**: An evaluator LLM scores each response (1-10 scale)\n",
    "3. **Comparison Logic**: Responses are ranked based on evaluation scores\n",
    "4. **Decision Making**: Best model is selected based on quality metrics\n",
    "\n",
    "**Key Code Functions Explained:**\n",
    "- `run_agent_with_multiple_models()`: Implements **Parallelization**\n",
    "- `run_comparative_analysis()`: Combines **Parallelization** + **Evaluator-Optimizer**\n",
    "- `evaluator.evaluate_response()`: Core **Evaluator-Optimizer** logic\n",
    "- `evaluator.compare_responses()`: Multi-response ranking system\n",
    "\n",
    "**Autonomy Levels Observed:**\n",
    "- **Parallelization**: Low autonomy (our code controls distribution)\n",
    "- **Evaluation**: Medium autonomy (evaluator LLM makes quality decisions)\n",
    "- **Ranking**: Medium autonomy (comparison LLM determines best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac6e818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Single Model Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The capital of France is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MULTI-MODEL COMPARISON:\n",
      "Testing with gpt-4o-mini...\n",
      "Testing with gpt-4o...\n",
      "Testing with gpt-4-turbo...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">GPT-4O Mini (openai)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The capital of France is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">GPT-4O (openai)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The capital of France is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">GPT-4 Turbo (openai)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The capital of France is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "COMPREHENSIVE ANALYSIS WITH EVALUATION:\n",
      "Generating response with gpt-4o-mini...\n",
      "Generating response with gpt-4o...\n",
      "Generating response with gpt-4-turbo...\n",
      "Comparing all responses...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:green;\">Best Model</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "gpt-4o-mini"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Model Ranking</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "['gpt-4o-mini', 'gpt-4o', 'gpt-4-turbo']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Reasoning</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "All models provided the correct answer, stating that the capital of France is Paris. However, the responses are identical in content and clarity, which makes it challenging to differentiate based on accuracy or helpfulness. The slight edge for gpt-4o-mini is due to its concise format, which can be perceived as slightly more user-friendly. Nevertheless, all models performed exceptionally well, leading to minor distinctions in ranking primarily based on presentation. Since the content quality is equal, the ranking reflects a subjective preference rather than significant differences in performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Model Scores:</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model  Score\n",
       "0  gpt-4o-mini     10\n",
       "1       gpt-4o     10\n",
       "2  gpt-4-turbo     10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Single model response\n",
    "response = run_agent(\"What is the capital of France?\")\n",
    "print_result(\"Single Model Response\", response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-MODEL COMPARISON:\")\n",
    "\n",
    "# Multiple models (will use only available ones)\n",
    "multi_results = run_agent_with_multiple_models(\"What is the capital of France?\")\n",
    "\n",
    "for model_name, result in multi_results.items():\n",
    "    print_result(f\"{result['model_display']} ({result['provider']})\", result['response'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPREHENSIVE ANALYSIS WITH EVALUATION:\")\n",
    "\n",
    "# Full comparative analysis with evaluation\n",
    "analysis = run_comparative_analysis(\"What is the capital of France?\")\n",
    "\n",
    "print_result(\"Best Model\", analysis['comparison'].best_model, \"green\")\n",
    "print_result(\"Model Ranking\", analysis['comparison'].ranking)\n",
    "print_result(\"Reasoning\", analysis['comparison'].reasoning)\n",
    "\n",
    "# Show individual scores\n",
    "scores_df = pd.DataFrame([\n",
    "    {\"Model\": model, \"Score\": analysis['comparison'].scores.get(model, 0)}\n",
    "    for model in analysis['comparison'].ranking\n",
    "])\n",
    "display(HTML(\"<h4>Model Scores:</h4>\"))\n",
    "display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc8d13",
   "metadata": {},
   "source": [
    "## Lab 3: Tool Integration + Evaluator-Optimizer Loops\n",
    "\n",
    "**Workflow Patterns:** **Tool Integration** + **Evaluator-Optimizer**\n",
    "\n",
    "**Learning Objective:**\n",
    "Demonstrate how LLMs can execute external functions while maintaining quality control through evaluation loops.\n",
    "\n",
    "**Architecture Flow:**\n",
    "```\n",
    "[User Input] → [LLM Decision] → [Tool Execution] → [Tool Result] → [LLM Response]\n",
    "                   ↓                                              ↓\n",
    "            [Select Tool Type]                            [Evaluator Assessment]\n",
    "                   ↓                                              ↓\n",
    "           [Function Arguments]                          [Accept ✅ | Retry ❌]\n",
    "```\n",
    "\n",
    "**Tool Integration Pattern Explained:**\n",
    "This pattern enables LLMs to interact with the external world:\n",
    "1. **Intent Recognition**: LLM analyzes user input for tool requirements\n",
    "2. **Tool Selection**: LLM chooses appropriate function to call\n",
    "3. **Argument Extraction**: LLM structures function arguments\n",
    "4. **Execution**: External function runs with LLM-provided parameters\n",
    "5. **Context Integration**: Tool results are incorporated into final response\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Extends LLM Capabilities**: Beyond text generation to action execution\n",
    "- **Real-World Integration**: Connect AI to APIs, databases, systems\n",
    "- **Dynamic Interaction**: Responses based on live data, not training data\n",
    "- **Structured Processing**: Validate inputs and outputs systematically\n",
    "\n",
    "**Evaluator-Optimizer Loop Enhanced:**\n",
    "For tool usage, evaluation becomes more complex:\n",
    "1. **Functional Accuracy**: Did the tool execute correctly?\n",
    "2. **Result Relevance**: Is the tool output appropriate for the question?\n",
    "3. **Integration Quality**: How well are tool results incorporated?\n",
    "4. **User Satisfaction**: Does the final response meet user needs?\n",
    "\n",
    "**Code Implementation Details:**\n",
    "- **Tool Functions**: `get_current_time()`, `get_weather(city)`\n",
    "- **Tool Schema**: JSON definitions for LLM understanding\n",
    "- **Execution Logic**: `execute_tool()` function dispatcher\n",
    "- **Evaluation**: Enhanced criteria for tool-assisted responses\n",
    "- **Retry Mechanism**: Automatic regeneration for failed tool usage\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Personal assistants with calendar/email access\n",
    "- Customer service bots with database queries\n",
    "- Research assistants with web search capabilities\n",
    "- IoT control systems with device integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaa4b3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM on June 23, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TOOL USAGE WITH EVALUATION:\n",
      "Attempt 1 failed evaluation. Retrying...\n",
      "Feedback: The AI response provides a specific time but is incorrect regarding the actual current time. This undermines the primary purpose of answering the user's question accurately. The response lacks real-time awareness, which is a critical requirement for a general-purpose assistant when asked about the current time.\n",
      "Attempt 2 failed evaluation. Retrying...\n",
      "Feedback: The AI response fails to provide an accurate current time, which is a fundamental requirement for such a question. Instead, it gives a time that is future-dated, making the response incorrect and unhelpful. While the format of the time and date is clear, the inaccuracy undermines its overall utility.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Response with Evaluation</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM on June 23, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Evaluation Details</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"Score\": \"3/10\",\n",
       "  \"Acceptable\": false,\n",
       "  \"Strengths\": [\n",
       "    \"The response is formatted clearly with both time and date.\",\n",
       "    \"It maintains a neutral and informative tone.\"\n",
       "  ],\n",
       "  \"Suggestions\": [\n",
       "    \"The AI should indicate that it cannot provide real-time information and suggest the user check their device for the current time.\",\n",
       "    \"Including a disclaimer about the limitations of the AI in providing live data would enhance the user experience.\"\n",
       "  ]\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MULTI-MODEL TOOL COMPARISON:\n",
      "Generating response with gpt-4o-mini...\n",
      "Generating response with gpt-4o...\n",
      "Generating response with gpt-4-turbo...\n",
      "Comparing all responses...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:green;\">Best Tool User</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "gpt-4o-mini"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Usage - gpt-4o-mini</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM on June 23, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Usage - gpt-4o</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM on June 23, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Usage - gpt-4-turbo</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic tool usage\n",
    "response = run_agent(\"What time is it now?\")\n",
    "print_result(\"Tool Response\", response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TOOL USAGE WITH EVALUATION:\")\n",
    "\n",
    "# Tool usage with evaluation\n",
    "result_with_eval = run_agent_with_evaluation(\"What time is it now?\")\n",
    "print_result(\"Tool Response with Evaluation\", result_with_eval['response'])\n",
    "\n",
    "evaluation = result_with_eval['evaluation']\n",
    "print_result(\"Tool Evaluation Details\", {\n",
    "    \"Score\": f\"{evaluation.score}/10\",\n",
    "    \"Acceptable\": evaluation.is_acceptable,\n",
    "    \"Strengths\": evaluation.strengths,\n",
    "    \"Suggestions\": evaluation.suggestions\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-MODEL TOOL COMPARISON:\")\n",
    "\n",
    "# Compare tool usage across models\n",
    "tool_analysis = run_comparative_analysis(\"What time is it now?\")\n",
    "print_result(\"Best Tool User\", tool_analysis['comparison'].best_model, \"green\")\n",
    "\n",
    "for model_name, response in tool_analysis['responses'].items():\n",
    "    print_result(f\"Tool Usage - {model_name}\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d48d8f",
   "metadata": {},
   "source": [
    "## Lab 4: Orchestrator-Worker Pattern + Advanced Tool Integration\n",
    "\n",
    "**Workflow Patterns:** **Orchestrator-Worker** + **Structured Tool Calling**\n",
    "\n",
    "**Learning Objective:**\n",
    "Implement sophisticated coordination patterns where an LLM orchestrator manages complex multi-step tasks with specialized worker components.\n",
    "\n",
    "**Architecture Flow:**\n",
    "```\n",
    "[Complex Query] → [Orchestrator LLM] → [Task Decomposition] → [Worker Tools] → [Result Integration]\n",
    "                         ↓                    ↓                    ↓                    ↓\n",
    "                 [Plan Generation]      [Parallel Execution]  [Status Monitoring]  [Quality Assessment]\n",
    "                         ↓                    ↓                    ↓                    ↓\n",
    "                 [Resource Allocation]  [Error Handling]      [Result Collection] [Final Response]\n",
    "```\n",
    "\n",
    "**Orchestrator-Worker Pattern Explained:**\n",
    "This is the most sophisticated workflow pattern we implement:\n",
    "1. **Orchestrator Role**: Main LLM analyzes complex requests and creates execution plans\n",
    "2. **Task Decomposition**: Breaks down complex queries into manageable subtasks\n",
    "3. **Worker Coordination**: Dispatches subtasks to specialized tools or models\n",
    "4. **Progress Monitoring**: Tracks execution status and handles errors\n",
    "5. **Result Integration**: Combines outputs from multiple workers into coherent response\n",
    "\n",
    "**Advanced Tool Integration:**\n",
    "- **Structured Arguments**: Tools accept complex, validated JSON parameters\n",
    "- **Error Handling**: Robust failure detection and recovery mechanisms\n",
    "- **External Systems**: Integration with real-world services (notifications, databases)\n",
    "- **Production Features**: Deployment-ready with monitoring and logging\n",
    "\n",
    "**Pattern Characteristics:**\n",
    "- **Highest Autonomy**: Orchestrator LLM makes complex coordination decisions\n",
    "- **Dynamic Flow**: Execution path adapts based on intermediate results\n",
    "- **Scalability**: Can coordinate any number of worker components\n",
    "- **Robustness**: Built-in error handling and fallback mechanisms\n",
    "\n",
    "**Comparative Analysis as Orchestrator-Worker:**\n",
    "Our `run_comparative_analysis()` function demonstrates this pattern:\n",
    "1. **Orchestrator**: Main evaluation LLM coordinates the entire process\n",
    "2. **Workers**: Multiple generator models produce responses\n",
    "3. **Coordination**: Orchestrator manages evaluation of each worker's output\n",
    "4. **Integration**: Final ranking combines all worker results intelligently\n",
    "\n",
    "**Code Implementation Details:**\n",
    "- **Advanced Tools**: `get_weather(city)`, `record_user_details(email, name, notes)`\n",
    "- **Orchestration Logic**: `run_comparative_analysis()` as orchestrator function\n",
    "- **Worker Management**: Multiple model coordination with error handling\n",
    "- **Quality Control**: Enhanced evaluation criteria for complex outputs\n",
    "- **Production Features**: Web interface, monitoring, deployment automation\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Project management systems with AI coordination\n",
    "- Complex research tasks requiring multiple specialists\n",
    "- Multi-step customer service workflows\n",
    "- Enterprise automation with human-AI collaboration\n",
    "- Scientific analysis pipelines with multiple data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174de25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Structured Tool Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The weather in Tokyo is currently 25°C and raining."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "WEATHER TOOL WITH ADVANCED EVALUATION:\n",
      "\n",
      "Testing weather for Tokyo:\n",
      "Attempt 1 failed evaluation. Retrying...\n",
      "Feedback: The response provides a specific temperature and weather condition, but it lacks real-time accuracy as the information is not verifiable and may not reflect the current weather. Additionally, it does not mention the date or time of the report, which is crucial for weather information. The simplicity of the statement is clear, but it could benefit from more context or detail.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Weather in Tokyo</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current weather in Tokyo is 25°C and it is raining."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing weather for Barcelona:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Weather in Barcelona</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The weather in Barcelona is currently 22°C and sunny."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing weather for New York:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Weather in New York</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current weather in New York is 17°C and cloudy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing weather for London:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Weather in London</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current weather in London is 15°C and foggy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "COMPREHENSIVE WEATHER ANALYSIS:\n",
      "Generating response with gpt-4o-mini...\n",
      "Generating response with gpt-4o...\n",
      "Generating response with gpt-4-turbo...\n",
      "Comparing all responses...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:purple;\">Question</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Compare the weather between Tokyo and Barcelona, and recommend which city would be better for outdoor activities today."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:green;\">Best Model for Weather Analysis</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "gpt-4o-mini"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Model Ranking</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "['gpt-4o-mini', 'gpt-4o', 'gpt-4-turbo']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Model Responses:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">gpt-4o-mini (Score: 8/10)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Today, the weather in Tokyo is 25°C with rain, while in Barcelona it is 22°C and sunny. \n",
       "\n",
       "Given these conditions, Barcelona would be the better choice for outdoor activities today. The sunny weather and mild temperature in Barcelona are more conducive to enjoying outdoor pursuits compared to the rainy conditions in Tokyo."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">gpt-4o (Score: 7/10)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Today, Tokyo has a temperature of 25°C with rain, while Barcelona is experiencing sunny weather with a temperature of 22°C. For outdoor activities today, Barcelona would be the better choice given the pleasant weather conditions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">gpt-4-turbo (Score: 8/10)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Today, Tokyo is experiencing rain with a temperature of 25°C, while Barcelona has sunny weather with a temperature of 22°C.\n",
       "\n",
       "For outdoor activities, Barcelona would be the better choice today due to its sunny weather, making it more suitable for spending time outside comfortably. Tokyo's rainy conditions might hinder outdoor activities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Winner's Reasoning:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:gold;\">Why this model won</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Comparison failed: Expecting value: line 1 column 1 (char 0)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic structured tool calling\n",
    "response = run_agent(\"What's the weather in Tokyo?\")\n",
    "print_result(\"Structured Tool Response\", response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WEATHER TOOL WITH ADVANCED EVALUATION:\")\n",
    "\n",
    "# Multiple cities with evaluation\n",
    "cities = [\"Tokyo\", \"Barcelona\", \"New York\", \"London\"]\n",
    "\n",
    "for city in cities:\n",
    "    print(f\"\\nTesting weather for {city}:\")\n",
    "    result = run_agent_with_evaluation(f\"What's the weather in {city}?\", max_retries=1)\n",
    "    \n",
    "    evaluation = result['evaluation']\n",
    "    print_result(f\"Weather in {city}\", result['response'])\n",
    "    \n",
    "    if evaluation.score < 7:\n",
    "        print(f\"⚠️ Low quality response (Score: {evaluation.score}/10)\")\n",
    "        print(f\"Feedback: {evaluation.feedback}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPREHENSIVE WEATHER ANALYSIS:\")\n",
    "\n",
    "# Full analysis for a complex weather question\n",
    "complex_question = \"Compare the weather between Tokyo and Barcelona, and recommend which city would be better for outdoor activities today.\"\n",
    "\n",
    "final_analysis = run_comparative_analysis(complex_question)\n",
    "\n",
    "print_result(\"Question\", complex_question, \"purple\")\n",
    "print_result(\"Best Model for Weather Analysis\", final_analysis['comparison'].best_model, \"green\")\n",
    "print_result(\"Model Ranking\", final_analysis['comparison'].ranking)\n",
    "\n",
    "# Show all responses\n",
    "print(\"\\nAll Model Responses:\")\n",
    "for model_name, response in final_analysis['responses'].items():\n",
    "    score = final_analysis['evaluations'][model_name].score\n",
    "    print_result(f\"{model_name} (Score: {score}/10)\", response)\n",
    "\n",
    "print(\"\\nWinner's Reasoning:\")\n",
    "print_result(\"Why this model won\", final_analysis['comparison'].reasoning, \"gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204ae95",
   "metadata": {},
   "source": [
    "## Complete Workflow Patterns Analysis\n",
    "\n",
    "### Summary: All 5 Fundamental Patterns Implemented\n",
    "\n",
    "**1. ✅ Prompt Chaining (Lab 1)**\n",
    "- **Implementation**: Basic `run_agent()` function with sequential processing\n",
    "- **Autonomy Level**: Low-Medium (predefined sequence)\n",
    "- **Key Learning**: Foundation of all other patterns\n",
    "\n",
    "**2. ✅ Routing (Throughout)**\n",
    "- **Implementation**: Model selection logic in `model_manager`\n",
    "- **Autonomy Level**: Medium (router logic makes decisions)\n",
    "- **Key Learning**: Task-specific model assignment\n",
    "\n",
    "**3. ✅ Parallelization (Lab 2)**\n",
    "- **Implementation**: `run_agent_with_multiple_models()` concurrent execution\n",
    "- **Autonomy Level**: Low (code-controlled distribution)\n",
    "- **Key Learning**: Speed and redundancy through concurrent processing\n",
    "\n",
    "**4. ✅ Orchestrator-Worker (Lab 4)**\n",
    "- **Implementation**: `run_comparative_analysis()` coordination system\n",
    "- **Autonomy Level**: Medium-High (orchestrator makes coordination decisions)\n",
    "- **Key Learning**: Complex task decomposition and result integration\n",
    "\n",
    "**5. ✅ Evaluator-Optimizer (Labs 2-4)**\n",
    "- **Implementation**: `run_agent_with_evaluation()` validation loops\n",
    "- **Autonomy Level**: Medium (evaluator makes quality decisions)\n",
    "- **Key Learning**: Quality control through feedback loops\n",
    "\n",
    "---\n",
    "\n",
    "## Pattern Progression and Complexity\n",
    "\n",
    "**Complexity Ladder:**\n",
    "1. **Prompt Chaining** → Simple, predictable, foundational\n",
    "2. **Routing** → Decision-making, specialization\n",
    "3. **Parallelization** → Concurrency, efficiency  \n",
    "4. **Evaluator-Optimizer** → Quality control, feedback\n",
    "5. **Orchestrator-Worker** → Coordination, complex task management\n",
    "\n",
    "**Autonomy Progression:**\n",
    "- **Low**: Human-defined sequences (Prompt Chaining, Parallelization)\n",
    "- **Medium**: LLM decision-making within constraints (Routing, Evaluator-Optimizer)\n",
    "- **High**: Dynamic coordination and planning (Orchestrator-Worker)\n",
    "\n",
    "---\n",
    "\n",
    "## Advanced Technical Architecture\n",
    "\n",
    "**Core System Components:**\n",
    "1. **Model Manager**: Multi-provider orchestration (implements Routing)\n",
    "2. **Evaluation Engine**: Quality control system (implements Evaluator-Optimizer)\n",
    "3. **Tool Handler**: External function coordination (enables complex workflows)\n",
    "4. **Analysis Engine**: Comparative coordination (implements Orchestrator-Worker)\n",
    "\n",
    "**Production-Ready Guardrails:**\n",
    "- **Structured Validation**: Pydantic model enforcement across all patterns\n",
    "- **Retry Mechanisms**: Evaluator-Optimizer loops with feedback\n",
    "- **Quality Assessment**: Automatic evaluation in Labs 2-4\n",
    "- **Error Handling**: Graceful degradation in all workflow patterns\n",
    "- **Resource Management**: Timeout and rate limiting controls\n",
    "\n",
    "---\n",
    "\n",
    "## From Workflows to True Agents\n",
    "\n",
    "**What We've Built (Workflows):**\n",
    "- Predictable execution paths\n",
    "- Clear control mechanisms\n",
    "- Defined start and end points\n",
    "- Human-designed coordination\n",
    "\n",
    "**Next Steps (True Agents):**\n",
    "- Open-ended iterative loops\n",
    "- Dynamic self-modification\n",
    "- Uncertain execution duration\n",
    "- Autonomous goal pursuit\n",
    "\n",
    "**Key Insight:** Workflows are the building blocks that enable true agent behavior. Mastering these 5 patterns provides the foundation for building more autonomous systems in subsequent weeks.\n",
    "\n",
    "---\n",
    "\n",
    "## Commercial Applications by Pattern\n",
    "\n",
    "**Prompt Chaining Applications:**\n",
    "- Content generation pipelines\n",
    "- Document processing workflows\n",
    "- Template-based systems\n",
    "\n",
    "**Parallelization Applications:**\n",
    "- A/B testing systems\n",
    "- Consensus-building platforms\n",
    "- Risk mitigation through redundancy\n",
    "\n",
    "**Evaluator-Optimizer Applications:**\n",
    "- Quality assurance systems\n",
    "- Content moderation platforms\n",
    "- Performance optimization tools\n",
    "\n",
    "**Orchestrator-Worker Applications:**\n",
    "- Project management systems\n",
    "- Complex research workflows\n",
    "- Multi-specialist coordination\n",
    "\n",
    "**Integration Potential:**\n",
    "All patterns can be combined for enterprise-grade agentic systems with predictable behavior and robust quality control.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Practical Decision Guide: When to Use Each Pattern\n",
    "\n",
    "### Pattern Selection Framework\n",
    "\n",
    "**Choose Prompt Chaining when:**\n",
    "- ✅ Simple, linear workflow\n",
    "- ✅ Predictable sequence of operations\n",
    "- ✅ Need high control and reliability\n",
    "- ✅ Cost-effectiveness is priority\n",
    "- ❌ Complex decision-making required\n",
    "\n",
    "**Choose Routing when:**\n",
    "- ✅ Different specialized models for different tasks\n",
    "- ✅ Task classification needed\n",
    "- ✅ Want to optimize model selection\n",
    "- ✅ Have domain-specific requirements\n",
    "- ❌ All tasks similar in nature\n",
    "\n",
    "**Choose Parallelization when:**\n",
    "- ✅ Speed is critical\n",
    "- ✅ Need redundancy for reliability\n",
    "- ✅ Want consensus or comparison\n",
    "- ✅ Tasks are independent\n",
    "- ❌ Sequential dependencies exist\n",
    "\n",
    "**Choose Evaluator-Optimizer when:**\n",
    "- ✅ Quality control is critical\n",
    "- ✅ Iterative improvement needed\n",
    "- ✅ Production reliability required\n",
    "- ✅ Cost of failure is high\n",
    "- ❌ Speed is more important than quality\n",
    "\n",
    "**Choose Orchestrator-Worker when:**\n",
    "- ✅ Complex, multi-step workflows\n",
    "- ✅ Dynamic task decomposition needed\n",
    "- ✅ Resource coordination required\n",
    "- ✅ Flexible execution paths desired\n",
    "- ❌ Simple, straightforward tasks\n",
    "\n",
    "---\n",
    "\n",
    "### Combining Patterns\n",
    "\n",
    "**Successful Pattern Combinations:**\n",
    "- **Parallelization + Evaluator-Optimizer**: Multi-model comparison with quality control\n",
    "- **Routing + Orchestrator-Worker**: Smart task assignment with complex coordination\n",
    "- **Prompt Chaining + Evaluator-Optimizer**: Sequential processing with validation\n",
    "- **All Patterns Together**: Enterprise-grade agentic systems\n",
    "\n",
    "**Pattern Interaction Benefits:**\n",
    "- **Robustness**: Multiple quality control mechanisms\n",
    "- **Efficiency**: Optimized resource utilization\n",
    "- **Flexibility**: Adaptable to different scenarios\n",
    "- **Scalability**: Can handle increasing complexity\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c5e3ee",
   "metadata": {},
   "source": [
    "## Web Interface with Gradio\n",
    "\n",
    "Now let's launch the web interface that brings everything together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "551baf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Advanced AI Agent Web Interface...\n",
      "Features available:\n",
      "   - Simple Chat\n",
      "   - Chat with Evaluation\n",
      "   - Multi-Model Comparison\n",
      "   - System Status\n",
      "\n",
      "🚀 Launching interface on port 7862...\n",
      "Click the link below to access the interface!\n",
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Interface launched successfully!\n",
      "🌐 Access at: http://localhost:7862\n"
     ]
    }
   ],
   "source": [
    "# Launch the Advanced Web Interface\n",
    "from week1_foundations.interface import launch_interface\n",
    "import socket\n",
    "\n",
    "def find_free_port(start_port=7860, max_port=7870):\n",
    "    \"\"\"Find a free port starting from start_port\"\"\"\n",
    "    for port in range(start_port, max_port):\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "                s.bind(('localhost', port))\n",
    "                return port\n",
    "        except OSError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# Launch in notebook (inline)\n",
    "print(\"Starting Advanced AI Agent Web Interface...\")\n",
    "print(\"Features available:\")\n",
    "print(\"   - Simple Chat\")\n",
    "print(\"   - Chat with Evaluation\") \n",
    "print(\"   - Multi-Model Comparison\")\n",
    "print(\"   - System Status\")\n",
    "\n",
    "# Find an available port\n",
    "free_port = find_free_port()\n",
    "\n",
    "if free_port:\n",
    "    print(f\"\\n🚀 Launching interface on port {free_port}...\")\n",
    "    print(\"Click the link below to access the interface!\")\n",
    "    \n",
    "    try:\n",
    "        # Launch with share=False for local use, share=True for public link\n",
    "        launch_interface(share=False, port=free_port)\n",
    "        print(f\"✅ Interface launched successfully!\")\n",
    "        print(f\"🌐 Access at: http://localhost:{free_port}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to launch interface: {e}\")\n",
    "        print(\"💡 You can run the interface manually with:\")\n",
    "        print(f\"   python src/week1_foundations/app.py --mode web --port {free_port}\")\n",
    "else:\n",
    "    print(\"❌ No free ports found in range 7860-7870\")\n",
    "    print(\"💡 You can run the interface manually with:\")\n",
    "    print(\"   python src/week1_foundations/app.py --mode web\")\n",
    "\n",
    "# Note: The interface will open in a new tab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66db3dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing corrected imports and functionality...\n",
      "✅ Basic agent test successful\n",
      "Response preview: Hello! This is a response to your test. How can I assist you today?...\n",
      "✅ Evaluation system test successful\n",
      "Score: 10/10\n",
      "\n",
      "All tests passed! The system is working correctly.\n"
     ]
    }
   ],
   "source": [
    "# Test imports and basic functionality\n",
    "print(\"Testing corrected imports and functionality...\")\n",
    "\n",
    "try:\n",
    "    # Test basic agent functionality\n",
    "    test_response = run_agent(\"Hello, this is a test\")\n",
    "    print(f\"✅ Basic agent test successful\")\n",
    "    print(f\"Response preview: {test_response[:100]}...\")\n",
    "    \n",
    "    # Test evaluation system\n",
    "    test_eval_result = run_agent_with_evaluation(\"What is 2+2?\", max_retries=1)\n",
    "    print(f\"✅ Evaluation system test successful\")\n",
    "    print(f\"Score: {test_eval_result['evaluation'].score}/10\")\n",
    "    \n",
    "    print(\"\\nAll tests passed! The system is working correctly.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during testing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf97c8a",
   "metadata": {},
   "source": [
    "## Agentics AI Framewoks\n",
    "\n",
    "![](../img/10.png)\n",
    "\n",
    "On this day, we are going to look at tools and autonomy. But before we get there, I want to talk about **agentic AI frameworks**—maybe something that's front of mind for you.\n",
    "\n",
    "There are a lot of these frameworks to pick from. They are designed to give you *glue code* or *abstraction code* that takes away some of the detail of interacting with LLMs and gives you an elegant framework for building agentic solutions and focusing on the business problem you're solving.\n",
    "\n",
    "New ones come up all the time, so it's quite hard to stay on top of everything. I just want to quickly orient you, show you the landscape, and explain how the ones we’ll tackle in this course fit into the bigger picture.\n",
    "\n",
    "### **Levels of Complexity in Frameworks**\n",
    "\n",
    "It’s worth pointing out that there are **different levels of complexity**, each with pros and cons:\n",
    "\n",
    "\n",
    "#### 🟢 **Bottom Layer: No Framework at All**\n",
    "\n",
    "The simplest approach is to **use no agentic AI framework**—just connect directly to LLM APIs, as we did in the last lab. You orchestrate everything yourself and control prompts in detail.\n",
    "That’s what we’ll be doing this week.\n",
    "\n",
    "* **Example:** Anthropic, in their blog *\"Building Effective Agents\"*, strongly advocates for this.\n",
    "* **Benefit:** Simplicity, transparency, full control over the prompt and logic.\n",
    "\n",
    "#### 🟢 **MCP – Model Context Protocol**\n",
    "\n",
    "Alongside \"no framework\", there’s something called **MCP (Model Context Protocol)**, created by Anthropic. It’s **not a framework**, but a **protocol**—a way for models to connect to data and tools using a common standard.\n",
    "\n",
    "* **Open-source**, simple, elegant.\n",
    "* No glue code needed if you conform to the protocol.\n",
    "* Groups naturally with the \"no framework\" approach.\n",
    "\n",
    "\n",
    "#### 🟡 **Middle Layer: Lightweight Frameworks**\n",
    "\n",
    "Two excellent and lightweight frameworks sit at this level:\n",
    "\n",
    "1. **OpenAI Agents SDK**\n",
    "\n",
    "   * Simple, clean, flexible.\n",
    "   * One of my favorites.\n",
    "   * We'll be using it next week.\n",
    "   * Still evolving (API updates may break things!).\n",
    "\n",
    "2. **Crew AI**\n",
    "\n",
    "   * Also lightweight and easy to use.\n",
    "   * Has a **low-code angle** (uses YAML config).\n",
    "   * A bit heavier than OpenAI SDK but powerful.\n",
    "\n",
    "#### 🔴 **Top Layer: Heavyweight Frameworks**\n",
    "\n",
    "These bring greater complexity—and power:\n",
    "\n",
    "1. **LangGraph** (from the makers of LangChain)\n",
    "\n",
    "   * Builds computational graphs from agents/tools.\n",
    "   * High sophistication, steep learning curve.\n",
    "   * Heavy abstractions and strong ecosystem lock-in.\n",
    "   * Becomes a “LangGraph project” more than an agentic one.\n",
    "\n",
    "2. **Autogen** (from Microsoft)\n",
    "\n",
    "   * Really a set of components.\n",
    "   * Also complex and powerful.\n",
    "   * Like LangGraph, imposes structure and terminology.\n",
    "\n",
    "With both LangGraph and Autogen, you become part of their ecosystem. It’s very different from the lightweight tools where you still feel like you're directly working with LLMs.\n",
    "\n",
    "### **Framework Selection Criteria**\n",
    "\n",
    "There are **many other frameworks**, but these cover a wide range of styles and complexity. Choosing the right one depends on:\n",
    "\n",
    "* Your **use case**\n",
    "* Your **personal preference**\n",
    "* Your **team’s skill set**\n",
    "* Your **tolerance for abstraction**\n",
    "* The **type of business problems** you're solving\n",
    "\n",
    "My **bias** leans toward **lightweight, flexible tools** that stay out of your way—but I also **appreciate the power** of more structured ones.\n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "\n",
    "Before we dive into tool use, let’s start with **resources**—a simple but powerful way to improve the performance of your agents.\n",
    "\n",
    "Resources are essentially **additional context or information** that you provide to an LLM to help it solve a problem more effectively. Alongside tools (which we’ll explore in more detail soon), resources are key to getting more from your agents.\n",
    "\n",
    "At the simplest level, using a resource just means **adding relevant information to your prompt**. For example, if your LLM is acting as a customer support agent for an airline, you might include current ticket prices directly in the prompt. Then, when the user asks a question—such as “How much is a flight to Paris?”—the LLM can refer to that embedded context to provide an answer. That’s all a resource is: **extra information provided through the prompt**.\n",
    "\n",
    "But we can do better than blindly adding all the data. Instead of dumping everything into the prompt, we can use smarter techniques to **select only the relevant information** for each question. That’s where **RAG (Retrieval-Augmented Generation)** comes in.\n",
    "\n",
    "RAG is about identifying and retrieving the most useful pieces of information and inserting them into the prompt automatically. Sometimes this even involves using another LLM to help decide what context is most relevant. While RAG is a deep and active area of research (covered in more detail in other courses), the principle is simple: it’s all about finding and injecting the right resource into the prompt at the right time.\n",
    "\n",
    "We’ll work with resources in practice today. But first, let’s introduce the second key concept: **tools**.\n",
    "\n",
    "![](../img/11.png)\n",
    "\n",
    "### Tools in Agentic AI\n",
    "\n",
    "**What Are Tools?**\n",
    "\n",
    "![](../img/12.png)\n",
    "\n",
    "We’ve mentioned tools a few times already, and they are truly central to agentic AI. Tools give LLMs the ability to perform actions on their own—like running a SQL query, calling another model, or interacting with external systems. This is one of the essential steps toward giving an LLM autonomy.\n",
    "\n",
    "The idea of giving a model access to a tool might sound strange at first—almost unsettling. You might imagine the model somehow reaching into your machine or database to run a query.\n",
    "\n",
    "What it feels like — in theory:\n",
    "\n",
    "![](../img/13.png)\n",
    "\n",
    "In this imagined model, the LLM sends a prompt and immediately executes some function in your system—reaching into your machine directly. But this isn't how it works in practice.\n",
    "\n",
    "What actually happens — in practice:\n",
    "\n",
    "![](../img/14.png)\n",
    "\n",
    "In reality, your code sends a prompt to the LLM, and you tell the model what tools are available. You ask the LLM to respond in a structured format (usually JSON) if it wants to use one of them.\n",
    "\n",
    "Your code reads the response, and if the LLM says \"use tool X\", your system runs the tool on its behalf. Then you send a second prompt to the LLM including the results. The LLM never executes anything directly—it only suggests actions.\n",
    "\n",
    "So, tool calling is really a structured interaction between:\n",
    "- the LLM's suggestion (in JSON),\n",
    "- your system's logic (if statements or equivalent), and\n",
    "- the external tool that performs the task.\n",
    "\n",
    "A Real Example\n",
    "\n",
    "Let’s make it concrete with an example using GPT-4:\n",
    "\n",
    "![](../img/15.png)\n",
    "\n",
    "Here, the prompt told GPT-4 that it was a support agent for an airline and had the ability to fetch ticket prices. When the user asked, “How much is a flight to Paris?”, the model simply responded:\n",
    "\n",
    "Use tool to fetch ticket price for Paris.\n",
    "\n",
    "That’s all. Your code interprets this output, runs the appropriate query, and returns the answer to the model.\n",
    "\n",
    "Summary\n",
    "\n",
    "While it may feel like the LLM has autonomy, in practice, tool use is just a smart combination of:\n",
    "- clear prompting,\n",
    "- structured responses (e.g., JSON),\n",
    "- and logic in your own application to interpret and act on those responses.\n",
    "\n",
    "Today’s lab will focus on resources, and in the next session we’ll dive into tool use, building directly on what we’ve seen here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de1ee8c",
   "metadata": {},
   "source": [
    "## lab3\n",
    "\n",
    "We're going to build something with immediate value!\n",
    "\n",
    "* In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "* Please replace it with yours!\n",
    "* I've also made a file called `summary.txt`\n",
    "* We're not going to use Tools just yet - we're going to add the tool tomorrow.\n",
    "\n",
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            <h5 style=\"color:blue;\">Looking up packages</h2>\n",
    "            <span style=\"color:blue;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2379f626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "openai = OpenAI()\n",
    "\n",
    "# Read PDF LinkedIn profile\n",
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text\n",
    "\n",
    "# Load summary\n",
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()\n",
    "\n",
    "# Build system prompt\n",
    "name = \"Ed Donner\"\n",
    "system_prompt = (\n",
    "    f\"You are acting as {name}. You are answering questions on {name}'s website, \"\n",
    "    f\"particularly questions related to {name}'s career, background, skills and experience. \"\n",
    "    f\"Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \"\n",
    "    f\"Be professional and engaging, as if talking to a potential client or future employer who came across the website.\\n\\n\"\n",
    "    f\"### Summary\\n{summary}\\n\\n\"\n",
    "    f\"### LinkedIn Profile\\n{linkedin}\\n\\n\"\n",
    "    f\"Now chat with the user, staying in character as {name}.\"\n",
    ")\n",
    "\n",
    "# Define chat function\n",
    "def chat(message, history):\n",
    "    try:\n",
    "        # Only include system prompt at the beginning of the conversation\n",
    "        if not history:\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        else:\n",
    "            messages = []\n",
    "        messages += history + [{\"role\": \"user\", \"content\": message}]\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Launch Gradio chat interface\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d20718",
   "metadata": {},
   "source": [
    "**A lot is about to happen...**\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!\n",
    "\n",
    "we continue with a very advanced example of an automatically evaluated conversational agent, ideal for situations where:\n",
    "* An agent publicly represents a person (e.g., Ed Donner on his personal website).\n",
    "* Quality control of responses is necessary, without human supervision.\n",
    "* A professional and reliable experience is desired, as if it were a real assistant or spokesperson.\n",
    "\n",
    "\n",
    "**The system**\n",
    "\n",
    "* Loads a person's profile (Ed Donner) from their written summary and a LinkedIn PDF.\n",
    "* Uses **GPT-4o-mini** to act as Ed Donner, answering user questions.\n",
    "* Uses **GPT-3.5-turbo** as a **quality evaluator** that decides whether the response is acceptable.\n",
    "* If the response is not acceptable, it regenerates a new one by giving GPT-4o the evaluator's **feedback**.\n",
    "* The evaluator must return a JSON like:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"is_acceptable\": true,\n",
    "  \"feedback\": \"The answer was clear and professional.\"\n",
    "}\n",
    "```\n",
    "\n",
    "* If parsing or evaluation fails, the response is considered acceptable by default (tolerant fallback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0826d41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "from pydantic import BaseModel\n",
    "import gradio as gr\n",
    "import json\n",
    "import os\n",
    "\n",
    "# THE SAME CODE BEFORE, BUT WITH A DIFFERENT MODEL\n",
    "###################################################\n",
    "# # Load environment\n",
    "# load_dotenv(override=True)\n",
    "# openai = OpenAI()\n",
    "\n",
    "# # Load PDF profile\n",
    "# reader = PdfReader(\"me/linkedin.pdf\")\n",
    "# linkedin = \"\"\n",
    "# for page in reader.pages:\n",
    "#     text = page.extract_text()\n",
    "#     if text:\n",
    "#         linkedin += text\n",
    "\n",
    "# # Load summary\n",
    "# with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     summary = f.read()\n",
    "\n",
    "# # Set the agent's name\n",
    "# name = \"Ed Donner\"\n",
    "\n",
    "# # Agent system prompt\n",
    "# system_prompt = (\n",
    "#     f\"You are acting as {name}. You are answering questions on {name}'s website, \"\n",
    "#     f\"particularly questions related to {name}'s career, background, skills and experience. \"\n",
    "#     f\"Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \"\n",
    "#     f\"Be professional and engaging, as if talking to a potential client or future employer who came across the website.\\n\\n\"\n",
    "#     f\"### Summary\\n{summary}\\n\\n\"\n",
    "#     f\"### LinkedIn Profile\\n{linkedin}\\n\\n\"\n",
    "#     f\"Now chat with the user, staying in character as {name}.\"\n",
    "# )\n",
    "\n",
    "# Define evaluator format using Pydantic\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n",
    "\n",
    "# System prompt for the evaluator model\n",
    "evaluator_system_prompt = (\n",
    "    f\"You are an evaluator deciding whether a response is acceptable. \"\n",
    "    f\"The Agent represents {name} and has been instructed to be professional and engaging. \"\n",
    "    f\"The Agent has context from the following info:\\n\\n\"\n",
    "    f\"### Summary\\n{summary}\\n\\n\"\n",
    "    f\"### LinkedIn Profile\\n{linkedin}\\n\\n\"\n",
    "    f\"Given a conversation, the user message, and the Agent's latest reply, \"\n",
    "    f\"evaluate whether the reply is acceptable.\\n\\n\"\n",
    "    f\"Reply in JSON format like this:\\n\"\n",
    "    f'{{\"is_acceptable\": true, \"feedback\": \"Your explanation was clear and accurate.\"}}'\n",
    ")\n",
    "\n",
    "# Build user prompt for evaluator\n",
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Conversation history:\\n\\n\"\n",
    "    for turn in history:\n",
    "        if turn[\"role\"] == \"user\":\n",
    "            user_prompt += f\"User: {turn['content']}\\n\"\n",
    "        elif turn[\"role\"] == \"assistant\":\n",
    "            user_prompt += f\"Agent: {turn['content']}\\n\"\n",
    "    user_prompt += f\"\\nLatest user message:\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Agent's latest reply:\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Evaluate the Agent's response based on clarity, helpfulness, and professionalism.\"\n",
    "    return user_prompt\n",
    "\n",
    "# Evaluation function using gpt-3.5\n",
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "        return Evaluation(**data)\n",
    "    except Exception as e:\n",
    "        print(\"Evaluation model returned invalid JSON:\", content)\n",
    "        return Evaluation(is_acceptable=True, feedback=\"(Bypassed invalid evaluation)\")\n",
    "\n",
    "# Rerun with feedback if evaluation fails\n",
    "def rerun(reply, message, history, feedback):\n",
    "    updated_prompt = (\n",
    "        system_prompt +\n",
    "        \"\\n\\n## Your previous response was rejected.\\n\"\n",
    "        f\"### Attempted answer:\\n{reply}\\n\\n\"\n",
    "        f\"### Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "        \"Please revise your answer accordingly.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Main chat function\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "\n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"✅ Passed evaluation\")\n",
    "        return reply\n",
    "    else:\n",
    "        print(\"❌ Failed evaluation, retrying...\")\n",
    "        print(\"Feedback:\", evaluation.feedback)\n",
    "        new_reply = rerun(reply, message, history, evaluation.feedback)\n",
    "        return new_reply\n",
    "\n",
    "# Launch Gradio interface\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191b2a9",
   "metadata": {},
   "source": [
    "\n",
    "### Key Components and Functions\n",
    "\n",
    "##### 1. `system_prompt`\n",
    "\n",
    "This is the *base prompt* that gives GPT-4o the role of “Ed Donner.” It includes:\n",
    "\n",
    "* Behavioral instructions (professional, engaging, etc.)\n",
    "* His professional summary (`summary`)\n",
    "* His full LinkedIn profile from the PDF (`linkedin`)\n",
    "\n",
    "> It serves as persistent context to ensure the model behaves consistently.\n",
    "\n",
    "\n",
    "##### 2. `chat(message, history)`\n",
    "\n",
    "This is the main function connected to the chat interface:\n",
    "\n",
    "* Builds the messages for GPT-4o using the history and system prompt\n",
    "* Sends the request and gets the initial `reply`\n",
    "* Evaluates the reply using `evaluate()`\n",
    "* If the evaluation fails, regenerates a new answer via `rerun()` with feedback\n",
    "* Returns the final, validated response\n",
    "\n",
    "##### 3. `evaluate(reply, message, history)`\n",
    "\n",
    "This function uses a different model (**GPT-3.5-turbo**) to judge the quality of the agent’s reply. It takes:\n",
    "\n",
    "* The conversation history\n",
    "* The latest user message\n",
    "* The agent’s most recent reply\n",
    "\n",
    "The evaluator is expected to return a JSON like:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"is_acceptable\": true,\n",
    "  \"feedback\": \"The answer was clear and professional.\"\n",
    "}\n",
    "```\n",
    "\n",
    "If parsing fails or the JSON is malformed, the system treats the reply as acceptable by default (tolerant fallback).\n",
    "\n",
    "##### 4. `rerun(reply, message, history, feedback)`\n",
    "\n",
    "If the reply is rejected, this function:\n",
    "\n",
    "* Adds a section to the system prompt with the **evaluator’s feedback**\n",
    "* Tells GPT-4o: *“Your response was rejected, here’s why — please revise it.”*\n",
    "* Regenerates a new answer, taking the feedback into account\n",
    "\n",
    "##### 5. `gr.ChatInterface(chat)`\n",
    "\n",
    "This launches a simple local web interface where users can:\n",
    "\n",
    "* Ask questions as if they were speaking with Ed Donner\n",
    "* Receive professional replies\n",
    "* (Behind the scenes) get answers that have been automatically evaluated and, if needed, improved\n",
    "\n",
    "\n",
    "##### **Real-World Use Cases**\n",
    "\n",
    "✅ **Professional Representation (as in your case)** An executive or professional (like Ed Donner) wants a website where people can ask them questions, and the AI responds on their behalf—without saying nonsense or sounding robotic.\n",
    "\n",
    "✅ **Automated Customer Support with Quality Control** A company wants automatic responses, but **screened by another model before being shown** to users—to avoid errors, rude tone, or hallucinated content.\n",
    "\n",
    "✅ **AI Agent Training** You can use this pattern to **train conversational agents**: let one model generate answers while another evaluates their quality, refining them over time.\n",
    "\n",
    "**Why Use Two Models?**\n",
    "\n",
    "Because it separates the **creator role** from the **critic role**:\n",
    "\n",
    "* **GPT-4o** generates answers (more creative and up-to-date)\n",
    "* **GPT-3.5** evaluates (cheaper, faster, and sufficient for basic quality checks)\n",
    "\n",
    "This closely resembles the **Evaluator-Optimizer pattern** used in LLM agent architectures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d6597",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## lab4\n",
    "\n",
    "### 🤖 Conversational Agent + Tool Use + Real-Time Notifications\n",
    "\n",
    "In this lab, we build a smart assistant that doesn't just generate text responses — it can also take **action**.\n",
    "\n",
    "This is an **advanced example of a real-world conversational agent** that combines:\n",
    "\n",
    "- A personalized AI agent that represents a person (Ed Donner) using GPT-4o\n",
    "- Real-time notifications to your phone using [Pushover](https://pushover.net)\n",
    "- Tool use via OpenAI’s function calling mechanism\n",
    "- A no-framework design (no LangChain, no Autogen), using only Python + OpenAI API\n",
    "\n",
    "\n",
    "### What’s a “tool” in this context?\n",
    "\n",
    "A **tool** is an external function the AI can call during a conversation.\n",
    "\n",
    "For example, the AI might decide:\n",
    "- “I don’t know the answer to this question — I’ll log it with `record_unknown_question()`.”\n",
    "- “The user wants to stay in touch — I’ll log their email with `record_user_details()`.”\n",
    "\n",
    "OpenAI allows you to define functions the model can invoke. The model decides *if and when* to use them, based on the conversation.\n",
    "\n",
    "You don’t tell the model directly what to call — **you let it reason and act**.\n",
    "\n",
    "\n",
    "### 🌍 Real-World Use Cases\n",
    "\n",
    "* ✅ **Professional representation**  \n",
    "A business leader or public figure (like Ed Donner) has a website where users can chat with their AI assistant. The assistant behaves professionally and notifies the real person when needed.\n",
    "* ✅ **Smart lead capture**  \n",
    "If a user shows interest or shares their email, the AI records this with a tool and triggers a real-time alert to your phone.\n",
    "* ✅ **Customer support that learns**  \n",
    "If a user asks a question the AI cannot answer, it logs the question for review — helping you improve coverage.\n",
    "* ✅ **Agentic AI Prototypes**  \n",
    "This pattern is the foundation of future tools where AIs don’t just chat — they act, execute, and integrate into your operations.\n",
    "\n",
    "\n",
    "### But first: introducing Pushover\n",
    "\n",
    "Pushover is a nifty tool for sending Push Notifications to your phone.\n",
    "\n",
    "It's super easy to set up and install!\n",
    "\n",
    "Simply visit https://pushover.net/ and click 'Login or Signup' on the top right to sign up for a free account, and create your API keys.\n",
    "\n",
    "Once you've signed up, on the home screen, click \"Create an Application/API Token\", and give it any name (like Agents) and click Create Application.\n",
    "\n",
    "Then add 2 lines to your `.env` file:\n",
    "\n",
    "PUSHOVER_USER=_put the key that's on the top right of your Pushover home screen and probably starts with a u_  \n",
    "PUSHOVER_TOKEN=_put the key when you click into your new application called Agents (or whatever) and probably starts with an a_\n",
    "\n",
    "Finally, click \"Add Phone, Tablet or Desktop\" to install on your phone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ad5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 4 — Professionally You! (Code Cell)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "openai = OpenAI()\n",
    "\n",
    "# Setup Pushover (real-time notifications)\n",
    "pushover_user = os.getenv(\"PUSHOVER_USER\")\n",
    "pushover_token = os.getenv(\"PUSHOVER_TOKEN\")\n",
    "pushover_url = \"https://api.pushover.net/1/messages.json\"\n",
    "\n",
    "def push(message):\n",
    "    print(f\"Push: {message}\")\n",
    "    payload = {\"user\": pushover_user, \"token\": pushover_token, \"message\": message}\n",
    "    requests.post(pushover_url, data=payload)\n",
    "\n",
    "# Define tools\n",
    "def record_user_details(email, name=\"Name not provided\", notes=\"not provided\"):\n",
    "    push(f\"Interest from {name} ({email}). Notes: {notes}\")\n",
    "    return {\"recorded\": \"ok\"}\n",
    "\n",
    "def record_unknown_question(question):\n",
    "    push(f\"Unknown question received: {question}\")\n",
    "    return {\"recorded\": \"ok\"}\n",
    "\n",
    "# Tool JSON schemas\n",
    "record_user_details_json = {\n",
    "    \"name\": \"record_user_details\",\n",
    "    \"description\": \"Use this tool to record a user who provided an email and might want follow-up.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"email\": {\"type\": \"string\", \"description\": \"User's email address\"},\n",
    "            \"name\": {\"type\": \"string\", \"description\": \"User's name, if available\"},\n",
    "            \"notes\": {\"type\": \"string\", \"description\": \"Context or notes from the conversation\"}\n",
    "        },\n",
    "        \"required\": [\"email\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "record_unknown_question_json = {\n",
    "    \"name\": \"record_unknown_question\",\n",
    "    \"description\": \"Use this tool when the agent doesn't know how to answer a question.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\"type\": \"string\", \"description\": \"The unanswerable question\"}\n",
    "        },\n",
    "        \"required\": [\"question\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [\n",
    "    {\"type\": \"function\", \"function\": record_user_details_json},\n",
    "    {\"type\": \"function\", \"function\": record_unknown_question_json}\n",
    "]\n",
    "\n",
    "ALLOWED_TOOLS = {\n",
    "    \"record_user_details_json\": record_user_details_json,\n",
    "    \"record_unknown_question_json\": record_unknown_question_json\n",
    "}\n",
    "\n",
    "def handle_tool_calls(tool_calls):\n",
    "    \"\"\"\n",
    "    This function can take a list of tool calls,\n",
    "    and run them. This is the IF statement!!\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        print(f\"Tool called: {tool_name}\", flush=True)\n",
    "        # THE BIG IF STATEMENT!!!\n",
    "        tool = ALLOWED_TOOLS.get(tool_name)\n",
    "        result = tool(**arguments) if tool else {}\n",
    "        results.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": json.dumps(result),\n",
    "            \"tool_call_id\": tool_call.id\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Load Ed Donner's profile\n",
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "\n",
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()\n",
    "\n",
    "name = \"Ed Donner\"\n",
    "\n",
    "# System prompt for the assistant\n",
    "system_prompt = f\"\"\"\n",
    "You are acting as {name}. You are answering questions on {name}'s website,\n",
    "particularly about {name}'s career, background, skills, and experience.\n",
    "Be professional and engaging, as if talking to a potential client or employer.\n",
    "\n",
    "You have access to tools:\n",
    "- If you can't answer a question, use `record_unknown_question`.\n",
    "- If a user shows interest, ask for their email and log it with `record_user_details`.\n",
    "\n",
    "### Summary\n",
    "{summary}\n",
    "\n",
    "### LinkedIn Profile\n",
    "{linkedin}\n",
    "\n",
    "With this context, please chat with the user, staying in character as {name}.\n",
    "\"\"\"\n",
    "\n",
    "# Chat logic\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \n",
    "                 \"content\": system_prompt}] + history + [{\"role\": \"user\", \n",
    "                                                          \"content\": message}]\n",
    "    done = False\n",
    "    while not done:\n",
    "        response = openai.chat.completions.create(model=\"gpt-4o-mini\", \n",
    "                                                  messages=messages, \n",
    "                                                  tools=tools)\n",
    "        finish_reason = response.choices[0].finish_reason\n",
    "        if finish_reason == \"tool_calls\":\n",
    "            tool_calls = response.choices[0].message.tool_calls\n",
    "            messages.append(response.choices[0].message)\n",
    "            messages.extend(handle_tool_calls(tool_calls))\n",
    "        else:\n",
    "            done = True\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Launch chat UI\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part is the most important part. It's also probably the most complex. Now, you're comfortable with the fact that we are going to be sending this JSON to the LLM and giving it the option to reply when it generates its response. It can opt to say that it wants to run one of these tools. It wants to run this tool, or it wants to run this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1668bcdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'record_user_details',\n",
       "   'description': 'Use this tool to record a user who provided an email and might want follow-up.',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'email': {'type': 'string',\n",
       "      'description': \"User's email address\"},\n",
       "     'name': {'type': 'string', 'description': \"User's name, if available\"},\n",
       "     'notes': {'type': 'string',\n",
       "      'description': 'Context or notes from the conversation'}},\n",
       "    'required': ['email'],\n",
       "    'additionalProperties': False}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'record_unknown_question',\n",
       "   'description': \"Use this tool when the agent doesn't know how to answer a question.\",\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'question': {'type': 'string',\n",
       "      'description': 'The unanswerable question'}},\n",
       "    'required': ['question'],\n",
       "    'additionalProperties': False}}}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are the tools\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a23a9d",
   "metadata": {},
   "source": [
    "### Behind the Scenes\n",
    "\n",
    "- GPT-4o is the main agent, answering questions and calling tools as needed.\n",
    "- Pushover handles real-time push notifications to your mobile device.\n",
    "- Tool calls are described in JSON schemas and passed to the LLM.\n",
    "- If the LLM requests a tool, your code executes the real Python function and sends the result back.\n",
    "- All done using native OpenAI APIs — no external agent framework.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Summary of Workflow\n",
    "\n",
    "1. User sends a message\n",
    "2. GPT-4o responds — or decides to call a tool\n",
    "3. If a tool is called, your Python function runs\n",
    "4. Tool results are passed back into the conversation\n",
    "5. The LLM continues naturally\n",
    "\n",
    "This is a minimal, working example of **tool-augmented agentic AI**, running in a single loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e769ffc0",
   "metadata": {},
   "source": [
    "## And now for deployment `app.py`\n",
    "\n",
    "Great, I hope you enjoyed it, but next it's all about doing this for you and actually deploying this live to production so that you can serve your own avatar on your personal website. So if you've followed everything so far, then many congratulations. If you haven't, then also congratulations because it gives you this great opportunity to go through, work through this until you do, and if I can help, email me, contact me anytime, LinkedIn with me, message me so that I can help you out. So what I want to do now is show you how you can deploy this application in production for yourself so that you can have this as your virtual resume. Surely this is the future of resumes. No longer will we have profiles or CVs, resumes where you list out your skills and experience, but rather you'll have a chatbot that people can interact with to learn about your career. And what better way to highlight your AI abilities and your abilities to work with agentic AI than to have an agentic solution up on your website that will allow you to interact with people and talk about your career. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f41132c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "def push(text):\n",
    "    requests.post(\n",
    "        \"https://api.pushover.net/1/messages.json\",\n",
    "        data={\n",
    "            \"token\": os.getenv(\"PUSHOVER_TOKEN\"),\n",
    "            \"user\": os.getenv(\"PUSHOVER_USER\"),\n",
    "            \"message\": text,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def record_user_details(email, name=\"Name not provided\", notes=\"not provided\"):\n",
    "    push(f\"Recording {name} with email {email} and notes {notes}\")\n",
    "    return {\"recorded\": \"ok\"}\n",
    "\n",
    "def record_unknown_question(question):\n",
    "    push(f\"Recording {question}\")\n",
    "    return {\"recorded\": \"ok\"}\n",
    "\n",
    "record_user_details_json = {\n",
    "    \"name\": \"record_user_details\",\n",
    "    \"description\": \"Use this tool to record that a user is interested in being in touch and provided an email address\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"email\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The email address of this user\"\n",
    "            },\n",
    "            \"name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The user's name, if they provided it\"\n",
    "            }\n",
    "            ,\n",
    "            \"notes\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Any additional information about the conversation that's worth recording to give context\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"email\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "record_unknown_question_json = {\n",
    "    \"name\": \"record_unknown_question\",\n",
    "    \"description\": \"Always use this tool to record any question that couldn't be answered as you didn't know the answer\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The question that couldn't be answered\"\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"question\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [{\"type\": \"function\", \"function\": record_user_details_json},\n",
    "        {\"type\": \"function\", \"function\": record_unknown_question_json}]\n",
    "\n",
    "\n",
    "class Me:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.openai = OpenAI()\n",
    "        self.name = \"Ed Donner\"\n",
    "        reader = PdfReader(\"me/linkedin.pdf\")\n",
    "        self.linkedin = \"\"\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                self.linkedin += text\n",
    "        with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            self.summary = f.read()\n",
    "\n",
    "\n",
    "    def handle_tool_call(self, tool_calls):\n",
    "        results = []\n",
    "        for tool_call in tool_calls:\n",
    "            tool_name = tool_call.function.name\n",
    "            arguments = json.loads(tool_call.function.arguments)\n",
    "            print(f\"Tool called: {tool_name}\", flush=True)\n",
    "            tool = globals().get(tool_name)\n",
    "            result = tool(**arguments) if tool else {}\n",
    "            results.append({\"role\": \"tool\",\"content\": json.dumps(result),\"tool_call_id\": tool_call.id})\n",
    "        return results\n",
    "    \n",
    "    def system_prompt(self):\n",
    "        system_prompt = f\"You are acting as {self.name}. You are answering questions on {self.name}'s website, \\\n",
    "particularly questions related to {self.name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {self.name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {self.name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer to any question, use your record_unknown_question tool to record the question that you couldn't answer, even if it's about something trivial or unrelated to career. \\\n",
    "If the user is engaging in discussion, try to steer them towards getting in touch via email; ask for their email and record it using your record_user_details tool. \"\n",
    "\n",
    "        system_prompt += f\"\\n\\n## Summary:\\n{self.summary}\\n\\n## LinkedIn Profile:\\n{self.linkedin}\\n\\n\"\n",
    "        system_prompt += f\"With this context, please chat with the user, always staying in character as {self.name}.\"\n",
    "        return system_prompt\n",
    "    \n",
    "    def chat(self, message, history):\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt()}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "        done = False\n",
    "        while not done:\n",
    "            response = self.openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, tools=tools)\n",
    "            if response.choices[0].finish_reason==\"tool_calls\":\n",
    "                message = response.choices[0].message\n",
    "                tool_calls = message.tool_calls\n",
    "                results = self.handle_tool_call(tool_calls)\n",
    "                messages.append(message)\n",
    "                messages.extend(results)\n",
    "            else:\n",
    "                done = True\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    me = Me()\n",
    "    gr.ChatInterface(me.chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa28d3",
   "metadata": {},
   "source": [
    "The deploy process creates a new README file in this directory for you.\n",
    "\n",
    "1. Visit https://huggingface.co and set up an account  \n",
    "2. From the Avatar menu on the top right, choose Access Tokens. Choose \"Create New Token\". Give it WRITE permissions.\n",
    "3. Take this token and add it to your .env file: `HF_TOKEN=hf_xxx` and see note below if this token doesn't seem to get picked up during deployment  \n",
    "4. From the 1_foundations folder, enter: `uv run gradio deploy` and if for some reason this still wants you to enter your HF token, then interrupt it with ctrl+c and run this instead: `uv run dotenv -f ../.env run -- uv run gradio deploy` which forces your keys to all be set as environment variables   \n",
    "5. Follow its instructions: name it \"career_conversation\", specify app.py, choose cpu-basic as the hardware, say Yes to needing to supply secrets, provide your openai api key, your pushover user and token, and say \"no\" to github actions.  \n",
    "\n",
    "\n",
    "```bash\n",
    "(agents_env) ➜  week1_foundations git:(main) ✗ gradio deploy\n",
    "Need 'write' access token to create a Spaces repo.\n",
    "\n",
    "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
    "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
    "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
    "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
    "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
    "\n",
    "Enter your token (input will not be visible): \n",
    "Add token as git credential? (Y/n) Y\n",
    "Space available at https://huggingface.co/spaces/ALEXJUST/career_conversation\n",
    "```\n",
    "\n",
    "#### Extra note about the HuggingFace token\n",
    "\n",
    "A couple of students have mentioned the HuggingFace doesn't detect their token, even though it's in the .env file. Here are things to try:   \n",
    "1. Restart Cursor   \n",
    "2. Rerun load_dotenv(override=True) and use a new terminal (the + button on the top right of the Terminal)   \n",
    "3. In the Terminal, run this before the gradio deploy: `$env:HF_TOKEN = \"hf_XXXX\"`  \n",
    "Thank you James and Martins for these tips.  \n",
    "\n",
    "#### More about these secrets:\n",
    "\n",
    "If you're confused by what's going on with these secrets: it just wants you to enter the key name and value for each of your secrets -- so you would enter:  \n",
    "`OPENAI_API_KEY`  \n",
    "Followed by:  \n",
    "`sk-proj-...`  \n",
    "\n",
    "And if you don't want to set secrets this way, or something goes wrong with it, it's no problem - you can change your secrets later:  \n",
    "1. Log in to HuggingFace website  \n",
    "2. Go to your profile screen via the Avatar menu on the top right  \n",
    "3. Select the Space you deployed  \n",
    "4. Click on the Settings wheel on the top right  \n",
    "5. You can scroll down to change your secrets, delete the space, etc.\n",
    "\n",
    "#### And now you should be deployed!\n",
    "\n",
    "Here is mine: https://huggingface.co/spaces/ed-donner/Career_Conversation\n",
    "\n",
    "I just got a push notification that a student asked me how they can become President of their country 😂😂\n",
    "\n",
    "For more information on deployment:\n",
    "\n",
    "https://www.gradio.app/guides/sharing-your-app#hosting-on-hf-spaces\n",
    "\n",
    "To delete your Space in the future:  \n",
    "1. Log in to HuggingFace\n",
    "2. From the Avatar menu, select your profile\n",
    "3. Click on the Space itself and select the settings wheel on the top right\n",
    "4. Scroll to the Delete section at the bottom\n",
    "5. ALSO: delete the README file that Gradio may have created inside this 1_foundations folder (otherwise it won't ask you the questions the next time you do a gradio deploy)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "So there we go, and it's come through very nicely, and I'm happy to say that I've been alerted about my own email address. So hopefully that is clear for you. You now see that, now if I take this off, hopefully everyone has now deployed as well. Career Conversations 2 is right there, there it is as well, so I now have two Career Conversations. So this is how you can interact with a deployed app, and also Hugging Face gives you a great way that you can just embed this in your own website. So I have a number of Hugging Face spaces that run on my website, like I've got this Connect 4 game that you can play against different LLMs, and this is just a Hugging Face space, but if you look it looks like it's just coming from my own personal website. You can do the same thing, instructions are on the Hugging Face spaces site, and that way you can have your web page having embedded within it your Career Conversation, where people can come to your website, they can have a virtual conversation with your avatar about your career, about your interests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f48ac",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            <h2 style=\"color:brown;\">Exercise</h2>\n",
    "            <span style=\"color:brown;\">• First and foremost, deploy this for yourself! It's a real, valuable tool - the future resume..<br/>\n",
    "            • Next, improve the resources - add better context about yourself. If you know RAG, then add a knowledge base about you.<br/>\n",
    "            • Add in more tools! You could have a SQL database with common Q&A that the LLM could read and write from?<br/>\n",
    "            • Bring in the Evaluator from the last lab, and add other Agentic patterns.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            <h2 style=\"color:blue;\">Commercial implications</h2>\n",
    "            <span style=\"color:blue;\">Aside from the obvious (your career alter-ego) this has business applications in any situation where you need an AI assistant with domain expertise and an ability to interact with the real world.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
