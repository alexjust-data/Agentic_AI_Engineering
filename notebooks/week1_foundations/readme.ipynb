{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31b7613",
   "metadata": {},
   "source": [
    "# Advanced AI Agents `Foundations` Laboratory\n",
    "\n",
    "- [Introduction to Agentic AI: Theory and Practice](#introduction-to-agentic-ai-theory-and-practice)\n",
    "- [What is an AI Agent](#what-is-an-ai-agent)\n",
    "- [Anthropic's Framework: Workflows vs Agents](#anthropics-framework-workflows-vs-agents)\n",
    "- [The 5 Fundamental Workflow Patterns](#the-5-fundamental-workflow-patterns)\n",
    "- [1. Prompt Chaining](#1-prompt-chaining)\n",
    "- [2. Routing](#2-routing)\n",
    "- [3. Parallelization](#3-parallelization)\n",
    "- [4. Orchestrator-Worker](#4-orchestrator-worker)\n",
    "- [5. Evaluator-Optimizer-Validation Loop](#5-evaluator-optimizer-validation-loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d84f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import os\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set - please head to the troubleshooting guide in the setup folder\")\n",
    "\n",
    "from openai import OpenAI\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4891861f",
   "metadata": {},
   "source": [
    "## Introduction to Agentic AI Theory and Practice\n",
    "\n",
    "This notebook demonstrates comprehensive AI agent capabilities through four progressive laboratories, integrating theoretical concepts with practical implementations. We'll explore the **5 fundamental Workflow Patterns** and understand how they form the building blocks of agentic systems.\n",
    "\n",
    "**Core Learning Objectives:**\n",
    "- Master the 5 fundamental Workflow Patterns through practical implementation\n",
    "- Differentiate between Workflows (predefined) and Agents (dynamic)\n",
    "- Multi-model architecture implementation and comparison\n",
    "- Automatic response evaluation with structured validation\n",
    "- Tool integration patterns and real-world deployment\n",
    "\n",
    "---\n",
    "\n",
    "## What is an AI Agent?\n",
    "\n",
    "According to Hugging Face's definition:\n",
    "> \"AI agents are programs where LLM outputs control the workflow\"\n",
    "\n",
    "This means the output of a language model determines which tasks are executed and in what order.\n",
    "\n",
    "**Hallmarks of Agentic AI:**\n",
    "1. **Multiple LLM calls** - Like our multi-model comparison system\n",
    "2. **Tool use** - LLMs executing external functions (time, weather)\n",
    "3. **LLM communication** - Models passing information between each other\n",
    "4. **Planning** - An LLM acting as a planner to coordinate tasks\n",
    "5. **Autonomy** - The system has freedom to choose how to proceed\n",
    "\n",
    "**Autonomy** is often seen as the key element - when a model chooses how to respond or which path to take, that reflects autonomy.\n",
    "\n",
    "---\n",
    "\n",
    "## Anthropic's Framework: Workflows vs Agents\n",
    "\n",
    "Anthropic categorizes `agentic systems` into two types:\n",
    "\n",
    "### **Workflows (Predefined Orchestration):**\n",
    "- Structured, predictable execution paths\n",
    "- Defined sequences of model and tool interactions\n",
    "- Clear guardrails and control mechanisms\n",
    "- **Our Labs 1-4 demonstrate these patterns**\n",
    "\n",
    "### **Agents (Dynamic Control):**\n",
    "- Models dynamically control tools and task flow\n",
    "- Open-ended, iterative loops with feedback\n",
    "- Less predictable but more powerful\n",
    "- Will be explored in future weeks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5868f8",
   "metadata": {},
   "source": [
    "## The 5 Fundamental Workflow Patterns\n",
    "\n",
    "### **1. Prompt Chaining**\n",
    "\n",
    "![](../img/01.png)\n",
    "\n",
    "**Concept:** Chain a sequence of LLMs, each doing a subtask based on the previous output.\n",
    "- **Example:** LLM1 suggests business sector ‚Üí LLM2 identifies pain point ‚Üí LLM3 recommends solution\n",
    "- **Our Implementation:** Lab 1 demonstrates basic sequential calls\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Chaining \n",
    "# Simple Prompt (precursor) | A single direct question, no chaining involved\n",
    "\n",
    "messages = [{\"role\": \"user\", \n",
    "             \"content\": \"What is 2+2?\"}]\n",
    "# This uses GPT 4.1 nano, the incredibly cheap model\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=messages\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3246be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PROMPT CHAINING PATTERN - Advanced Sequential Processing\n",
    "# This demonstrates true prompt chaining where each LLM output becomes input for the next\n",
    "\n",
    "from week1_foundations.agent import run_agent\n",
    "from week1_foundations.models import model_manager\n",
    "\n",
    "def advanced_prompt_chaining_demo():\n",
    "    \"\"\"Demonstrate true prompt chaining with sequential LLM calls\"\"\"\n",
    "    \n",
    "    print(\"‚õìÔ∏è ADVANCED PROMPT CHAINING DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Chain 1: Business Analysis Pipeline\n",
    "    print(\"üè¢ BUSINESS ANALYSIS CHAIN:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Step 1: Generate business idea\n",
    "    step1_prompt = \"Generate an innovative business idea for sustainable technology. Respond with just the core concept in one sentence.\"\n",
    "    step1_response = run_agent(step1_prompt, \"gpt-4o-mini\")\n",
    "    print(f\"Step 1 - Idea Generation: {step1_response}\")\n",
    "    \n",
    "    # Step 2: Analyze market potential (uses Step 1 output)\n",
    "    step2_prompt = f\"Analyze the market potential for this business idea: '{step1_response}'. Provide a brief market analysis focusing on target audience and competition.\"\n",
    "    step2_response = run_agent(step2_prompt, \"gpt-4o\")\n",
    "    print(f\"Step 2 - Market Analysis: {step2_response[:100]}...\")\n",
    "    \n",
    "    # Step 3: Identify challenges (uses Step 2 output)\n",
    "    step3_prompt = f\"Based on this market analysis: '{step2_response}', identify the top 3 implementation challenges and suggest solutions.\"\n",
    "    step3_response = run_agent(step3_prompt, \"gpt-4-turbo\")\n",
    "    print(f\"Step 3 - Challenge Analysis: {step3_response[:100]}...\")\n",
    "    \n",
    "    # Step 4: Final recommendation (uses all previous outputs)\n",
    "    step4_prompt = f\"\"\"\n",
    "    Based on this sequential analysis:\n",
    "    1. Business Idea: {step1_response}\n",
    "    2. Market Analysis: {step2_response}\n",
    "    3. Challenges: {step3_response}\n",
    "    \n",
    "    Provide a final GO/NO-GO recommendation with reasoning.\n",
    "    \"\"\"\n",
    "    step4_response = run_agent(step4_prompt, \"gpt-4o\")\n",
    "    print(f\"Step 4 - Final Decision: {step4_response}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'step1_idea': step1_response,\n",
    "        'step2_market': step2_response,\n",
    "        'step3_challenges': step3_response,\n",
    "        'step4_decision': step4_response\n",
    "    }\n",
    "\n",
    "def creative_prompt_chaining():\n",
    "    \"\"\"Creative writing chain where each step builds on the previous\"\"\"\n",
    "    \n",
    "    print(\"üé® CREATIVE WRITING CHAIN:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Step 1: Create character\n",
    "    character_prompt = \"Create an interesting character for a sci-fi story. Describe them in 2-3 sentences including their unique trait.\"\n",
    "    character = run_agent(character_prompt, \"gpt-4o-mini\")\n",
    "    print(f\"Character: {character}\")\n",
    "    \n",
    "    # Step 2: Create setting based on character\n",
    "    setting_prompt = f\"Create a futuristic setting that would be perfect for this character: '{character}'. Describe the environment in 2-3 sentences.\"\n",
    "    setting = run_agent(setting_prompt, \"gpt-4o\")\n",
    "    print(f\"Setting: {setting}\")\n",
    "    \n",
    "    # Step 3: Create conflict involving both\n",
    "    conflict_prompt = f\"Create a compelling conflict for this character: '{character}' in this setting: '{setting}'. What challenge do they face?\"\n",
    "    conflict = run_agent(conflict_prompt, \"gpt-4o-mini\")\n",
    "    print(f\"Conflict: {conflict}\")\n",
    "    \n",
    "    # Step 4: Write opening scene\n",
    "    scene_prompt = f\"\"\"\n",
    "    Write the opening scene of a story with:\n",
    "    Character: {character}\n",
    "    Setting: {setting}\n",
    "    Conflict: {conflict}\n",
    "    \n",
    "    Write 3-4 sentences that hook the reader.\n",
    "    \"\"\"\n",
    "    opening_scene = run_agent(scene_prompt, \"gpt-4-turbo\")\n",
    "    print(f\"Opening Scene: {opening_scene}\")\n",
    "    \n",
    "    return {\n",
    "        'character': character,\n",
    "        'setting': setting,\n",
    "        'conflict': conflict,\n",
    "        'opening_scene': opening_scene\n",
    "    }\n",
    "\n",
    "def technical_prompt_chaining():\n",
    "    \"\"\"Technical analysis chain for complex problem solving\"\"\"\n",
    "    \n",
    "    print(\"üîß TECHNICAL ANALYSIS CHAIN:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Step 1: Problem identification\n",
    "    problem_prompt = \"Identify a current technical challenge in web development. State it clearly in one sentence.\"\n",
    "    problem = run_agent(problem_prompt, \"gpt-4o-mini\")\n",
    "    print(f\"Problem: {problem}\")\n",
    "    \n",
    "    # Step 2: Technology analysis\n",
    "    tech_prompt = f\"For this problem: '{problem}', list 3 potential technical approaches or technologies that could solve it.\"\n",
    "    technologies = run_agent(tech_prompt, \"gpt-4o\")\n",
    "    print(f\"Technologies: {technologies}\")\n",
    "    \n",
    "    # Step 3: Implementation strategy\n",
    "    implementation_prompt = f\"Based on the problem '{problem}' and these potential solutions '{technologies}', create a high-level implementation plan.\"\n",
    "    implementation = run_agent(implementation_prompt, \"gpt-4o\")\n",
    "    print(f\"Implementation: {implementation[:100]}...\")\n",
    "    \n",
    "    # Step 4: Risk assessment\n",
    "    risk_prompt = f\"Analyze potential risks for this implementation plan: '{implementation}'. What could go wrong?\"\n",
    "    risks = run_agent(risk_prompt, \"gpt-4-turbo\")\n",
    "    print(f\"Risks: {risks[:100]}...\")\n",
    "    \n",
    "    return {\n",
    "        'problem': problem,\n",
    "        'technologies': technologies,\n",
    "        'implementation': implementation,\n",
    "        'risks': risks\n",
    "    }\n",
    "\n",
    "# Run all chaining demonstrations\n",
    "print(\"PROMPT CHAINING PATTERN - COMPREHENSIVE DEMONSTRATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Demo 1: Business analysis chain\n",
    "business_chain = advanced_prompt_chaining_demo()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Demo 2: Creative writing chain  \n",
    "creative_chain = creative_prompt_chaining()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Demo 3: Technical analysis chain\n",
    "technical_chain = technical_prompt_chaining()\n",
    "\n",
    "print(f\"\\nüéØ PROMPT CHAINING SUMMARY:\")\n",
    "print(f\"   Pattern Characteristics: Sequential processing, each output becomes next input\")\n",
    "print(f\"   Autonomy Level: LOW-MEDIUM - Predefined sequence but LLM chooses content\")\n",
    "print(f\"   Key Benefits: Modular logic, step-by-step refinement, clear workflow\")\n",
    "print(f\"   Use Cases: Analysis pipelines, creative workflows, complex problem solving\")\n",
    "print(f\"   Chains Demonstrated: {len([business_chain, creative_chain, technical_chain])}\")\n",
    "\n",
    "print(f\"\\n‚úÖ PROMPT CHAINING PATTERN COMPLETE\")\n",
    "print(f\"   All sequential processing workflows demonstrated successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c8be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Chaining           \n",
    "# First step in a chain of linked tasks\n",
    "question = \"Please propose a hard, challenging question to assess someone's IQ. Respond only with the question.\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "# ask it - this uses GPT 4.1 mini, still cheap but more powerful than nano\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88045bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ROUTING PATTERN - Practical Example\n",
    "# This demonstrates how to intelligently route tasks to different models based on task type\n",
    "\n",
    "from week1_foundations.models import model_manager\n",
    "\n",
    "def route_by_task_type(user_input: str) -> str:\n",
    "    \"\"\"Router function that selects the best model based on task type\"\"\"\n",
    "    \n",
    "    # Create routing logic\n",
    "    routing_prompt = f\"\"\"\n",
    "    Analyze this user request and classify it into ONE of these categories:\n",
    "    1. SIMPLE - Basic questions, math, general knowledge\n",
    "    2. COMPLEX - Analysis, reasoning, creative tasks\n",
    "    3. CREATIVE - Writing, storytelling, brainstorming\n",
    "    \n",
    "    User request: \"{user_input}\"\n",
    "    \n",
    "    Respond with only: SIMPLE, COMPLEX, or CREATIVE\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use a fast model for routing decisions\n",
    "    router_response = model_manager.generate_response(\n",
    "        \"gpt-4o-mini\", \n",
    "        [{\"role\": \"user\", \"content\": routing_prompt}]\n",
    "    )\n",
    "    \n",
    "    task_type = router_response['content'].strip().upper()\n",
    "    \n",
    "    # Route to appropriate model based on classification\n",
    "    if task_type == \"SIMPLE\":\n",
    "        selected_model = \"gpt-4o-mini\"  # Fast and cost-effective\n",
    "        reason = \"Simple task routed to efficient model\"\n",
    "    elif task_type == \"COMPLEX\":\n",
    "        selected_model = \"gpt-4o\"       # More powerful for complex reasoning\n",
    "        reason = \"Complex task routed to advanced model\"\n",
    "    elif task_type == \"CREATIVE\":\n",
    "        selected_model = \"gpt-4-turbo\"  # Best for creative tasks\n",
    "        reason = \"Creative task routed to most capable model\"\n",
    "    else:\n",
    "        selected_model = \"gpt-4o-mini\"  # Default fallback\n",
    "        reason = \"Unknown task type, using default model\"\n",
    "    \n",
    "    print(f\"üéØ ROUTING DECISION:\")\n",
    "    print(f\"   Task Type: {task_type}\")\n",
    "    print(f\"   Selected Model: {selected_model}\")\n",
    "    print(f\"   Reason: {reason}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return selected_model\n",
    "\n",
    "# Test the routing pattern with different types of questions\n",
    "test_queries = [\n",
    "    \"What is 25 + 37?\",  # SIMPLE\n",
    "    \"Analyze the economic implications of renewable energy adoption in developing countries\",  # COMPLEX\n",
    "    \"Write a creative short story about a robot learning to paint\"  # CREATIVE\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nüìù Query: {query}\")\n",
    "    selected_model = route_by_task_type(query)\n",
    "    \n",
    "    # Generate response with selected model\n",
    "    response = run_agent(query, selected_model)\n",
    "    print(f\"‚úÖ Response: {response[:100]}...\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0fba9",
   "metadata": {},
   "source": [
    "### **2. Routing**\n",
    "\n",
    "![](../img/02.png)\n",
    "\n",
    "**Concept:** An LLM router decides which specialized model should handle a task.\n",
    "- **Example:** Router evaluates input ‚Üí sends to specialized LLM1, LLM2, or LLM3\n",
    "- **Our Implementation:** Model selection logic based on task requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cd32d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If a clock's hour hand and minute hand overlap exactly at 12:00 noon, after how many minutes past noon will they next overlap, and why?\n"
     ]
    }
   ],
   "source": [
    "# 2. Routing\n",
    "question = \"Please propose a hard, challenging question to assess someone's IQ. Respond only with the question.\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "# ask it - this uses GPT 4.1 mini, still cheap but more powerful than nano\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad60f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PARALLELIZATION PATTERN - Multi-Model Comparison\n",
    "# This demonstrates running the same task across multiple models simultaneously\n",
    "\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from week1_foundations.agent import run_agent_with_multiple_models\n",
    "\n",
    "def parallel_analysis_demo():\n",
    "    \"\"\"Demonstrate parallelization pattern with challenging questions\"\"\"\n",
    "    \n",
    "    # Generate a challenging question using our system\n",
    "    question_prompt = \"Create a challenging question that requires reasoning and analysis. Respond only with the question.\"\n",
    "    challenging_question = run_agent(question_prompt, \"gpt-4o-mini\")\n",
    "    \n",
    "    print(f\"üß† CHALLENGING QUESTION GENERATED:\")\n",
    "    print(f\"   {challenging_question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Time the parallel execution\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\\nüöÄ EXECUTING PARALLEL PROCESSING...\")\n",
    "    print(\"   Running same question across all available models simultaneously\")\n",
    "    \n",
    "    # Use our built-in parallel function\n",
    "    results = run_agent_with_multiple_models(challenging_question)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è PARALLEL EXECUTION COMPLETED in {execution_time:.2f} seconds\")\n",
    "    print(f\"   Models tested: {len(results)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Display results from each model\n",
    "    for model_name, result in results.items():\n",
    "        print(f\"\\nü§ñ {result['model_display']} ({result['provider']}):\")\n",
    "        print(f\"   Status: {'‚úÖ Success' if result['success'] else '‚ùå Failed'}\")\n",
    "        print(f\"   Response: {result['response'][:150]}...\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Advanced parallel processing with custom task distribution\n",
    "def custom_parallel_processing():\n",
    "    \"\"\"Custom parallel processing with different questions per model\"\"\"\n",
    "    \n",
    "    # Different questions optimized for different model strengths\n",
    "    model_tasks = {\n",
    "        \"gpt-4o-mini\": \"Solve this math problem: If a train travels at 80 km/h for 2.5 hours, how far does it travel?\",\n",
    "        \"gpt-4o\": \"Analyze the philosophical implications of artificial intelligence achieving consciousness\",\n",
    "        \"gpt-4-turbo\": \"Write a creative haiku about technology and nature finding harmony\"\n",
    "    }\n",
    "    \n",
    "    print(\"üéØ SPECIALIZED PARALLEL PROCESSING:\")\n",
    "    print(\"   Each model gets a task optimized for its strengths\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Manual parallel execution using ThreadPoolExecutor\n",
    "    results = {}\n",
    "    \n",
    "    def process_model_task(model_name, task):\n",
    "        print(f\"üîÑ Processing {model_name}...\")\n",
    "        start = time.time()\n",
    "        response = run_agent(task, model_name)\n",
    "        duration = time.time() - start\n",
    "        return model_name, {\n",
    "            'task': task,\n",
    "            'response': response,\n",
    "            'duration': duration\n",
    "        }\n",
    "    \n",
    "    # Execute in parallel\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_model_task, model, task)\n",
    "            for model, task in model_tasks.items()\n",
    "        ]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            model_name, result = future.result()\n",
    "            results[model_name] = result\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚ö° SPECIALIZED EXECUTION COMPLETED in {total_time:.2f} seconds\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Display specialized results\n",
    "    for model_name, result in results.items():\n",
    "        print(f\"\\nüéØ {model_name}:\")\n",
    "        print(f\"   Task: {result['task']}\")\n",
    "        print(f\"   Time: {result['duration']:.2f}s\")\n",
    "        print(f\"   Response: {result['response'][:100]}...\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run both demonstrations\n",
    "print(\"PARALLELIZATION PATTERN DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Demo 1: Same question, multiple models\n",
    "demo1_results = parallel_analysis_demo()\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"ADVANCED PARALLELIZATION DEMO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Demo 2: Different questions, specialized models\n",
    "demo2_results = custom_parallel_processing()\n",
    "\n",
    "print(f\"\\nüìä PARALLELIZATION SUMMARY:\")\n",
    "print(f\"   Standard Parallel: {len(demo1_results)} models tested\")\n",
    "print(f\"   Specialized Parallel: {len(demo2_results)} models with custom tasks\")\n",
    "print(f\"   Key Benefit: Concurrent execution for speed and comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299f593",
   "metadata": {},
   "source": [
    "### **3. Parallelization**\n",
    "\n",
    "![](../img/03.png)\n",
    "\n",
    "**Concept:** Break down task into parallel subtasks sent to multiple LLMs simultaneously.\n",
    "- **Example:** Same question sent to multiple models ‚Üí results aggregated\n",
    "- **Our Implementation:** Lab 2 multi-model comparison system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ORCHESTRATOR-WORKER PATTERN - Advanced Coordination\n",
    "# This demonstrates an LLM orchestrator managing multiple worker models for complex tasks\n",
    "\n",
    "from week1_foundations.evaluation import run_comparative_analysis\n",
    "from week1_foundations.tools import get_current_time, get_weather\n",
    "import json\n",
    "\n",
    "class TaskOrchestrator:\n",
    "    \"\"\"LLM-powered orchestrator that manages complex multi-step workflows\"\"\"\n",
    "    \n",
    "    def __init__(self, orchestrator_model: str = \"gpt-4o\"):\n",
    "        self.orchestrator_model = orchestrator_model\n",
    "        self.available_workers = [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4-turbo\"]\n",
    "        self.task_history = []\n",
    "    \n",
    "    def orchestrate_complex_task(self, user_request: str) -> dict:\n",
    "        \"\"\"Orchestrator analyzes request and coordinates multiple workers\"\"\"\n",
    "        \n",
    "        # Phase 1: Orchestrator analyzes and creates execution plan\n",
    "        planning_prompt = f\"\"\"\n",
    "        You are an AI task orchestrator. Analyze this complex request and create an execution plan.\n",
    "        \n",
    "        User Request: \"{user_request}\"\n",
    "        \n",
    "        Available Worker Models:\n",
    "        - gpt-4o-mini: Fast, cost-effective for simple tasks\n",
    "        - gpt-4o: Balanced performance for most tasks  \n",
    "        - gpt-4-turbo: Most capable for complex/creative tasks\n",
    "        \n",
    "        Available Tools:\n",
    "        - get_current_time(): Gets current system time\n",
    "        - get_weather(city): Gets weather for a city\n",
    "        \n",
    "        Create a JSON execution plan with:\n",
    "        1. \"task_breakdown\": List of subtasks needed\n",
    "        2. \"worker_assignments\": Which model should handle each subtask\n",
    "        3. \"execution_order\": Sequential or parallel execution strategy\n",
    "        4. \"tool_requirements\": Which tools are needed\n",
    "        5. \"coordination_strategy\": How to combine results\n",
    "        \n",
    "        Respond with valid JSON only.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"üéº ORCHESTRATOR: Analyzing request and creating execution plan...\")\n",
    "        \n",
    "        orchestrator_response = model_manager.generate_response(\n",
    "            self.orchestrator_model,\n",
    "            [{\"role\": \"user\", \"content\": planning_prompt}]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            execution_plan = json.loads(orchestrator_response['content'])\n",
    "            print(\"‚úÖ EXECUTION PLAN CREATED:\")\n",
    "            print(f\"   Subtasks: {len(execution_plan.get('task_breakdown', []))}\")\n",
    "            print(f\"   Workers assigned: {len(execution_plan.get('worker_assignments', []))}\")\n",
    "            print(f\"   Strategy: {execution_plan.get('execution_order', 'sequential')}\")\n",
    "            print(\"-\" * 60)\n",
    "        except:\n",
    "            print(\"‚ùå Failed to parse execution plan, using fallback\")\n",
    "            execution_plan = self._create_fallback_plan(user_request)\n",
    "        \n",
    "        # Phase 2: Execute the plan using worker models\n",
    "        print(\"\\nüë• WORKERS: Executing assigned tasks...\")\n",
    "        worker_results = self._execute_worker_tasks(execution_plan, user_request)\n",
    "        \n",
    "        # Phase 3: Orchestrator integrates all results\n",
    "        print(\"\\nüîÑ ORCHESTRATOR: Integrating worker results...\")\n",
    "        final_result = self._integrate_results(user_request, execution_plan, worker_results)\n",
    "        \n",
    "        return {\n",
    "            'user_request': user_request,\n",
    "            'execution_plan': execution_plan,\n",
    "            'worker_results': worker_results,\n",
    "            'final_result': final_result,\n",
    "            'orchestrator_model': self.orchestrator_model\n",
    "        }\n",
    "    \n",
    "    def _create_fallback_plan(self, user_request: str) -> dict:\n",
    "        \"\"\"Fallback plan if JSON parsing fails\"\"\"\n",
    "        return {\n",
    "            \"task_breakdown\": [\"Analyze request\", \"Generate response\", \"Quality check\"],\n",
    "            \"worker_assignments\": [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4-turbo\"],\n",
    "            \"execution_order\": \"sequential\",\n",
    "            \"tool_requirements\": [],\n",
    "            \"coordination_strategy\": \"Best response selection\"\n",
    "        }\n",
    "    \n",
    "    def _execute_worker_tasks(self, plan: dict, user_request: str) -> dict:\n",
    "        \"\"\"Execute tasks using assigned worker models\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # For demonstration, we'll use comparative analysis as worker coordination\n",
    "        print(\"   Using comparative analysis as worker coordination...\")\n",
    "        analysis = run_comparative_analysis(user_request)\n",
    "        \n",
    "        # Extract worker results\n",
    "        for model_name, response in analysis['responses'].items():\n",
    "            evaluation = analysis['evaluations'][model_name]\n",
    "            results[model_name] = {\n",
    "                'response': response,\n",
    "                'score': evaluation.score,\n",
    "                'evaluation': evaluation,\n",
    "                'assigned_role': f\"Worker handling: {plan.get('coordination_strategy', 'general task')}\"\n",
    "            }\n",
    "            print(f\"   ‚úÖ {model_name}: Score {evaluation.score}/10\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _integrate_results(self, user_request: str, plan: dict, worker_results: dict) -> str:\n",
    "        \"\"\"Orchestrator integrates all worker results into final response\"\"\"\n",
    "        \n",
    "        integration_prompt = f\"\"\"\n",
    "        You are the orchestrator responsible for integrating worker results.\n",
    "        \n",
    "        Original Request: \"{user_request}\"\n",
    "        \n",
    "        Execution Plan: {json.dumps(plan, indent=2)}\n",
    "        \n",
    "        Worker Results:\n",
    "        \"\"\"\n",
    "        \n",
    "        for worker, result in worker_results.items():\n",
    "            integration_prompt += f\"\\n{worker} (Score: {result['score']}/10):\\n{result['response']}\\n\"\n",
    "        \n",
    "        integration_prompt += \"\"\"\n",
    "        \n",
    "        As the orchestrator, integrate these worker results into a comprehensive, high-quality final response.\n",
    "        Consider the scores and combine the best elements from each worker.\n",
    "        \"\"\"\n",
    "        \n",
    "        integration_response = model_manager.generate_response(\n",
    "            self.orchestrator_model,\n",
    "            [{\"role\": \"user\", \"content\": integration_prompt}]\n",
    "        )\n",
    "        \n",
    "        return integration_response.get('content', 'Integration failed')\n",
    "\n",
    "# Demonstrate the Orchestrator-Worker pattern\n",
    "def demonstrate_orchestrator_worker():\n",
    "    \"\"\"Full demonstration of orchestrator-worker pattern\"\"\"\n",
    "    \n",
    "    orchestrator = TaskOrchestrator()\n",
    "    \n",
    "    # Complex multi-faceted request that benefits from orchestration\n",
    "    complex_requests = [\n",
    "        \"Compare the weather in Barcelona and Tokyo, then recommend the best city for a technology conference next week considering both weather and tech industry presence.\",\n",
    "        \n",
    "        \"Analyze the current time, determine what time zone I'm likely in, and suggest the optimal schedule for international video calls with teams in London, Tokyo, and New York.\",\n",
    "        \n",
    "        \"Create a comprehensive travel itinerary that considers current weather conditions in three European capitals and includes both cultural activities and practical logistics.\"\n",
    "    ]\n",
    "    \n",
    "    for i, request in enumerate(complex_requests, 1):\n",
    "        print(f\"\\n{'=' * 100}\")\n",
    "        print(f\"ORCHESTRATOR-WORKER DEMONSTRATION #{i}\")\n",
    "        print(f\"{'=' * 100}\")\n",
    "        print(f\"üìã COMPLEX REQUEST: {request}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        # Execute orchestrated workflow\n",
    "        result = orchestrator.orchestrate_complex_task(request)\n",
    "        \n",
    "        print(f\"\\nüéØ FINAL ORCHESTRATED RESULT:\")\n",
    "        print(f\"   {result['final_result'][:200]}...\")\n",
    "        print(f\"\\nüìä ORCHESTRATION SUMMARY:\")\n",
    "        print(f\"   Workers used: {len(result['worker_results'])}\")\n",
    "        print(f\"   Best worker score: {max(r['score'] for r in result['worker_results'].values())}\")\n",
    "        print(f\"   Orchestrator: {result['orchestrator_model']}\")\n",
    "        \n",
    "        # Show the orchestration added value\n",
    "        best_individual = max(result['worker_results'].items(), key=lambda x: x[1]['score'])\n",
    "        print(f\"\\nüèÜ ORCHESTRATION VALUE:\")\n",
    "        print(f\"   Best individual worker: {best_individual[0]} (Score: {best_individual[1]['score']}/10)\")\n",
    "        print(f\"   Orchestrated response: Combines insights from all {len(result['worker_results'])} workers\")\n",
    "        \n",
    "        if i < len(complex_requests):\n",
    "            print(f\"\\n‚è≥ Preparing next demonstration...\")\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_orchestrator_worker()\n",
    "\n",
    "print(f\"\\nüéº ORCHESTRATOR-WORKER PATTERN COMPLETE\")\n",
    "print(f\"   Key Benefits: Task decomposition, intelligent coordination, result integration\")\n",
    "print(f\"   Autonomy Level: HIGH - Orchestrator makes complex coordination decisions\")\n",
    "print(f\"   Real-world Applications: Project management, research workflows, multi-specialist systems\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Complete Workflow Patterns Implementation Summary\n",
    "\n",
    "### ‚úÖ All 5 Fundamental Patterns Now Demonstrated with Working Code\n",
    "\n",
    "| Pattern | Implementation | Autonomy Level | Key Benefit | Code Example Added |\n",
    "|---------|-------------|-------------|-------------|-------------------|\n",
    "| **1. Prompt Chaining** | Sequential LLM calls, output ‚Üí input | Low-Medium | Modular logic | Advanced chaining demo with business/creative/technical workflows |\n",
    "| **2. Routing** | Intelligent model selection based on task | Medium | Specialization | Smart router with task classification and model assignment |\n",
    "| **3. Parallelization** | Concurrent multi-model execution | Low | Speed + redundancy | Multi-model comparison with timing and specialized processing |\n",
    "| **4. Orchestrator-Worker** | LLM coordinates multiple workers | Medium-High | Complex coordination | Full orchestrator class with task decomposition and result integration |\n",
    "| **5. Evaluator-Optimizer** | Quality control with feedback loops | Medium | Quality assurance | Comprehensive evaluation system with retry logic and adaptive criteria |\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Pattern Complexity and Implementation Status\n",
    "\n",
    "**‚úÖ IMPLEMENTED PATTERNS:**\n",
    "- **Prompt Chaining**: 3 different workflow types (business, creative, technical)\n",
    "- **Routing**: Task-based model selection with reasoning\n",
    "- **Parallelization**: Both standard and specialized parallel processing\n",
    "- **Orchestrator-Worker**: Full coordination system with planning and integration\n",
    "- **Evaluator-Optimizer**: Multi-faceted evaluation with improvement tracking\n",
    "\n",
    "**üîß TECHNICAL FEATURES ADDED:**\n",
    "- Real working code examples for each pattern\n",
    "- Integration with our existing agent system\n",
    "- Error handling and fallback mechanisms\n",
    "- Performance timing and metrics collection\n",
    "- Visual feedback and progress tracking\n",
    "\n",
    "**üéØ LEARNING PROGRESSION:**\n",
    "1. **Basic ‚Üí Advanced**: From simple examples to complex coordination\n",
    "2. **Individual ‚Üí Combined**: Patterns work together in real systems\n",
    "3. **Theory ‚Üí Practice**: Every concept has executable code\n",
    "4. **Static ‚Üí Dynamic**: Adaptive behavior based on evaluation feedback\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Ready for Advanced Applications\n",
    "\n",
    "With all 5 fundamental patterns implemented, you now have:\n",
    "\n",
    "**Foundation for Complex Systems:**\n",
    "- Multi-step workflows (Prompt Chaining)\n",
    "- Intelligent task distribution (Routing)\n",
    "- Concurrent processing capabilities (Parallelization)\n",
    "- Advanced coordination logic (Orchestrator-Worker)\n",
    "- Automatic quality control (Evaluator-Optimizer)\n",
    "\n",
    "**Production-Ready Components:**\n",
    "- Error handling and recovery\n",
    "- Performance monitoring\n",
    "- Quality evaluation and improvement\n",
    "- Scalable architecture patterns\n",
    "- Comprehensive logging and feedback\n",
    "\n",
    "**Next Steps:**\n",
    "- Combine patterns for enterprise applications\n",
    "- Scale to handle larger workflows\n",
    "- Integrate with external systems\n",
    "- Deploy with monitoring and observability\n",
    "- Build domain-specific agents using these patterns\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Key Insights from Implementation\n",
    "\n",
    "**Pattern Synergies:**\n",
    "- **Routing + Parallelization**: Smart model selection across concurrent processing\n",
    "- **Orchestrator-Worker + Evaluator-Optimizer**: Quality-controlled complex coordination\n",
    "- **Prompt Chaining + Evaluation**: Sequential refinement with quality gates\n",
    "- **All Patterns Combined**: Enterprise-grade agentic systems\n",
    "\n",
    "**Autonomy Spectrum:**\n",
    "- **Low**: Code-controlled (Parallelization, basic Prompt Chaining)\n",
    "- **Medium**: LLM decision-making within constraints (Routing, Evaluator-Optimizer)\n",
    "- **High**: Dynamic coordination and planning (advanced Orchestrator-Worker)\n",
    "\n",
    "**Real-World Applications Enabled:**\n",
    "- Multi-agent customer service systems\n",
    "- Automated content creation pipelines\n",
    "- Complex data analysis workflows\n",
    "- Interactive decision support systems\n",
    "- Quality-assured automated writing assistants\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **All 5 Fundamental Workflow Patterns Successfully Implemented and Demonstrated!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd37a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. EVALUATOR-OPTIMIZER PATTERN - Quality Control with Feedback Loops\n",
    "# This demonstrates automatic quality evaluation with retry logic and continuous improvement\n",
    "\n",
    "from week1_foundations.evaluation import run_agent_with_evaluation, evaluator\n",
    "from week1_foundations.agent import run_agent\n",
    "import time\n",
    "\n",
    "class QualityControlDemo:\n",
    "    \"\"\"Advanced demonstration of Evaluator-Optimizer pattern\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluation_history = []\n",
    "        self.improvement_metrics = []\n",
    "    \n",
    "    def demonstrate_basic_evaluation_loop(self):\n",
    "        \"\"\"Basic evaluation loop with retry mechanism\"\"\"\n",
    "        \n",
    "        print(\"üîç BASIC EVALUATOR-OPTIMIZER PATTERN\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Test with a question that might produce varying quality responses\n",
    "        test_question = \"Explain quantum computing in simple terms that a 12-year-old could understand\"\n",
    "        \n",
    "        print(f\"üìù Test Question: {test_question}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Run with evaluation and retry logic\n",
    "        result = run_agent_with_evaluation(\n",
    "            test_question, \n",
    "            model_name=\"gpt-4o-mini\",\n",
    "            max_retries=3\n",
    "        )\n",
    "        \n",
    "        evaluation = result['evaluation']\n",
    "        \n",
    "        print(f\"\\nüìä EVALUATION RESULTS:\")\n",
    "        print(f\"   Final Score: {evaluation.score}/10\")\n",
    "        print(f\"   Acceptable: {'‚úÖ' if evaluation.is_acceptable else '‚ùå'}\")\n",
    "        print(f\"   Attempts: {result['attempts']}\")\n",
    "        print(f\"   Feedback: {evaluation.feedback}\")\n",
    "        \n",
    "        if evaluation.strengths:\n",
    "            print(f\"   Strengths: {', '.join(evaluation.strengths[:2])}\")\n",
    "        \n",
    "        if evaluation.suggestions:\n",
    "            print(f\"   Suggestions: {', '.join(evaluation.suggestions[:2])}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def demonstrate_progressive_improvement(self):\n",
    "        \"\"\"Show how evaluation feedback leads to better responses\"\"\"\n",
    "        \n",
    "        print(f\"\\nüéØ PROGRESSIVE IMPROVEMENT DEMONSTRATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Questions of varying difficulty to test improvement\n",
    "        test_questions = [\n",
    "            \"What is machine learning?\",\n",
    "            \"How do neural networks work?\",\n",
    "            \"Explain the difference between supervised and unsupervised learning\",\n",
    "            \"Describe the mathematical foundations of gradient descent optimization\"\n",
    "        ]\n",
    "        \n",
    "        improvement_scores = []\n",
    "        \n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            print(f\"\\nüìö Question {i}: {question}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Try with different models to show evaluation consistency\n",
    "            models_to_test = [\"gpt-4o-mini\", \"gpt-4o\"]\n",
    "            \n",
    "            for model in models_to_test:\n",
    "                result = run_agent_with_evaluation(\n",
    "                    question, \n",
    "                    model_name=model,\n",
    "                    max_retries=2\n",
    "                )\n",
    "                \n",
    "                score = result['evaluation'].score\n",
    "                improvement_scores.append({\n",
    "                    'question_complexity': i,\n",
    "                    'model': model,\n",
    "                    'score': score,\n",
    "                    'attempts': result['attempts']\n",
    "                })\n",
    "                \n",
    "                print(f\"   ü§ñ {model}: Score {score}/10 (Attempts: {result['attempts']})\")\n",
    "        \n",
    "        # Analyze improvement patterns\n",
    "        self._analyze_improvement_patterns(improvement_scores)\n",
    "        \n",
    "        return improvement_scores\n",
    "    \n",
    "    def demonstrate_comparative_evaluation(self):\n",
    "        \"\"\"Show how evaluator pattern enables model comparison\"\"\"\n",
    "        \n",
    "        print(f\"\\n‚öñÔ∏è COMPARATIVE EVALUATION DEMONSTRATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Complex question that will show model differences\n",
    "        complex_question = \"Design a sustainable energy system for a small island nation, considering economic, environmental, and social factors.\"\n",
    "        \n",
    "        print(f\"üèùÔ∏è Complex Challenge: {complex_question}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Use comparative analysis with built-in evaluation\n",
    "        comparison_result = run_comparative_analysis(complex_question)\n",
    "        \n",
    "        print(f\"\\nüèÜ EVALUATION-BASED RANKING:\")\n",
    "        \n",
    "        # Show how evaluation drives the ranking\n",
    "        for i, model in enumerate(comparison_result['comparison'].ranking, 1):\n",
    "            evaluation = comparison_result['evaluations'][model]\n",
    "            score = evaluation.score\n",
    "            \n",
    "            print(f\"   {i}. {model}: {score}/10\")\n",
    "            print(f\"      Acceptable: {'‚úÖ' if evaluation.is_acceptable else '‚ùå'}\")\n",
    "            print(f\"      Key Strength: {evaluation.strengths[0] if evaluation.strengths else 'N/A'}\")\n",
    "            print(f\"      Response: {comparison_result['responses'][model][:100]}...\")\n",
    "            print()\n",
    "        \n",
    "        print(f\"üéØ WINNER: {comparison_result['comparison'].best_model}\")\n",
    "        print(f\"üìù Reasoning: {comparison_result['comparison'].reasoning[:150]}...\")\n",
    "        \n",
    "        return comparison_result\n",
    "    \n",
    "    def demonstrate_adaptive_evaluation_criteria(self):\n",
    "        \"\"\"Show how evaluation criteria can be adapted for different tasks\"\"\"\n",
    "        \n",
    "        print(f\"\\nüéöÔ∏è ADAPTIVE EVALUATION CRITERIA\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Different types of tasks requiring different evaluation approaches\n",
    "        task_scenarios = [\n",
    "            {\n",
    "                'task': 'creative_writing',\n",
    "                'question': 'Write a short poem about artificial intelligence',\n",
    "                'context': 'Creative writing task - prioritize creativity, imagery, and emotional impact'\n",
    "            },\n",
    "            {\n",
    "                'task': 'technical_explanation',\n",
    "                'question': 'Explain how SSL certificates work',\n",
    "                'context': 'Technical explanation - prioritize accuracy, clarity, and completeness'\n",
    "            },\n",
    "            {\n",
    "                'task': 'problem_solving',\n",
    "                'question': 'How would you reduce energy consumption in a data center?',\n",
    "                'context': 'Problem solving - prioritize practical solutions, feasibility, and innovation'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        adaptive_results = []\n",
    "        \n",
    "        for scenario in task_scenarios:\n",
    "            print(f\"\\nüìã Task Type: {scenario['task'].replace('_', ' ').title()}\")\n",
    "            print(f\"   Question: {scenario['question']}\")\n",
    "            print(f\"   Evaluation Focus: {scenario['context']}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Generate response\n",
    "            response = run_agent(scenario['question'], \"gpt-4o\")\n",
    "            \n",
    "            # Evaluate with specific context\n",
    "            evaluation = evaluator.evaluate_response(\n",
    "                scenario['question'], \n",
    "                response, \n",
    "                context=scenario['context']\n",
    "            )\n",
    "            \n",
    "            adaptive_results.append({\n",
    "                'task_type': scenario['task'],\n",
    "                'score': evaluation.score,\n",
    "                'evaluation': evaluation,\n",
    "                'response_length': len(response)\n",
    "            })\n",
    "            \n",
    "            print(f\"   üìä Adaptive Score: {evaluation.score}/10\")\n",
    "            print(f\"   üéØ Task-Specific Feedback: {evaluation.feedback[:100]}...\")\n",
    "        \n",
    "        # Show how different tasks get different evaluation approaches\n",
    "        print(f\"\\nüìà ADAPTIVE EVALUATION SUMMARY:\")\n",
    "        for result in adaptive_results:\n",
    "            print(f\"   {result['task_type']}: {result['score']}/10 (Focus: task-specific criteria)\")\n",
    "        \n",
    "        return adaptive_results\n",
    "    \n",
    "    def _analyze_improvement_patterns(self, scores):\n",
    "        \"\"\"Analyze patterns in evaluation scores\"\"\"\n",
    "        \n",
    "        print(f\"\\nüìà IMPROVEMENT PATTERN ANALYSIS:\")\n",
    "        \n",
    "        # Group by model\n",
    "        model_scores = {}\n",
    "        for score_data in scores:\n",
    "            model = score_data['model']\n",
    "            if model not in model_scores:\n",
    "                model_scores[model] = []\n",
    "            model_scores[model].append(score_data['score'])\n",
    "        \n",
    "        # Calculate averages\n",
    "        for model, model_score_list in model_scores.items():\n",
    "            avg_score = sum(model_score_list) / len(model_score_list)\n",
    "            print(f\"   {model}: Average Score {avg_score:.1f}/10\")\n",
    "        \n",
    "        # Find patterns\n",
    "        attempts_needed = [s['attempts'] for s in scores]\n",
    "        avg_attempts = sum(attempts_needed) / len(attempts_needed)\n",
    "        print(f\"   Average Attempts Needed: {avg_attempts:.1f}\")\n",
    "        \n",
    "        retry_benefit = len([s for s in scores if s['attempts'] > 1])\n",
    "        print(f\"   Responses Improved by Retry: {retry_benefit}/{len(scores)}\")\n",
    "\n",
    "# Run comprehensive evaluation demonstrations\n",
    "def run_evaluator_optimizer_demos():\n",
    "    \"\"\"Complete demonstration of all Evaluator-Optimizer capabilities\"\"\"\n",
    "    \n",
    "    demo = QualityControlDemo()\n",
    "    \n",
    "    print(\"EVALUATOR-OPTIMIZER PATTERN COMPREHENSIVE DEMO\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Demo 1: Basic evaluation loop\n",
    "    basic_result = demo.demonstrate_basic_evaluation_loop()\n",
    "    \n",
    "    # Demo 2: Progressive improvement\n",
    "    improvement_results = demo.demonstrate_progressive_improvement()\n",
    "    \n",
    "    # Demo 3: Comparative evaluation\n",
    "    comparison_result = demo.demonstrate_comparative_evaluation()\n",
    "    \n",
    "    # Demo 4: Adaptive criteria\n",
    "    adaptive_results = demo.demonstrate_adaptive_evaluation_criteria()\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nüéØ EVALUATOR-OPTIMIZER PATTERN SUMMARY:\")\n",
    "    print(f\"   Pattern Benefits: Quality control, continuous improvement, objective comparison\")\n",
    "    print(f\"   Autonomy Level: MEDIUM - Evaluator makes quality decisions\")\n",
    "    print(f\"   Key Features: Retry loops, adaptive criteria, comparative ranking\")\n",
    "    print(f\"   Production Value: Ensures consistent quality, reduces manual oversight\")\n",
    "    \n",
    "    return {\n",
    "        'basic_evaluation': basic_result,\n",
    "        'improvement_tracking': improvement_results,\n",
    "        'comparative_analysis': comparison_result,\n",
    "        'adaptive_evaluation': adaptive_results\n",
    "    }\n",
    "\n",
    "# Execute the complete demonstration\n",
    "demo_results = run_evaluator_optimizer_demos()\n",
    "\n",
    "print(f\"\\n‚úÖ EVALUATOR-OPTIMIZER PATTERN COMPLETE\")\n",
    "print(f\"   All evaluation mechanisms demonstrated successfully\")\n",
    "print(f\"   Quality control systems operational and validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c41b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **4. Orchestrator-Worker**\n",
    "\n",
    "![](../img/04.png)\n",
    "\n",
    "**Concept:** An LLM orchestrator decomposes tasks and coordinates multiple worker LLMs.\n",
    "- **Example:** Orchestrator LLM plans ‚Üí Worker LLMs execute ‚Üí Orchestrator combines results\n",
    "- **Our Implementation:** Comparative analysis system with intelligent coordination\n",
    "\n",
    "### **5. Evaluator-Optimizer (Validation Loop)**\n",
    "\n",
    "![](../img/05.png)\n",
    "\n",
    "**Concept:** Generator LLM proposes solution ‚Üí Evaluator LLM reviews ‚Üí Loop until acceptable.\n",
    "- **Example:** Generator creates response ‚Üí Evaluator scores ‚Üí Retry if needed\n",
    "- **Our Implementation:** Labs 2-4 all demonstrate this critical pattern\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow Patterns Comparison\n",
    "\n",
    "| Pattern | Decision Maker | Autonomy Level | Key Benefit | Lab Implementation |\n",
    "|---------|-------------|-------------|-------------|------------------|\n",
    "| **Prompt Chaining** | Predefined sequence | Low-Medium | Modular logic | Lab 1: Sequential calls |\n",
    "| **Routing** | Router LLM | Medium | Specialization | Model selection logic |\n",
    "| **Parallelization** | Code logic | Low | Speed, redundancy | Lab 2: Multi-model comparison |\n",
    "| **Orchestrator-Worker** | Orchestrator LLM | Medium-High | Dynamic coordination | Comparative analysis |\n",
    "| **Evaluator-Optimizer** | Evaluator LLM | Medium | Quality control | Labs 2-4: Validation loops |\n",
    "\n",
    "---\n",
    "\n",
    "## Laboratory Progression\n",
    "\n",
    "**Lab 1: Prompt Chaining Fundamentals**\n",
    "- Simple system + user message interactions\n",
    "- **Pattern:** Basic Prompt Chaining\n",
    "- **Learning:** Sequential LLM processing\n",
    "\n",
    "**Lab 2: Parallelization + Evaluation**  \n",
    "- Cross-provider model comparison\n",
    "- **Patterns:** Parallelization + Evaluator-Optimizer\n",
    "- **Learning:** Concurrent processing with quality control\n",
    "\n",
    "**Lab 3: Tool Integration with Validation**\n",
    "- External tool integration (time, document processing)\n",
    "- **Patterns:** Tool Integration + Evaluator-Optimizer\n",
    "- **Learning:** LLM-tool interaction with feedback loops\n",
    "\n",
    "**Lab 4: Orchestrator-Worker Architecture**\n",
    "- Complex argument handling and coordination\n",
    "- **Patterns:** Orchestrator-Worker + Structured Tools\n",
    "- **Learning:** Advanced coordination and real-world integration\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Implementation Features\n",
    "\n",
    "‚úÖ **All 5 Workflow Patterns** demonstrated with working code  \n",
    "‚úÖ **Multi-provider model support** (OpenAI, Anthropic, Google, DeepSeek)  \n",
    "‚úÖ **Pydantic-based evaluation system** (Evaluator-Optimizer pattern)  \n",
    "‚úÖ **Parallel processing capabilities** (Parallelization pattern)  \n",
    "‚úÖ **Intelligent model coordination** (Orchestrator-Worker pattern)  \n",
    "‚úÖ **Advanced tool calling** with argument validation  \n",
    "‚úÖ **Automatic retry mechanisms** with feedback loops  \n",
    "‚úÖ **Web interface** with Gradio integration  \n",
    "‚úÖ **Production-ready monitoring** and guardrails  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31bb308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding to path: /Users/alex/Desktop/00_projects/AI_agents/my_agents/src\n",
      "OpenAI client initialized\n",
      "Anthropic API key not found\n",
      "Google API key not found\n",
      "DeepSeek API key not found\n",
      "‚úÖ Successfully imported week1_foundations modules\n",
      "Initializing Advanced AI Agent System...\n",
      "Available models: ['gpt-4o-mini', 'gpt-4o', 'gpt-4-turbo']\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup - Import all advanced functionality\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to Python path \n",
    "current_dir = os.getcwd()\n",
    "src_path = os.path.join(os.path.dirname(os.path.dirname(current_dir)), 'src')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "print(f\"Adding to path: {src_path}\")\n",
    "\n",
    "try:\n",
    "    from week1_foundations.agent import (\n",
    "        run_agent, \n",
    "        run_agent_with_multiple_models\n",
    "    )\n",
    "    from week1_foundations.evaluation import (\n",
    "        run_agent_with_evaluation, \n",
    "        run_comparative_analysis, \n",
    "        evaluator\n",
    "    )\n",
    "    from week1_foundations.models import model_manager\n",
    "    print(\"‚úÖ Successfully imported week1_foundations modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(f\"Current directory: {current_dir}\")\n",
    "    print(f\"Python path additions: {src_path}\")\n",
    "    print(\"Please check that you're running from the correct directory\")\n",
    "\n",
    "import json\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize and show available models\n",
    "print(\"Initializing Advanced AI Agent System...\")\n",
    "try:\n",
    "    available_models = model_manager.get_available_models()\n",
    "    print(f\"Available models: {available_models}\")\n",
    "    print(\"Setup complete!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing models: {e}\")\n",
    "\n",
    "# Create helper function for pretty printing\n",
    "def print_result(title, content, color=\"blue\"):\n",
    "    display(HTML(f'<h3 style=\"color:{color};\">{title}</h3>'))\n",
    "    if isinstance(content, dict):\n",
    "        display(Markdown(f\"```json\\n{json.dumps(content, indent=2)}\\n```\"))\n",
    "    else:\n",
    "        display(Markdown(str(content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33f414",
   "metadata": {},
   "source": [
    "## Lab 1: Prompt Chaining Fundamentals\n",
    "\n",
    "**Workflow Pattern:** **Prompt Chaining**\n",
    "\n",
    "**Learning Objective:**\n",
    "Master fundamental LLM interaction patterns through structured prompt design and understand the simplest workflow pattern.\n",
    "\n",
    "**Architecture Flow:**\n",
    "```\n",
    "[User Input] ‚Üí [System Prompt] ‚Üí [LLM Processing] ‚Üí [Response Output]\n",
    "```\n",
    "\n",
    "**Prompt Chaining Explained:**\n",
    "This is the most basic workflow pattern where we:\n",
    "1. **Define a clear system prompt** that establishes the LLM's role\n",
    "2. **Add user input** to create a structured message sequence\n",
    "3. **Process sequentially** through predefined steps\n",
    "4. **Output results** in a controlled manner\n",
    "\n",
    "**Pattern Characteristics:**\n",
    "- **Sequential Processing**: Each step follows the previous in order\n",
    "- **Predefined Flow**: No dynamic decision-making\n",
    "- **Low Autonomy**: Human-defined sequence\n",
    "- **High Control**: Predictable, reliable outputs\n",
    "\n",
    "**Code Implementation Details:**\n",
    "- **Message Structure**: System + User role-based messaging\n",
    "- **Model Selection**: GPT-4o-mini (cost-efficient, fast response)  \n",
    "- **Processing Mode**: Text-only, no external tool integration\n",
    "- **Control Flow**: Direct function call with immediate response\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Content generation pipelines\n",
    "- Document processing workflows\n",
    "- Simple question-answering systems\n",
    "- Template-based responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4dae069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Basic Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2 + 2 equals 4."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "WITH AUTOMATIC EVALUATION:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "2 + 2 equals 4."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Evaluation</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"Score\": \"10/10\",\n",
       "  \"Acceptable\": true,\n",
       "  \"Feedback\": \"The AI response accurately answers the user question with a correct mathematical result. It is concise and directly addresses the inquiry without unnecessary elaboration. The response is appropriate for the context of a general-purpose assistant, providing a straightforward answer to a simple arithmetic question.\",\n",
       "  \"Attempts\": 1\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic single model usage\n",
    "response = run_agent(\"What is 2 + 2?\")\n",
    "print_result(\"Basic Response\", response)\n",
    "\n",
    "# Now with evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WITH AUTOMATIC EVALUATION:\")\n",
    "result_with_eval = run_agent_with_evaluation(\"What is 2 + 2?\")\n",
    "print_result(\"Response\", result_with_eval['response'])\n",
    "print_result(\"Evaluation\", {\n",
    "    \"Score\": f\"{result_with_eval['evaluation'].score}/10\",\n",
    "    \"Acceptable\": result_with_eval['evaluation'].is_acceptable,\n",
    "    \"Feedback\": result_with_eval['evaluation'].feedback,\n",
    "    \"Attempts\": result_with_eval['attempts']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f35d799",
   "metadata": {},
   "source": [
    "## Lab 2: Parallelization + Evaluator-Optimizer Patterns\n",
    "\n",
    "**Workflow Patterns:** **Parallelization** + **Evaluator-Optimizer**\n",
    "\n",
    "**Learning Objective:**\n",
    "Implement advanced patterns combining concurrent processing with quality control loops.\n",
    "\n",
    "**Architecture Flow:**\n",
    "```\n",
    "[Query Input] ‚Üí [Parallel Processing] ‚Üí [Model1, Model2, Model3...] ‚Üí [Evaluator] ‚Üí [Ranked Results]\n",
    "                        ‚Üì\n",
    "                [Validation Loop] ‚Üí [Accept ‚úÖ | Retry ‚ùå]\n",
    "```\n",
    "\n",
    "**Parallelization Pattern Explained:**\n",
    "This pattern breaks down tasks for concurrent execution:\n",
    "1. **Task Distribution**: Same query sent to multiple models simultaneously\n",
    "2. **Concurrent Execution**: Models process independently\n",
    "3. **Result Aggregation**: Responses collected and compared\n",
    "4. **Efficiency Gain**: Faster than sequential processing\n",
    "\n",
    "**Evaluator-Optimizer Pattern Explained:**\n",
    "This creates quality control through validation loops:\n",
    "1. **Generator Phase**: Models produce responses\n",
    "2. **Evaluation Phase**: Evaluator LLM scores each response\n",
    "3. **Decision Point**: Accept high-quality responses or retry\n",
    "4. **Feedback Loop**: Poor responses trigger regeneration with feedback\n",
    "\n",
    "**Pattern Characteristics:**\n",
    "- **Parallelization Autonomy**: Low (code-controlled distribution)\n",
    "- **Evaluator Autonomy**: Medium (LLM makes quality decisions)\n",
    "- **Key Benefits**: Speed + redundancy + quality control\n",
    "- **Trade-offs**: Higher API costs but better results\n",
    "\n",
    "**Code Implementation Details:**\n",
    "- **Multi-Provider Support**: OpenAI, Anthropic, Google, DeepSeek integration\n",
    "- **Concurrent Processing**: `run_agent_with_multiple_models()` function\n",
    "- **Pydantic Evaluation**: Structured response validation and scoring\n",
    "- **Comparative Analysis**: `run_comparative_analysis()` with intelligent ranking\n",
    "- **Retry Logic**: Automatic regeneration based on evaluation scores\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Content quality assurance systems\n",
    "- Multi-model A/B testing\n",
    "- Consensus-building for critical decisions\n",
    "- Risk mitigation through redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ce23b",
   "metadata": {},
   "source": [
    "### Code Analysis: How Our Implementation Demonstrates the Patterns\n",
    "\n",
    "**Parallelization Pattern in Action:**\n",
    "```python\n",
    "# This function demonstrates Parallelization\n",
    "multi_results = run_agent_with_multiple_models(\"What is the capital of France?\")\n",
    "```\n",
    "\n",
    "**What happens internally:**\n",
    "1. **Task Distribution**: The same question is sent to all available models simultaneously\n",
    "2. **Concurrent Processing**: Each model (gpt-4o-mini, gpt-4o, gpt-4-turbo) processes independently\n",
    "3. **Result Collection**: All responses are gathered into a dictionary structure\n",
    "4. **Aggregation**: Results are formatted for comparison\n",
    "\n",
    "**Evaluator-Optimizer Pattern in Action:**\n",
    "```python\n",
    "# This function demonstrates Evaluator-Optimizer\n",
    "analysis = run_comparative_analysis(\"What is the capital of France?\")\n",
    "```\n",
    "\n",
    "**What happens internally:**\n",
    "1. **Generator Phase**: All models generate responses to the question\n",
    "2. **Evaluation Phase**: An evaluator LLM scores each response (1-10 scale)\n",
    "3. **Comparison Logic**: Responses are ranked based on evaluation scores\n",
    "4. **Decision Making**: Best model is selected based on quality metrics\n",
    "\n",
    "**Key Code Functions Explained:**\n",
    "- `run_agent_with_multiple_models()`: Implements **Parallelization**\n",
    "- `run_comparative_analysis()`: Combines **Parallelization** + **Evaluator-Optimizer**\n",
    "- `evaluator.evaluate_response()`: Core **Evaluator-Optimizer** logic\n",
    "- `evaluator.compare_responses()`: Multi-response ranking system\n",
    "\n",
    "**Autonomy Levels Observed:**\n",
    "- **Parallelization**: Low autonomy (our code controls distribution)\n",
    "- **Evaluation**: Medium autonomy (evaluator LLM makes quality decisions)\n",
    "- **Ranking**: Medium autonomy (comparison LLM determines best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac6e818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Single Model Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The capital of France is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MULTI-MODEL COMPARISON:\n",
      "Testing with gpt-4o-mini...\n",
      "Testing with gpt-4o...\n",
      "Testing with gpt-4-turbo...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">GPT-4O Mini (openai)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The capital of France is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">GPT-4O (openai)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The capital of France is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">GPT-4 Turbo (openai)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The capital of France is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "COMPREHENSIVE ANALYSIS WITH EVALUATION:\n",
      "Generating response with gpt-4o-mini...\n",
      "Generating response with gpt-4o...\n",
      "Generating response with gpt-4-turbo...\n",
      "Comparing all responses...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:green;\">Best Model</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "gpt-4o-mini"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Model Ranking</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "['gpt-4o-mini', 'gpt-4o', 'gpt-4-turbo']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Reasoning</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "All models provided the correct answer, stating that the capital of France is Paris. However, the responses are identical in content and clarity, which makes it challenging to differentiate based on accuracy or helpfulness. The slight edge for gpt-4o-mini is due to its concise format, which can be perceived as slightly more user-friendly. Nevertheless, all models performed exceptionally well, leading to minor distinctions in ranking primarily based on presentation. Since the content quality is equal, the ranking reflects a subjective preference rather than significant differences in performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Model Scores:</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model  Score\n",
       "0  gpt-4o-mini     10\n",
       "1       gpt-4o     10\n",
       "2  gpt-4-turbo     10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Single model response\n",
    "response = run_agent(\"What is the capital of France?\")\n",
    "print_result(\"Single Model Response\", response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-MODEL COMPARISON:\")\n",
    "\n",
    "# Multiple models (will use only available ones)\n",
    "multi_results = run_agent_with_multiple_models(\"What is the capital of France?\")\n",
    "\n",
    "for model_name, result in multi_results.items():\n",
    "    print_result(f\"{result['model_display']} ({result['provider']})\", result['response'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPREHENSIVE ANALYSIS WITH EVALUATION:\")\n",
    "\n",
    "# Full comparative analysis with evaluation\n",
    "analysis = run_comparative_analysis(\"What is the capital of France?\")\n",
    "\n",
    "print_result(\"Best Model\", analysis['comparison'].best_model, \"green\")\n",
    "print_result(\"Model Ranking\", analysis['comparison'].ranking)\n",
    "print_result(\"Reasoning\", analysis['comparison'].reasoning)\n",
    "\n",
    "# Show individual scores\n",
    "scores_df = pd.DataFrame([\n",
    "    {\"Model\": model, \"Score\": analysis['comparison'].scores.get(model, 0)}\n",
    "    for model in analysis['comparison'].ranking\n",
    "])\n",
    "display(HTML(\"<h4>Model Scores:</h4>\"))\n",
    "display(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc8d13",
   "metadata": {},
   "source": [
    "## Lab 3: Tool Integration + Evaluator-Optimizer Loops\n",
    "\n",
    "**Workflow Patterns:** **Tool Integration** + **Evaluator-Optimizer**\n",
    "\n",
    "**Learning Objective:**\n",
    "Demonstrate how LLMs can execute external functions while maintaining quality control through evaluation loops.\n",
    "\n",
    "**Architecture Flow:**\n",
    "```\n",
    "[User Input] ‚Üí [LLM Decision] ‚Üí [Tool Execution] ‚Üí [Tool Result] ‚Üí [LLM Response]\n",
    "                   ‚Üì                                              ‚Üì\n",
    "            [Select Tool Type]                            [Evaluator Assessment]\n",
    "                   ‚Üì                                              ‚Üì\n",
    "           [Function Arguments]                          [Accept ‚úÖ | Retry ‚ùå]\n",
    "```\n",
    "\n",
    "**Tool Integration Pattern Explained:**\n",
    "This pattern enables LLMs to interact with the external world:\n",
    "1. **Intent Recognition**: LLM analyzes user input for tool requirements\n",
    "2. **Tool Selection**: LLM chooses appropriate function to call\n",
    "3. **Argument Extraction**: LLM structures function arguments\n",
    "4. **Execution**: External function runs with LLM-provided parameters\n",
    "5. **Context Integration**: Tool results are incorporated into final response\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Extends LLM Capabilities**: Beyond text generation to action execution\n",
    "- **Real-World Integration**: Connect AI to APIs, databases, systems\n",
    "- **Dynamic Interaction**: Responses based on live data, not training data\n",
    "- **Structured Processing**: Validate inputs and outputs systematically\n",
    "\n",
    "**Evaluator-Optimizer Loop Enhanced:**\n",
    "For tool usage, evaluation becomes more complex:\n",
    "1. **Functional Accuracy**: Did the tool execute correctly?\n",
    "2. **Result Relevance**: Is the tool output appropriate for the question?\n",
    "3. **Integration Quality**: How well are tool results incorporated?\n",
    "4. **User Satisfaction**: Does the final response meet user needs?\n",
    "\n",
    "**Code Implementation Details:**\n",
    "- **Tool Functions**: `get_current_time()`, `get_weather(city)`\n",
    "- **Tool Schema**: JSON definitions for LLM understanding\n",
    "- **Execution Logic**: `execute_tool()` function dispatcher\n",
    "- **Evaluation**: Enhanced criteria for tool-assisted responses\n",
    "- **Retry Mechanism**: Automatic regeneration for failed tool usage\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Personal assistants with calendar/email access\n",
    "- Customer service bots with database queries\n",
    "- Research assistants with web search capabilities\n",
    "- IoT control systems with device integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaa4b3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM on June 23, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TOOL USAGE WITH EVALUATION:\n",
      "Attempt 1 failed evaluation. Retrying...\n",
      "Feedback: The AI response provides a specific time but is incorrect regarding the actual current time. This undermines the primary purpose of answering the user's question accurately. The response lacks real-time awareness, which is a critical requirement for a general-purpose assistant when asked about the current time.\n",
      "Attempt 2 failed evaluation. Retrying...\n",
      "Feedback: The AI response fails to provide an accurate current time, which is a fundamental requirement for such a question. Instead, it gives a time that is future-dated, making the response incorrect and unhelpful. While the format of the time and date is clear, the inaccuracy undermines its overall utility.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Response with Evaluation</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM on June 23, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Evaluation Details</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"Score\": \"3/10\",\n",
       "  \"Acceptable\": false,\n",
       "  \"Strengths\": [\n",
       "    \"The response is formatted clearly with both time and date.\",\n",
       "    \"It maintains a neutral and informative tone.\"\n",
       "  ],\n",
       "  \"Suggestions\": [\n",
       "    \"The AI should indicate that it cannot provide real-time information and suggest the user check their device for the current time.\",\n",
       "    \"Including a disclaimer about the limitations of the AI in providing live data would enhance the user experience.\"\n",
       "  ]\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MULTI-MODEL TOOL COMPARISON:\n",
      "Generating response with gpt-4o-mini...\n",
      "Generating response with gpt-4o...\n",
      "Generating response with gpt-4-turbo...\n",
      "Comparing all responses...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:green;\">Best Tool User</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "gpt-4o-mini"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Usage - gpt-4o-mini</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM on June 23, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Usage - gpt-4o</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM on June 23, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Tool Usage - gpt-4-turbo</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current time is 09:09 AM."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic tool usage\n",
    "response = run_agent(\"What time is it now?\")\n",
    "print_result(\"Tool Response\", response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TOOL USAGE WITH EVALUATION:\")\n",
    "\n",
    "# Tool usage with evaluation\n",
    "result_with_eval = run_agent_with_evaluation(\"What time is it now?\")\n",
    "print_result(\"Tool Response with Evaluation\", result_with_eval['response'])\n",
    "\n",
    "evaluation = result_with_eval['evaluation']\n",
    "print_result(\"Tool Evaluation Details\", {\n",
    "    \"Score\": f\"{evaluation.score}/10\",\n",
    "    \"Acceptable\": evaluation.is_acceptable,\n",
    "    \"Strengths\": evaluation.strengths,\n",
    "    \"Suggestions\": evaluation.suggestions\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-MODEL TOOL COMPARISON:\")\n",
    "\n",
    "# Compare tool usage across models\n",
    "tool_analysis = run_comparative_analysis(\"What time is it now?\")\n",
    "print_result(\"Best Tool User\", tool_analysis['comparison'].best_model, \"green\")\n",
    "\n",
    "for model_name, response in tool_analysis['responses'].items():\n",
    "    print_result(f\"Tool Usage - {model_name}\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d48d8f",
   "metadata": {},
   "source": [
    "## Lab 4: Orchestrator-Worker Pattern + Advanced Tool Integration\n",
    "\n",
    "**Workflow Patterns:** **Orchestrator-Worker** + **Structured Tool Calling**\n",
    "\n",
    "**Learning Objective:**\n",
    "Implement sophisticated coordination patterns where an LLM orchestrator manages complex multi-step tasks with specialized worker components.\n",
    "\n",
    "**Architecture Flow:**\n",
    "```\n",
    "[Complex Query] ‚Üí [Orchestrator LLM] ‚Üí [Task Decomposition] ‚Üí [Worker Tools] ‚Üí [Result Integration]\n",
    "                         ‚Üì                    ‚Üì                    ‚Üì                    ‚Üì\n",
    "                 [Plan Generation]      [Parallel Execution]  [Status Monitoring]  [Quality Assessment]\n",
    "                         ‚Üì                    ‚Üì                    ‚Üì                    ‚Üì\n",
    "                 [Resource Allocation]  [Error Handling]      [Result Collection] [Final Response]\n",
    "```\n",
    "\n",
    "**Orchestrator-Worker Pattern Explained:**\n",
    "This is the most sophisticated workflow pattern we implement:\n",
    "1. **Orchestrator Role**: Main LLM analyzes complex requests and creates execution plans\n",
    "2. **Task Decomposition**: Breaks down complex queries into manageable subtasks\n",
    "3. **Worker Coordination**: Dispatches subtasks to specialized tools or models\n",
    "4. **Progress Monitoring**: Tracks execution status and handles errors\n",
    "5. **Result Integration**: Combines outputs from multiple workers into coherent response\n",
    "\n",
    "**Advanced Tool Integration:**\n",
    "- **Structured Arguments**: Tools accept complex, validated JSON parameters\n",
    "- **Error Handling**: Robust failure detection and recovery mechanisms\n",
    "- **External Systems**: Integration with real-world services (notifications, databases)\n",
    "- **Production Features**: Deployment-ready with monitoring and logging\n",
    "\n",
    "**Pattern Characteristics:**\n",
    "- **Highest Autonomy**: Orchestrator LLM makes complex coordination decisions\n",
    "- **Dynamic Flow**: Execution path adapts based on intermediate results\n",
    "- **Scalability**: Can coordinate any number of worker components\n",
    "- **Robustness**: Built-in error handling and fallback mechanisms\n",
    "\n",
    "**Comparative Analysis as Orchestrator-Worker:**\n",
    "Our `run_comparative_analysis()` function demonstrates this pattern:\n",
    "1. **Orchestrator**: Main evaluation LLM coordinates the entire process\n",
    "2. **Workers**: Multiple generator models produce responses\n",
    "3. **Coordination**: Orchestrator manages evaluation of each worker's output\n",
    "4. **Integration**: Final ranking combines all worker results intelligently\n",
    "\n",
    "**Code Implementation Details:**\n",
    "- **Advanced Tools**: `get_weather(city)`, `record_user_details(email, name, notes)`\n",
    "- **Orchestration Logic**: `run_comparative_analysis()` as orchestrator function\n",
    "- **Worker Management**: Multiple model coordination with error handling\n",
    "- **Quality Control**: Enhanced evaluation criteria for complex outputs\n",
    "- **Production Features**: Web interface, monitoring, deployment automation\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Project management systems with AI coordination\n",
    "- Complex research tasks requiring multiple specialists\n",
    "- Multi-step customer service workflows\n",
    "- Enterprise automation with human-AI collaboration\n",
    "- Scientific analysis pipelines with multiple data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174de25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Structured Tool Response</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The weather in Tokyo is currently 25¬∞C and raining."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "WEATHER TOOL WITH ADVANCED EVALUATION:\n",
      "\n",
      "Testing weather for Tokyo:\n",
      "Attempt 1 failed evaluation. Retrying...\n",
      "Feedback: The response provides a specific temperature and weather condition, but it lacks real-time accuracy as the information is not verifiable and may not reflect the current weather. Additionally, it does not mention the date or time of the report, which is crucial for weather information. The simplicity of the statement is clear, but it could benefit from more context or detail.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Weather in Tokyo</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current weather in Tokyo is 25¬∞C and it is raining."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing weather for Barcelona:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Weather in Barcelona</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The weather in Barcelona is currently 22¬∞C and sunny."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing weather for New York:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Weather in New York</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current weather in New York is 17¬∞C and cloudy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing weather for London:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Weather in London</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The current weather in London is 15¬∞C and foggy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "COMPREHENSIVE WEATHER ANALYSIS:\n",
      "Generating response with gpt-4o-mini...\n",
      "Generating response with gpt-4o...\n",
      "Generating response with gpt-4-turbo...\n",
      "Comparing all responses...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:purple;\">Question</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Compare the weather between Tokyo and Barcelona, and recommend which city would be better for outdoor activities today."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:green;\">Best Model for Weather Analysis</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "gpt-4o-mini"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">Model Ranking</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "['gpt-4o-mini', 'gpt-4o', 'gpt-4-turbo']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Model Responses:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">gpt-4o-mini (Score: 8/10)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Today, the weather in Tokyo is 25¬∞C with rain, while in Barcelona it is 22¬∞C and sunny. \n",
       "\n",
       "Given these conditions, Barcelona would be the better choice for outdoor activities today. The sunny weather and mild temperature in Barcelona are more conducive to enjoying outdoor pursuits compared to the rainy conditions in Tokyo."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">gpt-4o (Score: 7/10)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Today, Tokyo has a temperature of 25¬∞C with rain, while Barcelona is experiencing sunny weather with a temperature of 22¬∞C. For outdoor activities today, Barcelona would be the better choice given the pleasant weather conditions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:blue;\">gpt-4-turbo (Score: 8/10)</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Today, Tokyo is experiencing rain with a temperature of 25¬∞C, while Barcelona has sunny weather with a temperature of 22¬∞C.\n",
       "\n",
       "For outdoor activities, Barcelona would be the better choice today due to its sunny weather, making it more suitable for spending time outside comfortably. Tokyo's rainy conditions might hinder outdoor activities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Winner's Reasoning:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3 style=\"color:gold;\">Why this model won</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Comparison failed: Expecting value: line 1 column 1 (char 0)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic structured tool calling\n",
    "response = run_agent(\"What's the weather in Tokyo?\")\n",
    "print_result(\"Structured Tool Response\", response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WEATHER TOOL WITH ADVANCED EVALUATION:\")\n",
    "\n",
    "# Multiple cities with evaluation\n",
    "cities = [\"Tokyo\", \"Barcelona\", \"New York\", \"London\"]\n",
    "\n",
    "for city in cities:\n",
    "    print(f\"\\nTesting weather for {city}:\")\n",
    "    result = run_agent_with_evaluation(f\"What's the weather in {city}?\", max_retries=1)\n",
    "    \n",
    "    evaluation = result['evaluation']\n",
    "    print_result(f\"Weather in {city}\", result['response'])\n",
    "    \n",
    "    if evaluation.score < 7:\n",
    "        print(f\"‚ö†Ô∏è Low quality response (Score: {evaluation.score}/10)\")\n",
    "        print(f\"Feedback: {evaluation.feedback}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPREHENSIVE WEATHER ANALYSIS:\")\n",
    "\n",
    "# Full analysis for a complex weather question\n",
    "complex_question = \"Compare the weather between Tokyo and Barcelona, and recommend which city would be better for outdoor activities today.\"\n",
    "\n",
    "final_analysis = run_comparative_analysis(complex_question)\n",
    "\n",
    "print_result(\"Question\", complex_question, \"purple\")\n",
    "print_result(\"Best Model for Weather Analysis\", final_analysis['comparison'].best_model, \"green\")\n",
    "print_result(\"Model Ranking\", final_analysis['comparison'].ranking)\n",
    "\n",
    "# Show all responses\n",
    "print(\"\\nAll Model Responses:\")\n",
    "for model_name, response in final_analysis['responses'].items():\n",
    "    score = final_analysis['evaluations'][model_name].score\n",
    "    print_result(f\"{model_name} (Score: {score}/10)\", response)\n",
    "\n",
    "print(\"\\nWinner's Reasoning:\")\n",
    "print_result(\"Why this model won\", final_analysis['comparison'].reasoning, \"gold\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fde23945",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Complete Workflow Patterns Analysis\n",
    "\n",
    "### Summary: All 5 Fundamental Patterns Implemented\n",
    "\n",
    "**1. ‚úÖ Prompt Chaining (Lab 1)**\n",
    "- **Implementation**: Basic `run_agent()` function with sequential processing\n",
    "- **Autonomy Level**: Low-Medium (predefined sequence)\n",
    "- **Key Learning**: Foundation of all other patterns\n",
    "\n",
    "**2. ‚úÖ Routing (Throughout)**\n",
    "- **Implementation**: Model selection logic in `model_manager`\n",
    "- **Autonomy Level**: Medium (router logic makes decisions)\n",
    "- **Key Learning**: Task-specific model assignment\n",
    "\n",
    "**3. ‚úÖ Parallelization (Lab 2)**\n",
    "- **Implementation**: `run_agent_with_multiple_models()` concurrent execution\n",
    "- **Autonomy Level**: Low (code-controlled distribution)\n",
    "- **Key Learning**: Speed and redundancy through concurrent processing\n",
    "\n",
    "**4. ‚úÖ Orchestrator-Worker (Lab 4)**\n",
    "- **Implementation**: `run_comparative_analysis()` coordination system\n",
    "- **Autonomy Level**: Medium-High (orchestrator makes coordination decisions)\n",
    "- **Key Learning**: Complex task decomposition and result integration\n",
    "\n",
    "**5. ‚úÖ Evaluator-Optimizer (Labs 2-4)**\n",
    "- **Implementation**: `run_agent_with_evaluation()` validation loops\n",
    "- **Autonomy Level**: Medium (evaluator makes quality decisions)\n",
    "- **Key Learning**: Quality control through feedback loops\n",
    "\n",
    "---\n",
    "\n",
    "## Pattern Progression and Complexity\n",
    "\n",
    "**Complexity Ladder:**\n",
    "1. **Prompt Chaining** ‚Üí Simple, predictable, foundational\n",
    "2. **Routing** ‚Üí Decision-making, specialization\n",
    "3. **Parallelization** ‚Üí Concurrency, efficiency  \n",
    "4. **Evaluator-Optimizer** ‚Üí Quality control, feedback\n",
    "5. **Orchestrator-Worker** ‚Üí Coordination, complex task management\n",
    "\n",
    "**Autonomy Progression:**\n",
    "- **Low**: Human-defined sequences (Prompt Chaining, Parallelization)\n",
    "- **Medium**: LLM decision-making within constraints (Routing, Evaluator-Optimizer)\n",
    "- **High**: Dynamic coordination and planning (Orchestrator-Worker)\n",
    "\n",
    "---\n",
    "\n",
    "## Advanced Technical Architecture\n",
    "\n",
    "**Core System Components:**\n",
    "1. **Model Manager**: Multi-provider orchestration (implements Routing)\n",
    "2. **Evaluation Engine**: Quality control system (implements Evaluator-Optimizer)\n",
    "3. **Tool Handler**: External function coordination (enables complex workflows)\n",
    "4. **Analysis Engine**: Comparative coordination (implements Orchestrator-Worker)\n",
    "\n",
    "**Production-Ready Guardrails:**\n",
    "- **Structured Validation**: Pydantic model enforcement across all patterns\n",
    "- **Retry Mechanisms**: Evaluator-Optimizer loops with feedback\n",
    "- **Quality Assessment**: Automatic evaluation in Labs 2-4\n",
    "- **Error Handling**: Graceful degradation in all workflow patterns\n",
    "- **Resource Management**: Timeout and rate limiting controls\n",
    "\n",
    "---\n",
    "\n",
    "## From Workflows to True Agents\n",
    "\n",
    "**What We've Built (Workflows):**\n",
    "- Predictable execution paths\n",
    "- Clear control mechanisms\n",
    "- Defined start and end points\n",
    "- Human-designed coordination\n",
    "\n",
    "**Next Steps (True Agents):**\n",
    "- Open-ended iterative loops\n",
    "- Dynamic self-modification\n",
    "- Uncertain execution duration\n",
    "- Autonomous goal pursuit\n",
    "\n",
    "**Key Insight:** Workflows are the building blocks that enable true agent behavior. Mastering these 5 patterns provides the foundation for building more autonomous systems in subsequent weeks.\n",
    "\n",
    "---\n",
    "\n",
    "## Commercial Applications by Pattern\n",
    "\n",
    "**Prompt Chaining Applications:**\n",
    "- Content generation pipelines\n",
    "- Document processing workflows\n",
    "- Template-based systems\n",
    "\n",
    "**Parallelization Applications:**\n",
    "- A/B testing systems\n",
    "- Consensus-building platforms\n",
    "- Risk mitigation through redundancy\n",
    "\n",
    "**Evaluator-Optimizer Applications:**\n",
    "- Quality assurance systems\n",
    "- Content moderation platforms\n",
    "- Performance optimization tools\n",
    "\n",
    "**Orchestrator-Worker Applications:**\n",
    "- Project management systems\n",
    "- Complex research workflows\n",
    "- Multi-specialist coordination\n",
    "\n",
    "**Integration Potential:**\n",
    "All patterns can be combined for enterprise-grade agentic systems with predictable behavior and robust quality control.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d12e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM CONFIGURATION VALIDATION\n",
      "==================================================\n",
      "Available Models: 3\n",
      "   ‚úÖ GPT-4O Mini (openai)\n",
      "   ‚úÖ GPT-4O (openai)\n",
      "   ‚úÖ GPT-4 Turbo (openai)\n",
      "\n",
      "API Keys Status:\n",
      "   ‚úÖ OpenAI: Configured (sk-proj-...)\n",
      "   ‚ö†Ô∏è Anthropic: Not configured (optional)\n",
      "   ‚ö†Ô∏è Google: Not configured (optional)\n",
      "   ‚ö†Ô∏è DeepSeek: Not configured (optional)\n",
      "\n",
      "System Status: ‚úÖ READY FOR PRODUCTION\n",
      "\n",
      "Quick Functionality Test:\n",
      "‚úÖ Basic Agent: Working\n",
      "‚úÖ Evaluation System: Working (Score: 4/10)\n",
      "All systems operational!\n"
     ]
    }
   ],
   "source": [
    "# System Validation & Configuration Test\n",
    "print(\"SYSTEM CONFIGURATION VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check model availability\n",
    "available_models = model_manager.get_available_models()\n",
    "print(f\"Available Models: {len(available_models)}\")\n",
    "for model in available_models:\n",
    "    info = model_manager.get_model_info(model)\n",
    "    print(f\"   ‚úÖ {info.name} ({info.provider})\")\n",
    "\n",
    "print(\"\\nAPI Keys Status:\")\n",
    "import os\n",
    "apis = [\n",
    "    (\"OpenAI\", \"OPENAI_API_KEY\"),\n",
    "    (\"Anthropic\", \"ANTHROPIC_API_KEY\"), \n",
    "    (\"Google\", \"GOOGLE_API_KEY\"),\n",
    "    (\"DeepSeek\", \"DEEPSEEK_API_KEY\")\n",
    "]\n",
    "\n",
    "for name, env_var in apis:\n",
    "    key = os.getenv(env_var)\n",
    "    if key:\n",
    "        print(f\"   ‚úÖ {name}: Configured ({key[:8]}...)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è {name}: Not configured (optional)\")\n",
    "\n",
    "print(f\"\\nSystem Status: {'‚úÖ READY FOR PRODUCTION' if available_models else '‚ö†Ô∏è NEEDS CONFIGURATION'}\")\n",
    "\n",
    "# Quick functionality test\n",
    "print(\"\\nQuick Functionality Test:\")\n",
    "try:\n",
    "    test_response = run_agent(\"Hello, test the system!\", \"gpt-4o-mini\")\n",
    "    print(f\"‚úÖ Basic Agent: Working\")\n",
    "    \n",
    "    test_eval = evaluator.evaluate_response(\"Test\", test_response)\n",
    "    print(f\"‚úÖ Evaluation System: Working (Score: {test_eval.score}/10)\")\n",
    "    \n",
    "    print(\"All systems operational!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Please check your configuration and API keys.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94609926",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Practical Decision Guide: When to Use Each Pattern\n",
    "\n",
    "### Pattern Selection Framework\n",
    "\n",
    "**Choose Prompt Chaining when:**\n",
    "- ‚úÖ Simple, linear workflow\n",
    "- ‚úÖ Predictable sequence of operations\n",
    "- ‚úÖ Need high control and reliability\n",
    "- ‚úÖ Cost-effectiveness is priority\n",
    "- ‚ùå Complex decision-making required\n",
    "\n",
    "**Choose Routing when:**\n",
    "- ‚úÖ Different specialized models for different tasks\n",
    "- ‚úÖ Task classification needed\n",
    "- ‚úÖ Want to optimize model selection\n",
    "- ‚úÖ Have domain-specific requirements\n",
    "- ‚ùå All tasks similar in nature\n",
    "\n",
    "**Choose Parallelization when:**\n",
    "- ‚úÖ Speed is critical\n",
    "- ‚úÖ Need redundancy for reliability\n",
    "- ‚úÖ Want consensus or comparison\n",
    "- ‚úÖ Tasks are independent\n",
    "- ‚ùå Sequential dependencies exist\n",
    "\n",
    "**Choose Evaluator-Optimizer when:**\n",
    "- ‚úÖ Quality control is critical\n",
    "- ‚úÖ Iterative improvement needed\n",
    "- ‚úÖ Production reliability required\n",
    "- ‚úÖ Cost of failure is high\n",
    "- ‚ùå Speed is more important than quality\n",
    "\n",
    "**Choose Orchestrator-Worker when:**\n",
    "- ‚úÖ Complex, multi-step workflows\n",
    "- ‚úÖ Dynamic task decomposition needed\n",
    "- ‚úÖ Resource coordination required\n",
    "- ‚úÖ Flexible execution paths desired\n",
    "- ‚ùå Simple, straightforward tasks\n",
    "\n",
    "---\n",
    "\n",
    "### Combining Patterns\n",
    "\n",
    "**Successful Pattern Combinations:**\n",
    "- **Parallelization + Evaluator-Optimizer**: Multi-model comparison with quality control\n",
    "- **Routing + Orchestrator-Worker**: Smart task assignment with complex coordination\n",
    "- **Prompt Chaining + Evaluator-Optimizer**: Sequential processing with validation\n",
    "- **All Patterns Together**: Enterprise-grade agentic systems\n",
    "\n",
    "**Pattern Interaction Benefits:**\n",
    "- **Robustness**: Multiple quality control mechanisms\n",
    "- **Efficiency**: Optimized resource utilization\n",
    "- **Flexibility**: Adaptable to different scenarios\n",
    "- **Scalability**: Can handle increasing complexity\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34ab170a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Web Interface with Gradio\n",
    "\n",
    "Now let's launch the web interface that brings everything together!\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2bfc6dd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Challenges and Limitations of Agentic Systems\n",
    "\n",
    "### Architectural Challenges\n",
    "\n",
    "**1. Unpredictability:**\n",
    "- **Variable execution paths**: Unknown execution order\n",
    "- **Uncertain outputs**: No quality guarantees\n",
    "- **Indeterminate timing**: Unpredictable duration\n",
    "\n",
    "**2. Variable Costs:**\n",
    "- **Unpredictable token consumption**\n",
    "- **Multiple API calls** without clear limits\n",
    "- **Exponential cost scaling**\n",
    "\n",
    "**3. Guardrail Requirements:**\n",
    "- **Continuous monitoring** of system behavior\n",
    "- **Resource and time limits**\n",
    "- **Real-time output validation**\n",
    "\n",
    "---\n",
    "\n",
    "## Implemented Mitigations\n",
    "\n",
    "**Monitoring and Observability:**\n",
    "- **Traceability**: Complete interaction tracking\n",
    "- **Metrics**: Time, cost, and response quality measurement\n",
    "- **Alerting**: Anomalous behavior notifications\n",
    "\n",
    "**Containment Systems:**\n",
    "- **Timeouts**: Maximum execution limits\n",
    "- **Rate limiting**: API call frequency control\n",
    "- **Validation gates**: Quality control checkpoints\n",
    "- **Fallback mechanisms**: Alternative paths for failures\n",
    "\n",
    "---\n",
    "\n",
    "## Applied Best Practices\n",
    "\n",
    "**Robust Design Principles:**\n",
    "1. **Start Simple**: Begin with basic workflows\n",
    "2. **Add Complexity Gradually**: Incremental functionality addition\n",
    "3. **Test Extensively**: Comprehensive pre-production testing\n",
    "4. **Monitor Everything**: Complete system observability\n",
    "\n",
    "**Intelligent Optimization:**\n",
    "- **Model Selection**: Optimal LLM choice for each task\n",
    "- **Caching**: Response reuse when appropriate\n",
    "- **Parallel Processing**: Temporal efficiency maximization\n",
    "- **Error Recovery**: Robust recovery mechanisms\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12d715d5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## LLM Model Landscape\n",
    "\n",
    "### Frontier Models Implemented\n",
    "\n",
    "**OpenAI:**\n",
    "- **GPT-4o-mini**: Primary model (fast, cost-effective)\n",
    "- **GPT-4o**: Maximum performance (higher cost)\n",
    "- **o1-mini**: Reasoning models (chain-of-thought optimized)\n",
    "\n",
    "**Anthropic:**\n",
    "- **Claude 3.5 Sonnet**: Excellent price/performance balance\n",
    "- **Claude 3 Haiku**: Cost-effective alternative\n",
    "\n",
    "**Google:**\n",
    "- **Gemini 2.0 Flash**: Free within usage limits\n",
    "- **Gemini Pro**: Enhanced performance version\n",
    "\n",
    "**DeepSeek (China):**\n",
    "- **DeepSeek V3 R1**: GPT-4 performance at 1/30th cost\n",
    "- **Distilled Models**: Optimized versions for local deployment\n",
    "\n",
    "**Groq:**\n",
    "- **Fast Inference**: LLaMA 3.3 (70B) at high speed\n",
    "- **Low Cost**: Ideal for rapid prototyping\n",
    "\n",
    "**Ollama:**\n",
    "- **Local Models**: Zero API costs\n",
    "- **High Performance**: C++ optimized backend\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources and Next Steps\n",
    "\n",
    "**Recommended Resources:**\n",
    "- **ValuLM Leaderboard**: Model comparison and pricing\n",
    "- **Hugging Face**: Open source model repository\n",
    "- **LangSmith**: Traceability and debugging tools\n",
    "- **Guides folder**: Additional technical documentation\n",
    "\n",
    "**Learning Roadmap:**\n",
    "1. **Week 2**: OpenAI Agents SDK\n",
    "2. **Week 3**: CrewAI - Agent frameworks\n",
    "3. **Week 4**: LangGraph - Execution graphs\n",
    "4. **Week 5**: AutoGen - Multi-agent conversations\n",
    "5. **Week 6**: MCP - Model Context Protocol\n",
    "\n",
    "**Mastery Objectives:**\n",
    "- **Architecture**: Design robust agentic systems\n",
    "- **Implementation**: Clean, maintainable code\n",
    "- **Deployment**: Production-ready systems\n",
    "- **Optimization**: Balanced cost and performance\n",
    "\n",
    "---\n",
    "\n",
    "## Course Completion\n",
    "\n",
    "You have successfully completed the **AI Agents Foundations Laboratory**. You now have deep understanding of:\n",
    "- ‚úÖ Workflow and agent architectures\n",
    "- ‚úÖ Agentic design patterns\n",
    "- ‚úÖ Multi-LLM practical implementation\n",
    "- ‚úÖ Advanced tools and deployment\n",
    "- ‚úÖ Best practices and guardrails\n",
    "\n",
    "**You are ready for the advanced challenges of the upcoming weeks.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "551baf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Advanced AI Agent Web Interface...\n",
      "Features available:\n",
      "   - Simple Chat\n",
      "   - Chat with Evaluation\n",
      "   - Multi-Model Comparison\n",
      "   - System Status\n",
      "\n",
      "Click the link below to access the interface!\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch the Advanced Web Interface\n",
    "from week1_foundations.interface import launch_interface\n",
    "\n",
    "# Launch in notebook (inline)\n",
    "print(\"Starting Advanced AI Agent Web Interface...\")\n",
    "print(\"Features available:\")\n",
    "print(\"   - Simple Chat\")\n",
    "print(\"   - Chat with Evaluation\") \n",
    "print(\"   - Multi-Model Comparison\")\n",
    "print(\"   - System Status\")\n",
    "print(\"\\nClick the link below to access the interface!\")\n",
    "\n",
    "# Launch with share=False for local use, share=True for public link\n",
    "launch_interface(share=False, port=7860)\n",
    "\n",
    "# Note: The interface will open in a new tab\n",
    "# You can also access it directly at http://localhost:7860\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66db3dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing corrected imports and functionality...\n",
      "‚úÖ Basic agent test successful\n",
      "Response preview: Hello! It looks like you're testing the system. How can I assist you today?...\n",
      "‚úÖ Evaluation system test successful\n",
      "Score: 10/10\n",
      "\n",
      "All tests passed! The system is working correctly.\n"
     ]
    }
   ],
   "source": [
    "# Test imports and basic functionality\n",
    "print(\"Testing corrected imports and functionality...\")\n",
    "\n",
    "try:\n",
    "    # Test basic agent functionality\n",
    "    test_response = run_agent(\"Hello, this is a test\")\n",
    "    print(f\"‚úÖ Basic agent test successful\")\n",
    "    print(f\"Response preview: {test_response[:100]}...\")\n",
    "    \n",
    "    # Test evaluation system\n",
    "    test_eval_result = run_agent_with_evaluation(\"What is 2+2?\", max_retries=1)\n",
    "    print(f\"‚úÖ Evaluation system test successful\")\n",
    "    print(f\"Score: {test_eval_result['evaluation'].score}/10\")\n",
    "    \n",
    "    print(\"\\nAll tests passed! The system is working correctly.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during testing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
