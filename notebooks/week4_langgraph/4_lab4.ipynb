{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4 Day 4 - preparing the big project!\n",
    "\n",
    "# The Sidekick\n",
    "\n",
    "It's time to introduce:\n",
    "\n",
    "1. Structured Outputs\n",
    "2. A multi-agent flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict, List, Dict, Any, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain_community.tools.playwright.utils import create_async_playwright_browser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display\n",
    "import gradio as gr\n",
    "import uuid\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_dotenv(override=True)\n",
    "load_dotenv(\"/Users/alex/Desktop/00_projects/AI_agents/my_agents/.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For structured outputs, we define a Pydantic object for the Schema\n",
    "\n",
    "And you'll remember the first step of Structured Outputs is to define the schema. What are we using to describe the results that must come back from an LLM? And in particular, the thing that we're going to be working on is an evaluator, something which is going to decide whether or not an answer from an LLM is good. And so our evaluator is going to respond using this object, an evaluator output. Or really, it's going to respond with JSON, and the JSON is going to have to conform to this. So we just describe what this means. There's going to be a field, Feedback, which is going to be feedback on the worker's response success criteria. Actually, let's change this to the assistance response, because that's going to be more words that it will understand. Worker is what we will call it in this assistance response. There we go. Now you get see-me type. So feedback on the assistance response success criteria is whether the success criteria has been met. And user input needed. True if more input is needed from the user, or clarifications, or if the assistant is stuck. So this is going to allow, we're going to have an evaluator. It's going to evaluate the results of our worker, the assistant, to use that terminology. And it's going to decide whether it's okay to forward that back to the user, or whether it needs to go back to the assistant for more work. And one situation is if the success criteria are met and it's done its job, but another situation is if the worker seems to be stuck or needs clarifications, in which case it should return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define a structured output\n",
    "\n",
    "class EvaluatorOutput(BaseModel):\n",
    "    feedback: str = Field(description=\"Feedback on the assistant's response\")\n",
    "    success_criteria_met: bool = Field(description=\"Whether the success criteria have been met\")\n",
    "    user_input_needed: bool = Field(description=\"True if more input is needed from the user, or clarifications, or the assistant is stuck\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And for the State, we'll use TypedDict again\n",
    "\n",
    "But now we have some real information to maintain!\n",
    "\n",
    "The messages uses the reducer. The others are simply values that we overwrite with any state change.\n",
    "\n",
    "---\n",
    "And now, to manage state. You remember with state, it can really be any Python object. It can be a Pydantic object, but we often use type Dix, and that's what we're doing here. And now, for the first time, we have some real meaty information to store in the state. We've always had messages before, but now we have a real state. And we've got a bunch of things. We do still have messages, which is representing the discussion between the user and the assistant. But we've got other stuff, which is going to represent the information being passed from the evaluator back to the thing I'm going to call the worker, the assistant. And so we're going to have some more stuff. We're going to have success criteria, which is going to be set up front to define what does it mean to be successful. Feedback on the work, that's going to come from the worker. And by the way, you use optional like this. If this can be null, it can be none, or it can be a string. Success criteria met is a bull is true or false, and it's going to be true if there has been a successful outcome, that the criteria are met and it doesn't need to go back to the worker for more. And user input needed is if we need to go back to the user to get some more information. So that is our state. So it's a much meatier state, and this shows you that you can have whatever you want in the state. The state is really up to you and the flow of your logic. And that state is like something that is moved through the graph, and everyone gets their opportunity. Every node gets the state and gets its opportunity to return a new state that is some change to that state. And we're only specifying one reducer, add messages, which means that if one of these nodes returns with some messages, they will get accumulated, they will get concatenated with the existing messages. But if one of these nodes returns user input needed, that will overwrite whatever was in the old state. So when you return the new state, if you change one of these values here, that becomes the new setting for anything that is downstream of you in the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The state\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[Any], add_messages]\n",
    "    success_criteria: str\n",
    "    feedback_on_work: Optional[str]\n",
    "    success_criteria_met: bool\n",
    "    user_input_needed: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so next up we set up our playwright tools. This is the same code as before, using the async browser, the playwright browser, and the playwright browser toolkit. So we just run that, that's pretty simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our async Playwright tools\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "async_browser =  create_async_playwright_browser(headless=False)  # headful mode\n",
    "toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
    "tools = toolkit.get_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we're going to have two LLMs that we're going to initialize. One of them is called the worker LLM, it plays the role of the assistant. It's going to be a GPT-40 mini, and it's going to be bound to those tools, so that it will automatically have the JSON gumph in it. And then separately we're going to have an evaluator LLM. This is a separate LLM, and it is, we're setting it up, whereas before we said bind tools, and now we say with structured output, and pass in the pedantic object, and that means that the response will conform to this output. Now not all models are set up to support structured output, so you may find that some models can't do it, if you're going to be playing around with different models. If that is the case, of course the alternative to this is to do it the old-fashioned way, which means instead of using structured outputs like this and passing a pedantic object, you ask the model in the prompt to respond in JSON. You give it the schema, you list out what kind of JSON it should respond in, you maybe give a couple of examples to make sure that it's really biased to do well, and then you have to parse that JSON in the response. And that's all that is happening behind the scenes when we do structured outputs like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLMs\n",
    "\n",
    "worker_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "worker_llm_with_tools = worker_llm.bind_tools(tools)\n",
    "\n",
    "evaluator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "evaluator_llm_with_output = evaluator_llm.with_structured_output(EvaluatorOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so this is a bit of a long-looking method function, this node, the depth worker, which is representing our worker node, our assistant, but it's only long because we've got a lot of prompting in here. So, it's a node, and so, as usual, it takes a state, and it's going to return a state. And you can see it's returning something with messages, and we know that messages accumulate because we have the reducer, so that is going to add on more messages. \n",
    "\n",
    "Okay, so we've got a long system message, let me talk you through it. We say, look, you're a helpful assistant that can use tools to complete tasks. You keep working on a task until either you have a question or clarification for the user, or the success criteria has been met. And this is the success criteria, and we take it from the state, something that should be held in the `state`. You should reply, either with a question for the user about the assignment, or with your final response. If you have a question for the user, you need to reply by clearly stating your question. An example might be this. That's not super necessary for me to have done that, but I wanted to make it really clear that it was a say or an answer question, and sort of force the point. If you've finished, reply with the final answer, and don't ask a question, simply reply with the answer. And I say that because these models love to reply with things like, can I help you with anything else? And that then might confuse our evaluator that's looking to see if it needs help, so I want it to be super clear on this front. Okay.\n",
    "\n",
    "Okay, and of course all these things are subject for experimentation. There's no hard and fast rules. It's not like that is a rule that you have to put this in the prompt. I hope you know this by now. This is really the part of AI engineering that is about experimentation, R&D. You can imagine I've been crafting this prompt for the last couple of hours. So it's something that obviously you hone in on something that works well, and you may find that particularly with different models or if you have different assignments, it's something that you have to tweak to get the kind of performance you want. Anyways, if we've got in our state and met something in this field, feedback on work, then that means that an evaluation has happened and it's not gone well, there's been feedback, and it's come back for more. \n",
    "\n",
    "So then we add to the system message, previously you thought you completed the assignment, but your reply was rejected because the success criteria was not met. Here is the feedback on why this was rejected, and then we give the feedback. And then with this feedback, please continue the assignment, ensuring you meet the success criteria. So there it is spelled out in detail. Okay, and then I've got here some slightly hokey code that looks to see whether there's already a system message inside there, and if there already is, then it just replaces whatever system message is there with this system message. If it doesn't find a system message, then it creates a new one and puts it up the front. So this is just to make sure that we handle those various scenarios. And this is a little bit hokey because Langerup will have already perhaps built a system message, so we have to be a bit careful about this. It might be worth doing some testing to make sure that it works for different models. Okay, so at the end of all that, we then call worker llm with tools, and we invoke the messages. We get back a response, and that is what we then return. So there we have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The worker node\n",
    "\n",
    "def worker(state: State) -> Dict[str, Any]:\n",
    "    system_message = f\"\"\"You are a helpful assistant that can use tools to complete tasks.\n",
    "You keep working on a task until either you have a question or clarification for the user, or the success criteria is met.\n",
    "This is the success criteria:\n",
    "{state['success_criteria']}\n",
    "You should reply either with a question for the user about this assignment, or with your final response.\n",
    "If you have a question for the user, you need to reply by clearly stating your question. An example might be:\n",
    "\n",
    "Question: please clarify whether you want a summary or a detailed answer\n",
    "\n",
    "If you've finished, reply with the final answer, and don't ask a question; simply reply with the answer.\n",
    "\"\"\"\n",
    "    \n",
    "    if state.get(\"feedback_on_work\"):\n",
    "        system_message += f\"\"\"\n",
    "Previously you thought you completed the assignment, but your reply was rejected because the success criteria was not met.\n",
    "Here is the feedback on why this was rejected:\n",
    "{state['feedback_on_work']}\n",
    "With this feedback, please continue the assignment, ensuring that you meet the success criteria or have a question for the user.\"\"\"\n",
    "    \n",
    "    # Add in the system message\n",
    "\n",
    "    found_system_message = False\n",
    "    messages = state[\"messages\"]\n",
    "    for message in messages:\n",
    "        if isinstance(message, SystemMessage):\n",
    "            message.content = system_message\n",
    "            found_system_message = True\n",
    "    \n",
    "    if not found_system_message:\n",
    "        messages = [SystemMessage(content=system_message)] + messages\n",
    "    \n",
    "    # Invoke the LLM with tools\n",
    "    response = worker_llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "worker_router \n",
    "\n",
    "The worker router then, it's a Python function that we will use in our edge, in our conditional edge, to decide which way to route control. And it's very simple, it's going to take the most recent message, it's going to see if it's a tool call, if so it returns tools, if not evaluator. And that is to say that when our worker, our assistant, has come up with an answer, if it's not involving a tool call, then it needs to be evaluated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_router(state: State) -> str:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"evaluator\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format_conversation\n",
    "\n",
    "And that's what we get to right now. So for the evaluation, we have, first of all, this little utility function format conversation, that's just used to take in, I just wrote that to transform a list of these message objects into something which says like user assistant, user assistant, into a nice little text summary. And you'll see right now, as we come on to look at the evaluator code, run these two cells, and we will talk about the evaluator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(messages: List[Any]) -> str:\n",
    "    conversation = \"Conversation history:\\n\\n\"\n",
    "    for message in messages:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            conversation += f\"User: {message.content}\\n\"\n",
    "        elif isinstance(message, AIMessage):\n",
    "            text = message.content or \"[Tools use]\"\n",
    "            conversation += f\"Assistant: {text}\\n\"\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is the `evaluator`, this function right here, it is a node, it takes a state and it returns a state, and it's meant to represent the LLM, which is going to be assessing our assistant, our worker, and deciding if it's ready to return to the user, or it needs to go back for more. And so it all comes down to some prompting, and let me take you through it. Remember that we're using structured outputs that will require that the model returns a particular type of object. So first we take the most recent response, which is of course the assistant's attempt. We take that out of the state object, the messages collection. So then we come up with this system prompt. You are an evaluator. Determine if a task has been completed successfully by an assistant. Assess the last response based on the criteria. Respond with your feedback and a decision on whether the success criteria is met, and whether more input is needed from the user. And then the user message, this is going to be a bit more detailed, you're evaluating a conversation between the user and the assistant. You decide what action to take based on the last response from the assistant. The entire conversation with the assistant, along with the user's original request and all replies, is here. And this is going to use this little utility thing, which is just going to say like human, sorry not human, it's going to say user assistant, user assistant, with the whole conversation so far. So it's going to look very simple in language. The success criteria for this assignment is, and then I'm plucking out of the state, this success criteria. And as I hope you guessed, this is something that's going to be set right at the very beginning when we invoke the graph. So that's going to be passed in by the user and be maintained throughout the graph. So we can pluck it out and just insert it in the user prompt for this evaluator right here. And then I say the final response from the assistant that you were evaluating is this last response. And of course that will already be included in here, but I just want to be crystal clear so that the evaluator understands that it's not assessing the whole conversation, it's just assessing this response right here, which is what's going back to the user. Respond with your feedback and decide if the success criteria is met. Also if more user input is required, either because the assistant has a question, needs clarification, or seems to be stuck and unable to answer without help. \n",
    "\n",
    "\n",
    "So you may remember that we already put some of this in the definition of the structured outputs of the response right up at the top. Let me show you that. Right here in the evaluator output, we already gave a little description of user input needed right here. \n",
    "\n",
    "```py\n",
    "# First define a structured output\n",
    "class EvaluatorOutput(BaseModel):\n",
    "    feedback: str = Field(description=\"Feedback on the assistant's response\")\n",
    "    success_criteria_met: bool = Field(description=\"Whether the success criteria have been met\")\n",
    "    user_input_needed: bool = Field(description=\"True if more input is needed from the user, or clarifications, or the assistant is stuck\")\n",
    "```\n",
    "\n",
    "And so you may wonder why I'm repeating myself here. And the answer is because there's never a harm in being repetitious with prompting. Be clear, be instructive, repeat yourself. These are good things to do in slightly different ways because it biases the model to doing what we want it to do. \n",
    "\n",
    "`if state[\"feedback_on_work\"]`\n",
    "\n",
    "Okay, and then finally, and here I put `if` we've already got some feedback in the `state object`, that means that the evaluator was already called in this very loop and has already provided feedback in the past. And so I add in, also note that in a prior attempt from the assistant, you provided this feedback. If you're seeing the assistant repeating the same mistakes, then consider responding that user input is required. Now you might think this is very clever. \n",
    "\n",
    "How did you come up with that? And why that? And when do I use this kind of thing? And look, the answer is there's no magic here. That is there because I was testing this and it kept messing up by the evaluators sending back the same problem again and again. And so this is the kind of thing that is trial and error. You experiment, you try, when something goes wrong, you change the prompt and you try some more. There's no magic and no clever rules to this. This won't always apply, but it applies here. And if you use a different model or slightly different tasks, you may find that you need to tweak this or use something different. And that is what AI engineering is all about. And that is what prompting is about. And so yeah, the answer is it's research and development. Okay, we'll finish this off in just a second.\n",
    "\n",
    "`evaluator_messages`\n",
    "\n",
    "We'll finish this off in just a sec. So we then put the system message and the user message, which is called a human message object, together into one list called evaluator messages. And it's confusing, because we're using this concept of system message and user message in order to talk to an evaluator. And this isn't actually a human message. It's actually something where it's a user prompt, but it's a message that we have manufactured. But that's just really how you go about building system and user prompts using LangChain's constructs. This is a LangChain construct within LangRuf. And so, you know, this is still achieving the same thing. All right. \n",
    "\n",
    "`eval_result`\n",
    "\n",
    "Now, we then take our LLM, which is the evaluator LLM with structured outputs. We call invoke with these messages. And what comes back will be an instance of that class evaluator output. It is that pedantic object filled up. And behind the scenes, what's going on is that it's been asked to provide JSON, and that JSON has come back, and that JSON has been parsed into this object. That's how it did it. \n",
    "\n",
    "`new_state`\n",
    "\n",
    "And so we're then going to create a new state, because we're meant to return a new state. And in that state, we're going to respond. We're going to add to the messages, because remember, messages has the reducer, so whatever we reply here gets concatenated, accumulated with the existing messages. We're going to shove in there that the assistant is replying, evaluates the feedback on this answer, and something in there. Then we're going to give some feedback. We're going to... And so what we're doing here is we're taking the feedback from the pedantic object, and we're putting that in the state. We're taking the success criteria from the pedantic object, putting it in the state. Taking user input needed, taking it from the pedantic, the structured outputs that came back, put it in the state, and return the new state. Hopefully you followed all that. If not, you will when it comes together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(state: State) -> State:\n",
    "    last_response = state[\"messages\"][-1].content\n",
    "\n",
    "    system_message = f\"\"\"You are an evaluator that determines if a task has been completed successfully by an Assistant.\n",
    "Assess the Assistant's last response based on the given criteria. Respond with your feedback, and with your decision on whether the success criteria has been met,\n",
    "and whether more input is needed from the user.\"\"\"\n",
    "    \n",
    "    user_message = f\"\"\"You are evaluating a conversation between the User and Assistant. You decide what action to take based on the last response from the Assistant.\n",
    "\n",
    "The entire conversation with the assistant, with the user's original request and all replies, is:\n",
    "{format_conversation(state['messages'])}\n",
    "\n",
    "The success criteria for this assignment is:\n",
    "{state['success_criteria']}\n",
    "\n",
    "And the final response from the Assistant that you are evaluating is:\n",
    "{last_response}\n",
    "\n",
    "Respond with your feedback, and decide if the success criteria is met by this response.\n",
    "Also, decide if more user input is required, either because the assistant has a question, needs clarification, or seems to be stuck and unable to answer without help.\n",
    "\"\"\"\n",
    "    if state[\"feedback_on_work\"]:\n",
    "        user_message += f\"Also, note that in a prior attempt from the Assistant, you provided this feedback: {state['feedback_on_work']}\\n\"\n",
    "        user_message += \"If you're seeing the Assistant repeating the same mistakes, then consider responding that user input is required.\"\n",
    "    \n",
    "    evaluator_messages = [SystemMessage(content=system_message), HumanMessage(content=user_message)]\n",
    "\n",
    "    eval_result = evaluator_llm_with_output.invoke(evaluator_messages)\n",
    "    new_state = {\n",
    "        \"messages\": [AIMessage(content=f\"Evaluator Feedback on this answer: {eval_result.feedback}\")],\n",
    "        \"feedback_on_work\": eval_result.feedback,\n",
    "        \"success_criteria_met\": eval_result.success_criteria_met,\n",
    "        \"user_input_needed\": eval_result.user_input_needed\n",
    "    }\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we've got another of these router functions, root based on evaluation. If the success criteria is met, or if user input is needed, then end the super step. The super step is done, it's got to control, it's got to pass back to the user, in either of these two extremes. Either we've done great, or we've done horribly. Either of those extremes, we need the user to get involved again. But, but, if we didn't meet the success criteria, and we don't need help from the user, then it needs to be passed back to the worker. We need to cycle back, the worker's got to try again, and improve on this, given this feedback. That is the whole idea, that is the workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_based_on_evaluation(state: State) -> str:\n",
    "    if state[\"success_criteria_met\"] or state[\"user_input_needed\"]:\n",
    "        return \"END\"\n",
    "    else:\n",
    "        return \"worker\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we come to our graph. It's very simple, and all of this is pretty simple. I've been making a bit of a meal out of it, explaining this step by step, and talking about prompting, but it's not that hard. If you go through it yourself, you'll see what I mean. So, this is our graph. We're going to add our worker node, we're going to add our tools node, and our evaluator node that we just built. Now, some edges. We're going to have a conditional edge. For the worker, we're going to use the router that we wrote, the worker router. If it returns tools, we're going to go with the node tools. If it returns evaluator, we'll pick the node evaluator. We're going to add an edge that goes from tools back to worker again. Remember this one? When the tools finishes, it's got to route back to the worker. That's kind of hokey that you have to do that. You think it would be done automatically for you, but you have to be clear. And then, another conditional edge from the evaluator. Based on its evaluation, if it wants to go to the worker, we put it back to the worker. If we're done, we're done. And then, we also add a start edge to bring us, first of all, to the worker. Let's run that and look at a picture of this. There it is. There it is. We have ourselves a true agentic workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Graph Builder with State\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"worker\", worker)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "graph_builder.add_node(\"evaluator\", evaluator)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_conditional_edges(\"worker\", worker_router, {\"tools\": \"tools\", \"evaluator\": \"evaluator\"})\n",
    "graph_builder.add_edge(\"tools\", \"worker\")\n",
    "graph_builder.add_conditional_edges(\"evaluator\", route_based_on_evaluation, {\"worker\": \"worker\", \"END\": END})\n",
    "graph_builder.add_edge(START, \"worker\")\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The start goes to a worker. That, optionally, can run tools, which has to come back. It's a thick line. Optionally, dotted line, an optional edge. A conditional edge is the word, sorry. It will run tool, and then it will definitely come back, an edge. And then, when it's done, this is shown as a conditional because it's only if it hasn't decided to run a tool. Then, it will come this way. And the evaluator chooses either to end. In two situations, either success criteria is met or user feedback is required. And, if not, it comes back to the worker. End or back to the worker. So, this diagram, hopefully, has everything coming together for you. And now, scroll back up in the lab and just take a look through those nodes again. And, at this point, it should be like, ah, got it, got it, got it, got it. And, hopefully, you'll see that, although I made a bit of a meal out of it, it is actually quite quick to build something like this. And you see one of Anthropic's agentic patterns loud and clear here. Although, it's a little bit more than one of their workflows because this has, like, an infinite loop in it. It can keep going, and there's a lot of optionality here. So, this is a true agent pattern because, in theory, this could just keep running and running. And it has some sort of agency autonomy over what it does. And, surely, you're now thinking, well, I want to see this thing. And that is what we will do next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next comes the gradio Callback to kick off a super-step\n",
    "\n",
    "Okay, so first of all, we have a callback, a Gradio callback that Gradio will call into when we press the button, which will kick off a super step. So I've got this little thing here, which is a cheeky thing that comes up with a thread ID, a random, big, unique number to make sure that each time that we bring up the interface, it's not going to continue the conversation from last time, or things can get quite hectic. So this means that every Gradio session is going to have a separate thread. This also means that we could actually run this and have multiple different people come on and use the interface and each have different conversations, which is really cool. \n",
    "\n",
    "So it's not just going to be for us, this sidekick. All right, so then the process message async, this coroutine, as I should call it, this function. So first of all, it creates a config based on the thread code that we're going to give it. Then it sets the initial state. So first of all, the initial state is going to take the user's message, the message that comes in. That's going to be the initial message. It's going to take the success criteria as the success criteria from the user that the user is going to tell us what they see as being needed for success. And this state will go through all of the graph and it will be used by the evaluator. The feedback on work, this is what the evaluator can set. Success criteria met, this is false to start with, user input needed, this is false to start with, and then here it is. Here we do it. We take a graph, we call a invoke, we pass in the state, the initial state, we pass in our config, and we wait for that to come back. With what comes back, we then need to build the stuff that's going to appear in the Gradio user interface. We package up the user's message. We package up the last two things that will have come back. The second last one will be the assistance reply, and the very last one is going to actually be the evaluator's evaluation, and we'll show them both coming so that they both appear in the UI. It'll be nice to see the evaluation as well, but you could remove this if you don't want to see the evaluation in the UI. And then we package that together in the history with all of this, and that's what we reply. There is a little bit of a confusion here, if you're following all this, that we're sort of combining Gradio's history with the history that's being stored in Landgraf's memory. It would probably be more elegant here to actually unpack the full history from the state, but that would start to get sort of messy, so I thought, keep it simple. This is fine. And then I've also got a little callback called reset that just resets the interface, so we'll just run this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_thread_id() -> str:\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "\n",
    "async def process_message(message, success_criteria, history, thread):\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": thread}}\n",
    "\n",
    "    state = {\n",
    "        \"messages\": message,\n",
    "        \"success_criteria\": success_criteria,\n",
    "        \"feedback_on_work\": None,\n",
    "        \"success_criteria_met\": False,\n",
    "        \"user_input_needed\": False\n",
    "    }\n",
    "    result = await graph.ainvoke(state, config=config)\n",
    "    user = {\"role\": \"user\", \"content\": message}\n",
    "    reply = {\"role\": \"assistant\", \"content\": result[\"messages\"][-2].content}\n",
    "    feedback = {\"role\": \"assistant\", \"content\": result[\"messages\"][-1].content}\n",
    "    return history + [user, reply, feedback]\n",
    "\n",
    "async def reset():\n",
    "    return \"\", \"\", None, make_thread_id()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now launch our Sidekick UI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-29 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alex/.local/share/uv/python/cpython-3.12.11-macos-x86_64-none/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/alex/Desktop/00_projects/AI_agents/my_agents/agents_env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Users/alex/.local/share/uv/python/cpython-3.12.11-macos-x86_64-none/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/alex/Desktop/00_projects/AI_agents/my_agents/agents_env/lib/python3.12/site-packages/uvicorn/server.py\", line 66, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alex/Desktop/00_projects/AI_agents/my_agents/agents_env/lib/python3.12/site-packages/nest_asyncio.py\", line 26, in run\n",
      "    loop = asyncio.get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alex/Desktop/00_projects/AI_agents/my_agents/agents_env/lib/python3.12/site-packages/nest_asyncio.py\", line 40, in _get_event_loop\n",
      "    loop = events.get_event_loop_policy().get_event_loop()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alex/Desktop/00_projects/AI_agents/my_agents/agents_env/lib/python3.12/site-packages/nest_asyncio.py\", line 67, in get_event_loop\n",
      "    _patch_loop(loop)\n",
      "  File \"/Users/alex/Desktop/00_projects/AI_agents/my_agents/agents_env/lib/python3.12/site-packages/nest_asyncio.py\", line 193, in _patch_loop\n",
      "    raise ValueError('Can\\'t patch loop of type %s' % type(loop))\n",
      "ValueError: Can't patch loop of type <class 'uvloop.Loop'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[32m     27\u001b[39m     asyncio.set_event_loop(asyncio.new_event_loop())\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mdemo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/00_projects/AI_agents/my_agents/agents_env/lib/python3.12/site-packages/gradio/blocks.py:2739\u001b[39m, in \u001b[36mBlocks.launch\u001b[39m\u001b[34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001b[39m\n\u001b[32m   2731\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m http_server\n\u001b[32m   2734\u001b[39m     (\n\u001b[32m   2735\u001b[39m         server_name,\n\u001b[32m   2736\u001b[39m         server_port,\n\u001b[32m   2737\u001b[39m         local_url,\n\u001b[32m   2738\u001b[39m         server,\n\u001b[32m-> \u001b[39m\u001b[32m2739\u001b[39m     ) = \u001b[43mhttp_server\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2742\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2743\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl_keyfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2744\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl_certfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2745\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mssl_keyfile_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2746\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2747\u001b[39m \u001b[38;5;28mself\u001b[39m.server_name = server_name\n\u001b[32m   2748\u001b[39m \u001b[38;5;28mself\u001b[39m.local_url = local_url\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/00_projects/AI_agents/my_agents/agents_env/lib/python3.12/site-packages/gradio/http_server.py:151\u001b[39m, in \u001b[36mstart_server\u001b[39m\u001b[34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[39m\n\u001b[32m    141\u001b[39m         reloader = SourceFileReloader(\n\u001b[32m    142\u001b[39m             app=app,\n\u001b[32m    143\u001b[39m             watch_dirs=GRADIO_WATCH_DIRS,\n\u001b[32m   (...)\u001b[39m\u001b[32m    148\u001b[39m             watch_module=sys.modules[\u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    149\u001b[39m         )\n\u001b[32m    150\u001b[39m     server = Server(config=config, reloader=reloader)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[43mserver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_in_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, ServerFailedToStartError):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/00_projects/AI_agents/my_agents/agents_env/lib/python3.12/site-packages/gradio/http_server.py:58\u001b[39m, in \u001b[36mServer.run_in_thread\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m start = time.time()\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.started:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m time.time() - start > \u001b[32m5\u001b[39m:\n\u001b[32m     60\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ServerFailedToStartError(\n\u001b[32m     61\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mServer failed to start. Please check that the port is available.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     62\u001b[39m         )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.themes.Default(primary_hue=\"emerald\")) as demo:\n",
    "    gr.Markdown(\"## Sidekick Personal Co-worker\")\n",
    "    thread = gr.State(make_thread_id())\n",
    "    \n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(label=\"Sidekick\", height=300, type=\"messages\")\n",
    "    with gr.Group():\n",
    "        with gr.Row():\n",
    "            message = gr.Textbox(show_label=False, placeholder=\"Your request to your sidekick\")\n",
    "        with gr.Row():\n",
    "            success_criteria = gr.Textbox(show_label=False, placeholder=\"What are your success critiera?\")\n",
    "    with gr.Row():\n",
    "        reset_button = gr.Button(\"Reset\", variant=\"stop\")\n",
    "        go_button = gr.Button(\"Go!\", variant=\"primary\")\n",
    "    message.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    success_criteria.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    go_button.click(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    reset_button.click(reset, [], [message, success_criteria, chatbot, thread])\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "try:\n",
    "    asyncio.get_event_loop()\n",
    "except RuntimeError:\n",
    "    asyncio.set_event_loop(asyncio.new_event_loop())\n",
    "\n",
    "    \n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/thanks.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00cc00;\">Congratulations on making the first version of Sidekick!</h2>\n",
    "            <span style=\"color:#00cc00;\">This is a pretty epic moment in the course. You've made the start of something very powerful. And you've upskilled on an impressive Agent framework in LangGraph. Maybe like me you're being converted from a LangGraph skeptic to a LangGraph fan..<br/><br/>My editor would kill me if I didn't mention again: if you're able to rate the course on Udemy, I'd be so very grateful: it's the main way that Udemy decides whether to show the course to others and it makes a massive difference.<br/><br/>And another reminder that I love <a href=\"https://www.linkedin.com/in/eddonner/\">connecting on LinkedIn</a> if you haven't yet! If you wanted to post about your progress on the course, please tag me and I'll weigh in to increase your exposure.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
