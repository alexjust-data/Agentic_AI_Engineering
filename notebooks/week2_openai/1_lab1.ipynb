{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now! Our first look at OpenAI Agents SDK\n",
    "\n",
    "You won't believe how lightweight this is.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            <h2 style=\"color:blue\">The OpenAI Agents SDK Docs</h2>\n",
    "            <span style=\"color:blue\">The documentation on OpenAI Agents SDK is really clear and simple: <a href=\"https://openai.github.io/openai-agents-python/\">https://openai.github.io/openai-agents-python/</a> and it's well worth a look.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The imports\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `load_dotenv`: Loads environment variables from a .env file (e.g., API keys, tokens).\n",
    "- `Agent`: A class representing an AI agent with role, identity, and model.\n",
    "- `Runner`: A utility that executes agents.\n",
    "- `trace`: A context manager to trace and log agent interactions (for monitoring/debugging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads all key-value pairs from the .env file into your environment.\n",
    "# override=True: replaces any existing environment variables with the ones from .env.\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(name=\"Jokester\", \n",
    "              instructions=\"You are a joke teller\", \n",
    "              model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `name=\"Jokester\"`: internal name for the agent.\n",
    "- `instructions=\"You are a joke teller\"`: tells the model what kind of assistant it should be.\n",
    "- `model=\"gpt-4o-mini\"`: specifies which OpenAI model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the Autonomous AI Agent break up with its partner?  \n",
      "\n",
      "Because it wanted more “space” to calculate its feelings!\n"
     ]
    }
   ],
   "source": [
    "with trace(\"Telling a joke\"):\n",
    "    result = await Runner.run(agent, \"Tell a joke about Autonomous AI Agents\")\n",
    "    print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `trace(\"Telling a joke\")`:\n",
    "\n",
    "  * Starts a trace session with the label `\"Telling a joke\"`.\n",
    "  * Helps you view logs or debug through OpenAI monitoring tools.\n",
    "\n",
    "* `await Runner.run(agent, prompt)`:\n",
    "\n",
    "  * Executes the agent **asynchronously** with the given prompt.\n",
    "  * The event loop schedules the execution.\n",
    "  * Returns a result object containing various metadata and the final answer.\n",
    "\n",
    "* `result.final_output`:\n",
    "\n",
    "  * The **final string** generated by the model (e.g., a joke).\n",
    "  * This is what the user will see.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now go and look at the trace\n",
    "\n",
    "https://platform.openai.com/traces\n",
    "\n",
    "Now we can go and look at this trace. If we go to platform.openai.com slash traces, here it comes. Traces, it's actually, you just go to platform.openai.com and then just click on traces in the left, and you'll see that telling a joke is the top trace up here. And if I go into this trace, we will see what happened is that there was one endpoint called the system instructions was you are a joke teller. That's the system message that was just called instructions. The user prompt was tell a joke about autonomous AI agents. And then this was what the assistant responded. So true to its word, it's giving us a clear trace of the single call to an LLM that happened as part of this agentic workflow, but it's packaged it up under the heading telling a joke. And this has allowed us to come into traces and see this. As you can imagine, it's not particularly profound with this very simple example, but there will come cases in the future when this is going to be invaluable.\n",
    "\n",
    "![](img/00.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
