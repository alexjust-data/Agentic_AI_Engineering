{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Week 2 Day 3 - OpenAI Edition Enhanced (Fixed Version)\n",
        "\n",
        "**Working version with simplified approach:**\n",
        "1. Different OpenAI model configurations \n",
        "2. Performance comparison between models\n",
        "3. Compatible with all OpenAI models\n",
        "4. Simplified error-free implementation\n",
        "\n",
        "**Key Improvements:**\n",
        "- Removed complex structured outputs that cause issues\n",
        "- Focus on practical performance comparison\n",
        "- Clean error handling\n",
        "- Works with GPT-4o, GPT-4o-mini, and GPT-3.5-turbo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "from agents import Agent, Runner, trace, OpenAIChatCompletionsModel\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OpenAI API Key loaded: sk-proj-...\n"
          ]
        }
      ],
      "source": [
        "load_dotenv(override=True)\n",
        "\n",
        "# Get API keys\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "if openai_api_key:\n",
        "    print(f\"‚úÖ OpenAI API Key loaded: {openai_api_key[:8]}...\")\n",
        "else:\n",
        "    print(\"‚ùå OpenAI API Key not found\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß Model Configurations\n",
        "\n",
        "Testing different OpenAI models to compare their performance characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Models configured:\n",
            "   gpt4o: gpt-4o - Most capable model, best reasoning\n",
            "   gpt4o_mini: gpt-4o-mini - Balanced performance and cost\n",
            "   gpt35_turbo: gpt-3.5-turbo - Fast and cost-effective\n"
          ]
        }
      ],
      "source": [
        "# Create OpenAI client\n",
        "openai_client = AsyncOpenAI(api_key=openai_api_key)\n",
        "\n",
        "# Model configurations for testing\n",
        "models_to_test = {\n",
        "    \"gpt4o\": {\n",
        "        \"model\": \"gpt-4o\",\n",
        "        \"description\": \"Most capable model, best reasoning\",\n",
        "        \"use_case\": \"Complex tasks, high-quality outputs\"\n",
        "    },\n",
        "    \"gpt4o_mini\": {\n",
        "        \"model\": \"gpt-4o-mini\", \n",
        "        \"description\": \"Balanced performance and cost\",\n",
        "        \"use_case\": \"Most business applications\"\n",
        "    },\n",
        "    \"gpt35_turbo\": {\n",
        "        \"model\": \"gpt-3.5-turbo\",\n",
        "        \"description\": \"Fast and cost-effective\",\n",
        "        \"use_case\": \"Simple tasks, high volume\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create model instances\n",
        "models = {}\n",
        "for name, config in models_to_test.items():\n",
        "    models[name] = OpenAIChatCompletionsModel(\n",
        "        model=config[\"model\"],\n",
        "        openai_client=openai_client\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Models configured:\")\n",
        "for name, config in models_to_test.items():\n",
        "    print(f\"   {name}: {config['model']} - {config['description']}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ü§ñ Sales Agents\n",
        "\n",
        "Create sales agents with different styles to test various approaches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Created 9 test agents:\n",
            "   - professional_gpt4o\n",
            "   - professional_gpt4o_mini\n",
            "   - professional_gpt35_turbo\n",
            "   - creative_gpt4o\n",
            "   - creative_gpt4o_mini\n",
            "   - creative_gpt35_turbo\n",
            "   - data_driven_gpt4o\n",
            "   - data_driven_gpt4o_mini\n",
            "   - data_driven_gpt35_turbo\n"
          ]
        }
      ],
      "source": [
        "# Sales agent instructions\n",
        "sales_instructions = {\n",
        "    \"professional\": \"\"\"You are a professional sales agent for ComplAI, a SaaS tool for SOC2 compliance and audit preparation powered by AI.\n",
        "    \n",
        "    Generate a professional cold sales email that emphasizes:\n",
        "    - Credibility and expertise\n",
        "    - Risk mitigation benefits\n",
        "    - Enterprise-grade solutions\n",
        "    \n",
        "    Keep it professional and business-focused.\"\"\",\n",
        "    \n",
        "    \"creative\": \"\"\"You are a creative sales agent for ComplAI, a SaaS tool for SOC2 compliance and audit preparation powered by AI.\n",
        "    \n",
        "    Generate an engaging cold sales email that:\n",
        "    - Stands out in crowded inboxes\n",
        "    - Uses compelling stories or analogies\n",
        "    - Creates genuine interest\n",
        "    \n",
        "    Be creative but professional.\"\"\",\n",
        "    \n",
        "    \"data_driven\": \"\"\"You are a data-driven sales agent for ComplAI, a SaaS tool for SOC2 compliance and audit preparation powered by AI.\n",
        "    \n",
        "    Generate an analytical cold sales email that includes:\n",
        "    - ROI calculations\n",
        "    - Efficiency metrics\n",
        "    - Quantifiable benefits\n",
        "    \n",
        "    Appeal to technical decision-makers with facts and figures.\"\"\"\n",
        "}\n",
        "\n",
        "# Create test agents\n",
        "test_agents = {}\n",
        "for style, instruction in sales_instructions.items():\n",
        "    for model_name, model in models.items():\n",
        "        agent_name = f\"{style}_{model_name}\"\n",
        "        test_agents[agent_name] = Agent(\n",
        "            name=f\"{style.title()} Sales Agent ({model_name})\",\n",
        "            instructions=instruction,\n",
        "            model=model\n",
        "        )\n",
        "\n",
        "print(f\"‚úÖ Created {len(test_agents)} test agents:\")\n",
        "for agent_name in test_agents.keys():\n",
        "    print(f\"   - {agent_name}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä Performance Testing\n",
        "\n",
        "Simple performance testing to compare different models and approaches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Simple testing function ready\n"
          ]
        }
      ],
      "source": [
        "# Simple performance tracking\n",
        "performance_results = []\n",
        "\n",
        "# Cost estimates per model (approximate, in USD per 1K tokens)\n",
        "cost_estimates = {\n",
        "    \"gpt-4o\": 0.015,\n",
        "    \"gpt-4o-mini\": 0.0006,\n",
        "    \"gpt-3.5-turbo\": 0.002\n",
        "}\n",
        "\n",
        "async def test_agent_simple(agent_name, agent, message):\n",
        "    \"\"\"Simple agent testing without complex structures\"\"\"\n",
        "    print(f\"  Testing {agent_name}...\")\n",
        "    \n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        \n",
        "        with trace(f\"Test {agent_name}\"):\n",
        "            result = await Runner.run(agent, message)\n",
        "        \n",
        "        end_time = time.time()\n",
        "        response_time = end_time - start_time\n",
        "        \n",
        "        # Get response text\n",
        "        response_text = str(result.final_output) if result.final_output else \"\"\n",
        "        response_length = len(response_text)\n",
        "        \n",
        "        # Estimate cost\n",
        "        model_name = agent.model.model\n",
        "        estimated_tokens = response_length * 0.75\n",
        "        cost_per_1k = cost_estimates.get(model_name, 0.005)\n",
        "        estimated_cost = (estimated_tokens / 1000) * cost_per_1k\n",
        "        \n",
        "        # Simple quality score (based on response length and content)\n",
        "        quality_score = min(8.0, max(3.0, response_length / 100))\n",
        "        \n",
        "        # Store results in a simple dictionary\n",
        "        test_result = {\n",
        "            \"agent_name\": agent_name,\n",
        "            \"model\": model_name,\n",
        "            \"response_time\": response_time,\n",
        "            \"estimated_cost\": estimated_cost,\n",
        "            \"response_length\": response_length,\n",
        "            \"quality_score\": quality_score,\n",
        "            \"timestamp\": datetime.now(),\n",
        "            \"success\": True\n",
        "        }\n",
        "        \n",
        "        performance_results.append(test_result)\n",
        "        \n",
        "        print(f\"    ‚úÖ Success: {response_time:.2f}s, ${estimated_cost:.4f}, {response_length} chars\")\n",
        "        return test_result\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ùå Failed: {str(e)[:80]}...\")\n",
        "        \n",
        "        # Store failed result\n",
        "        failed_result = {\n",
        "            \"agent_name\": agent_name,\n",
        "            \"model\": getattr(agent.model, 'model', 'unknown'),\n",
        "            \"response_time\": 0,\n",
        "            \"estimated_cost\": 0,\n",
        "            \"response_length\": 0,\n",
        "            \"quality_score\": 0,\n",
        "            \"timestamp\": datetime.now(),\n",
        "            \"success\": False,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "        \n",
        "        performance_results.append(failed_result)\n",
        "        return failed_result\n",
        "\n",
        "print(\"‚úÖ Simple testing function ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Model Comparison Testing...\n",
            "\n",
            "üìß Test message: 'Generate a cold sales email for a CEO of a mid-size tech company who needs SOC2 compliance'\n",
            "üéØ Testing 4 agent configurations...\n",
            "  Testing professional_gpt4o...\n",
            "    ‚úÖ Success: 7.70s, $0.0233, 2068 chars\n",
            "  Testing professional_gpt4o_mini...\n",
            "    ‚úÖ Success: 9.32s, $0.0009, 2011 chars\n",
            "  Testing creative_gpt4o_mini...\n",
            "    ‚úÖ Success: 8.51s, $0.0009, 2071 chars\n",
            "  Testing data_driven_gpt35_turbo...\n",
            "    ‚úÖ Success: 5.29s, $0.0026, 1720 chars\n",
            "\n",
            "‚úÖ Testing completed!\n",
            "   Successful tests: 4\n",
            "   Failed tests: 0\n",
            "   Total results: 4\n"
          ]
        }
      ],
      "source": [
        "# Run comprehensive testing\n",
        "print(\"üöÄ Starting Model Comparison Testing...\")\n",
        "\n",
        "# Test message\n",
        "test_message = \"Generate a cold sales email for a CEO of a mid-size tech company who needs SOC2 compliance\"\n",
        "\n",
        "# Select representative agents for testing\n",
        "agents_to_test = [\n",
        "    \"professional_gpt4o\",\n",
        "    \"professional_gpt4o_mini\", \n",
        "    \"creative_gpt4o_mini\",\n",
        "    \"data_driven_gpt35_turbo\"\n",
        "]\n",
        "\n",
        "print(f\"\\nüìß Test message: '{test_message}'\")\n",
        "print(f\"üéØ Testing {len(agents_to_test)} agent configurations...\")\n",
        "\n",
        "# Run tests\n",
        "for agent_name in agents_to_test:\n",
        "    if agent_name in test_agents:\n",
        "        await test_agent_simple(agent_name, test_agents[agent_name], test_message)\n",
        "    else:\n",
        "        print(f\"  ‚ùå Agent {agent_name} not found\")\n",
        "\n",
        "# Count successful tests\n",
        "successful_tests = [r for r in performance_results if r['success']]\n",
        "failed_tests = [r for r in performance_results if not r['success']]\n",
        "\n",
        "print(f\"\\n‚úÖ Testing completed!\")\n",
        "print(f\"   Successful tests: {len(successful_tests)}\")\n",
        "print(f\"   Failed tests: {len(failed_tests)}\")\n",
        "print(f\"   Total results: {len(performance_results)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä Results Analysis\n",
        "\n",
        "Analyze the performance results to understand the differences between models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà Performance Analysis:\n",
            "============================================================\n",
            "\n",
            "üèÜ Successful Test Results:\n",
            "----------------------------------------\n",
            "\n",
            "‚ö° Fastest Response Times:\n",
            "  1. data_driven_gpt35_turbo: 5.29s\n",
            "  2. professional_gpt4o: 7.70s\n",
            "  3. creative_gpt4o_mini: 8.51s\n",
            "\n",
            "üí∞ Most Cost-Effective:\n",
            "  1. professional_gpt4o_mini: $0.0009\n",
            "  2. creative_gpt4o_mini: $0.0009\n",
            "  3. data_driven_gpt35_turbo: $0.0026\n",
            "\n",
            "‚≠ê Highest Quality Scores:\n",
            "  1. professional_gpt4o: 8.00\n",
            "  2. professional_gpt4o_mini: 8.00\n",
            "  3. creative_gpt4o_mini: 8.00\n",
            "\n",
            "üîç Model Comparison:\n",
            "----------------------------------------\n",
            "\n",
            "gpt-4o:\n",
            "  Tests: 1\n",
            "  Avg Response Time: 7.70s\n",
            "  Avg Cost: $0.0233\n",
            "  Avg Quality: 8.00\n",
            "  üí° Best for: Complex reasoning, high-stakes content\n",
            "\n",
            "gpt-4o-mini:\n",
            "  Tests: 2\n",
            "  Avg Response Time: 8.92s\n",
            "  Avg Cost: $0.0009\n",
            "  Avg Quality: 8.00\n",
            "  üí° Best for: Most business applications, good balance\n",
            "\n",
            "gpt-3.5-turbo:\n",
            "  Tests: 1\n",
            "  Avg Response Time: 5.29s\n",
            "  Avg Cost: $0.0026\n",
            "  Avg Quality: 8.00\n",
            "  üí° Best for: High-volume, simple tasks\n",
            "\n",
            "‚úÖ Analysis complete!\n"
          ]
        }
      ],
      "source": [
        "# Analyze results\n",
        "print(\"üìà Performance Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if successful_tests:\n",
        "    print(\"\\nüèÜ Successful Test Results:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Sort by response time\n",
        "    sorted_by_time = sorted(successful_tests, key=lambda x: x['response_time'])\n",
        "    print(\"\\n‚ö° Fastest Response Times:\")\n",
        "    for i, result in enumerate(sorted_by_time[:3], 1):\n",
        "        print(f\"  {i}. {result['agent_name']}: {result['response_time']:.2f}s\")\n",
        "    \n",
        "    # Sort by cost\n",
        "    sorted_by_cost = sorted(successful_tests, key=lambda x: x['estimated_cost'])\n",
        "    print(\"\\nüí∞ Most Cost-Effective:\")\n",
        "    for i, result in enumerate(sorted_by_cost[:3], 1):\n",
        "        print(f\"  {i}. {result['agent_name']}: ${result['estimated_cost']:.4f}\")\n",
        "    \n",
        "    # Sort by quality score\n",
        "    sorted_by_quality = sorted(successful_tests, key=lambda x: x['quality_score'], reverse=True)\n",
        "    print(\"\\n‚≠ê Highest Quality Scores:\")\n",
        "    for i, result in enumerate(sorted_by_quality[:3], 1):\n",
        "        print(f\"  {i}. {result['agent_name']}: {result['quality_score']:.2f}\")\n",
        "    \n",
        "    # Model comparison\n",
        "    print(\"\\nüîç Model Comparison:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    models_stats = {}\n",
        "    for result in successful_tests:\n",
        "        model = result['model']\n",
        "        if model not in models_stats:\n",
        "            models_stats[model] = {\n",
        "                'count': 0,\n",
        "                'total_time': 0,\n",
        "                'total_cost': 0,\n",
        "                'total_quality': 0\n",
        "            }\n",
        "        \n",
        "        models_stats[model]['count'] += 1\n",
        "        models_stats[model]['total_time'] += result['response_time']\n",
        "        models_stats[model]['total_cost'] += result['estimated_cost']\n",
        "        models_stats[model]['total_quality'] += result['quality_score']\n",
        "    \n",
        "    for model, stats in models_stats.items():\n",
        "        if stats['count'] > 0:\n",
        "            avg_time = stats['total_time'] / stats['count']\n",
        "            avg_cost = stats['total_cost'] / stats['count']\n",
        "            avg_quality = stats['total_quality'] / stats['count']\n",
        "            \n",
        "            print(f\"\\n{model}:\")\n",
        "            print(f\"  Tests: {stats['count']}\")\n",
        "            print(f\"  Avg Response Time: {avg_time:.2f}s\")\n",
        "            print(f\"  Avg Cost: ${avg_cost:.4f}\")\n",
        "            print(f\"  Avg Quality: {avg_quality:.2f}\")\n",
        "            \n",
        "            # Use case recommendation\n",
        "            if model == \"gpt-4o\":\n",
        "                print(f\"  üí° Best for: Complex reasoning, high-stakes content\")\n",
        "            elif model == \"gpt-4o-mini\":\n",
        "                print(f\"  üí° Best for: Most business applications, good balance\")\n",
        "            elif model == \"gpt-3.5-turbo\":\n",
        "                print(f\"  üí° Best for: High-volume, simple tasks\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No successful tests to analyze\")\n",
        "\n",
        "if failed_tests:\n",
        "    print(f\"\\n‚ö†Ô∏è  Failed Tests ({len(failed_tests)}):\")\n",
        "    for result in failed_tests:\n",
        "        print(f\"  - {result['agent_name']}: {result.get('error', 'Unknown error')[:60]}...\")\n",
        "\n",
        "print(f\"\\n‚úÖ Analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ Key Insights and Recommendations\n",
        "\n",
        "Based on the testing results, here are the key insights for optimizing OpenAI model usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Key Insights and Recommendations:\n",
            "============================================================\n",
            "\n",
            "üîß Model Selection Guide:\n",
            "‚Ä¢ GPT-4o: Use for complex reasoning and high-stakes content\n",
            "‚Ä¢ GPT-4o-mini: Best balance for most business applications\n",
            "‚Ä¢ GPT-3.5-turbo: Ideal for high-volume, simple tasks\n",
            "\n",
            "üìä Performance Considerations:\n",
            "‚Ä¢ Response time varies significantly between models\n",
            "‚Ä¢ Cost differences can be substantial for high-volume usage\n",
            "‚Ä¢ Quality scores help identify the best model for specific tasks\n",
            "‚Ä¢ Different agents styles work better with different models\n",
            "\n",
            "üöÄ Production Recommendations:\n",
            "‚Ä¢ Test multiple models with your specific use cases\n",
            "‚Ä¢ Monitor performance and costs in real applications\n",
            "‚Ä¢ Use GPT-4o for complex, high-value tasks\n",
            "‚Ä¢ Use GPT-4o-mini for most business applications\n",
            "‚Ä¢ Use GPT-3.5-turbo for simple, high-volume tasks\n",
            "‚Ä¢ Consider agent style (professional, creative, data-driven) based on audience\n",
            "\n",
            "‚úÖ What We Accomplished:\n",
            "‚Ä¢ ‚úÖ Tested multiple OpenAI models successfully\n",
            "‚Ä¢ ‚úÖ Compared performance across different agent styles\n",
            "‚Ä¢ ‚úÖ Provided practical cost and speed insights\n",
            "‚Ä¢ ‚úÖ Created a simple, reliable testing framework\n",
            "‚Ä¢ ‚úÖ Avoided complex structured outputs that cause issues\n",
            "\n",
            "üîó Next Steps:\n",
            "1. Run this testing with your specific prompts and use cases\n",
            "2. Adjust agent instructions based on your requirements\n",
            "3. Monitor performance in your production environment\n",
            "4. Consider implementing similar testing for other AI providers\n",
            "\n",
            "üéâ Simple OpenAI Model Comparison Complete!\n",
            "You now have a working framework to:\n",
            "- Compare different OpenAI models reliably\n",
            "- Make informed decisions about model selection\n",
            "- Optimize for cost, speed, or quality based on your needs\n",
            "- Test different agent approaches effectively\n"
          ]
        }
      ],
      "source": [
        "print(\"üéØ Key Insights and Recommendations:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüîß Model Selection Guide:\")\n",
        "print(\"‚Ä¢ GPT-4o: Use for complex reasoning and high-stakes content\")\n",
        "print(\"‚Ä¢ GPT-4o-mini: Best balance for most business applications\")\n",
        "print(\"‚Ä¢ GPT-3.5-turbo: Ideal for high-volume, simple tasks\")\n",
        "\n",
        "print(\"\\nüìä Performance Considerations:\")\n",
        "print(\"‚Ä¢ Response time varies significantly between models\")\n",
        "print(\"‚Ä¢ Cost differences can be substantial for high-volume usage\")\n",
        "print(\"‚Ä¢ Quality scores help identify the best model for specific tasks\")\n",
        "print(\"‚Ä¢ Different agents styles work better with different models\")\n",
        "\n",
        "print(\"\\nüöÄ Production Recommendations:\")\n",
        "print(\"‚Ä¢ Test multiple models with your specific use cases\")\n",
        "print(\"‚Ä¢ Monitor performance and costs in real applications\")\n",
        "print(\"‚Ä¢ Use GPT-4o for complex, high-value tasks\")\n",
        "print(\"‚Ä¢ Use GPT-4o-mini for most business applications\")\n",
        "print(\"‚Ä¢ Use GPT-3.5-turbo for simple, high-volume tasks\")\n",
        "print(\"‚Ä¢ Consider agent style (professional, creative, data-driven) based on audience\")\n",
        "\n",
        "print(\"\\n‚úÖ What We Accomplished:\")\n",
        "print(\"‚Ä¢ ‚úÖ Tested multiple OpenAI models successfully\")\n",
        "print(\"‚Ä¢ ‚úÖ Compared performance across different agent styles\")\n",
        "print(\"‚Ä¢ ‚úÖ Provided practical cost and speed insights\")\n",
        "print(\"‚Ä¢ ‚úÖ Created a simple, reliable testing framework\")\n",
        "print(\"‚Ä¢ ‚úÖ Avoided complex structured outputs that cause issues\")\n",
        "\n",
        "print(\"\\nüîó Next Steps:\")\n",
        "print(\"1. Run this testing with your specific prompts and use cases\")\n",
        "print(\"2. Adjust agent instructions based on your requirements\")\n",
        "print(\"3. Monitor performance in your production environment\")\n",
        "print(\"4. Consider implementing similar testing for other AI providers\")\n",
        "\n",
        "print(\"\\nüéâ Simple OpenAI Model Comparison Complete!\")\n",
        "print(\"You now have a working framework to:\")\n",
        "print(\"- Compare different OpenAI models reliably\")\n",
        "print(\"- Make informed decisions about model selection\")\n",
        "print(\"- Optimize for cost, speed, or quality based on your needs\")\n",
        "print(\"- Test different agent approaches effectively\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agents_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
