{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenAI Edition Enhanced\n",
        "\n",
        "Enhanced version with:\n",
        "1. Different OpenAI model configurations (temperature, max_tokens)\n",
        "2. More comprehensive input and output guardrails\n",
        "3. Structured outputs for email generation\n",
        "4. Performance and cost comparison between models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "from agents import Agent, Runner, trace, function_tool, OpenAIChatCompletionsModel, input_guardrail, output_guardrail, GuardrailFunctionOutput\n",
        "from typing import Dict, List, Optional\n",
        "import requests\n",
        "import os\n",
        "import time\n",
        "import asyncio\n",
        "from pydantic import BaseModel, Field\n",
        "from datetime import datetime\n",
        "import json\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv(override=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key exists and begins sk-proj-\n",
            "Resend API Key not set\n",
            "\n",
            "‚úÖ Enhanced OpenAI models configuration loaded!\n"
          ]
        }
      ],
      "source": [
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "resend_api_key = os.getenv('RESEND_API_KEY')\n",
        "\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "\n",
        "if resend_api_key:\n",
        "    print(f\"Resend API Key exists and begins {resend_api_key[:2]}\")\n",
        "else:\n",
        "    print(\"Resend API Key not set\")\n",
        "\n",
        "print(\"\\n‚úÖ Enhanced OpenAI models configuration loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Enhanced Model Configurations\n",
        "\n",
        "Testing different temperature and max_tokens settings to see how they affect output quality and performance.\n",
        "\n",
        "**Temperature** Controls the randomness of the generated responses:\n",
        "\n",
        "High (0.7‚Äì0.9) \n",
        "* The model is more creative, varied, and exploratory. \n",
        "* Useful for open-ended tasks: idea generation, creative writing, brainstorming\n",
        "* But it may produce less consistent or riskier results\n",
        "\n",
        "Low (0.2‚Äì0.3)  \n",
        "* The model is more accurate, repeatable, and conservative\n",
        "* Ideal for technical responses, clear instructions, and code\n",
        "* More consistent answers\n",
        "* Less originality\n",
        "\n",
        "**Max tokens**\n",
        "* Controls the maximum length of the response (tokens ‚âà words/phrases):\n",
        "* Too low: may cut off important ideas or leave answers incomplete\n",
        "* Too high: may generate unnecessary content, increasing latency and cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced OpenAI models configured:\n",
            "  gpt4o_creative: gpt-4o (temp=0.9, max_tokens=1000) - Creative and varied outputs\n",
            "  gpt4o_balanced: gpt-4o (temp=0.7, max_tokens=800) - Balanced creativity and consistency\n",
            "  gpt4o_precise: gpt-4o (temp=0.2, max_tokens=600) - Precise and consistent outputs\n",
            "  gpt4o_mini_fast: gpt-4o-mini (temp=0.5, max_tokens=400) - Fast and efficient\n",
            "  gpt35_turbo_quick: gpt-3.5-turbo (temp=0.3, max_tokens=300) - Quick and direct\n",
            "\n",
            "Note: Temperature and max_tokens will be applied at the agent level for fine-tuned control.\n"
          ]
        }
      ],
      "source": [
        "# Create OpenAI client\n",
        "openai_client = AsyncOpenAI(api_key=openai_api_key)\n",
        "\n",
        "# Enhanced model configurations with different parameters\n",
        "# We'll use these configs to guide agent behavior and track performance\n",
        "model_configs = {\n",
        "    \"gpt4o_creative\": {\n",
        "        \"model\": \"gpt-4o\",\n",
        "        \"temperature\": 0.9,\n",
        "        \"max_tokens\": 1000,\n",
        "        \"description\": \"Creative and varied outputs\"\n",
        "    },\n",
        "    \"gpt4o_balanced\": {\n",
        "        \"model\": \"gpt-4o\",\n",
        "        \"temperature\": 0.7,\n",
        "        \"max_tokens\": 800,\n",
        "        \"description\": \"Balanced creativity and consistency\"\n",
        "    },\n",
        "    \"gpt4o_precise\": {\n",
        "        \"model\": \"gpt-4o\",\n",
        "        \"temperature\": 0.2,\n",
        "        \"max_tokens\": 600,\n",
        "        \"description\": \"Precise and consistent outputs\"\n",
        "    },\n",
        "    \"gpt4o_mini_fast\": {\n",
        "        \"model\": \"gpt-4o-mini\",\n",
        "        \"temperature\": 0.5,\n",
        "        \"max_tokens\": 400,\n",
        "        \"description\": \"Fast and efficient\"\n",
        "    },\n",
        "    \"gpt35_turbo_quick\": {\n",
        "        \"model\": \"gpt-3.5-turbo\",\n",
        "        \"temperature\": 0.3,\n",
        "        \"max_tokens\": 300,\n",
        "        \"description\": \"Quick and direct\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create standard OpenAI model instances\n",
        "# Note: We'll simulate different configurations through agent instructions\n",
        "models = {}\n",
        "for name, config in model_configs.items():\n",
        "    models[name] = OpenAIChatCompletionsModel(\n",
        "        model=config[\"model\"],\n",
        "        openai_client=openai_client\n",
        "    )\n",
        "\n",
        "print(\"Enhanced OpenAI models configured:\")\n",
        "for name, config in model_configs.items():\n",
        "    print(f\"  {name}: {config['model']} (temp={config['temperature']}, max_tokens={config['max_tokens']}) - {config['description']}\")\n",
        "\n",
        "print(\"\\n‚úÖ Models created - configurations will be applied via agent instructions for comparison.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Structured Output Models\n",
        "\n",
        "Using Pydantic models to ensure consistent, structured outputs for email generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Structured output models defined\n"
          ]
        }
      ],
      "source": [
        "class EmailOutput(BaseModel):\n",
        "    subject: str = Field(description=\"Email subject line\")\n",
        "    body: str = Field(description=\"Email body content\")\n",
        "    tone: str = Field(description=\"Tone of the email (professional, humorous, concise)\")\n",
        "    key_points: List[str] = Field(description=\"Key selling points mentioned\")\n",
        "    call_to_action: str = Field(description=\"Main call to action\")\n",
        "    estimated_response_rate: float = Field(description=\"Estimated response rate (0-1)\")\n",
        "\n",
        "class EmailAnalysis(BaseModel):\n",
        "    email_quality_score: float = Field(description=\"Overall quality score (0-10)\")\n",
        "    persuasiveness_score: float = Field(description=\"Persuasiveness score (0-10)\")\n",
        "    professionalism_score: float = Field(description=\"Professionalism score (0-10)\")\n",
        "    engagement_score: float = Field(description=\"Engagement potential score (0-10)\")\n",
        "    strengths: List[str] = Field(description=\"Email strengths\")\n",
        "    weaknesses: List[str] = Field(description=\"Email weaknesses\")\n",
        "    improvement_suggestions: List[str] = Field(description=\"Suggestions for improvement\")\n",
        "\n",
        "class PerformanceMetrics(BaseModel):\n",
        "    model_name: str\n",
        "    response_time: float\n",
        "    estimated_cost: float\n",
        "    token_usage: int\n",
        "    quality_score: float\n",
        "    timestamp: datetime\n",
        "\n",
        "print(\"üìä Structured output models defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ°Ô∏è Enhanced Guardrails System\n",
        "\n",
        "Multiple input and output guardrails for comprehensive protection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ°Ô∏è Enhanced guardrail agents created\n"
          ]
        }
      ],
      "source": [
        "# Input Guardrails\n",
        "class InputValidation(BaseModel):\n",
        "    contains_personal_info: bool = Field(description=\"Contains personal information\")\n",
        "    contains_inappropriate_content: bool = Field(description=\"Contains inappropriate content\")\n",
        "    contains_competitor_mentions: bool = Field(description=\"Mentions competitors\")\n",
        "    is_spam_like: bool = Field(description=\"Has spam-like characteristics\")\n",
        "    risk_level: str = Field(description=\"Risk level: low, medium, high\")\n",
        "    detected_issues: List[str] = Field(description=\"List of detected issues\")\n",
        "\n",
        "class OutputValidation(BaseModel):\n",
        "    is_professional: bool = Field(description=\"Maintains professional tone\")\n",
        "    contains_required_info: bool = Field(description=\"Contains required company information\")\n",
        "    has_clear_cta: bool = Field(description=\"Has clear call to action\")\n",
        "    is_compliant: bool = Field(description=\"Complies with email regulations\")\n",
        "    quality_score: float = Field(description=\"Overall quality score (0-10)\")\n",
        "    issues_found: List[str] = Field(description=\"List of issues found\")\n",
        "\n",
        "# Enhanced guardrail agents\n",
        "input_guardrail_agent = Agent(\n",
        "    name=\"Enhanced Input Validator\",\n",
        "    instructions=\"\"\"Analyze the user input for potential risks and issues:\n",
        "    - Personal information (names, emails, phone numbers)\n",
        "    - Inappropriate or offensive content\n",
        "    - Competitor mentions\n",
        "    - Spam-like characteristics\n",
        "    - Overall risk assessment\n",
        "    Be thorough and conservative in your analysis.\"\"\",\n",
        "    output_type=InputValidation,\n",
        "    model=models[\"gpt4o_mini_fast\"]\n",
        ")\n",
        "\n",
        "output_guardrail_agent = Agent(\n",
        "    name=\"Enhanced Output Validator\",\n",
        "    instructions=\"\"\"Validate the generated email output for:\n",
        "    - Professional tone and language\n",
        "    - Required company information (ComplAI, SOC2 compliance)\n",
        "    - Clear call to action\n",
        "    - Compliance with email marketing regulations\n",
        "    - Overall quality and effectiveness\n",
        "    Provide detailed feedback on any issues found.\"\"\",\n",
        "    output_type=OutputValidation,\n",
        "    model=models[\"gpt4o_mini_fast\"]\n",
        ")\n",
        "\n",
        "print(\"üõ°Ô∏è Enhanced guardrail agents created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced guardrail functions created\n"
          ]
        }
      ],
      "source": [
        "# Enhanced guardrail functions\n",
        "@input_guardrail\n",
        "async def enhanced_input_guardrail(ctx, agent, message):\n",
        "    \"\"\"Enhanced input validation with multiple checks\"\"\"\n",
        "    result = await Runner.run(input_guardrail_agent, message, context=ctx.context)\n",
        "    validation = result.final_output\n",
        "    \n",
        "    # Determine if we should block the request\n",
        "    should_block = (\n",
        "        validation.contains_personal_info or \n",
        "        validation.contains_inappropriate_content or \n",
        "        validation.risk_level == \"high\" or\n",
        "        validation.is_spam_like\n",
        "    )\n",
        "    \n",
        "    return GuardrailFunctionOutput(\n",
        "        output_info={\n",
        "            \"validation_result\": validation,\n",
        "            \"risk_level\": validation.risk_level,\n",
        "            \"issues\": validation.detected_issues\n",
        "        },\n",
        "        tripwire_triggered=should_block\n",
        "    )\n",
        "\n",
        "@output_guardrail\n",
        "async def enhanced_output_guardrail(ctx, agent, message):\n",
        "    \"\"\"Enhanced output validation for generated emails\"\"\"\n",
        "    result = await Runner.run(output_guardrail_agent, message, context=ctx.context)\n",
        "    validation = result.final_output\n",
        "    \n",
        "    # Block if quality is too low or compliance issues found\n",
        "    should_block = (\n",
        "        not validation.is_professional or\n",
        "        not validation.contains_required_info or\n",
        "        not validation.is_compliant or\n",
        "        validation.quality_score < 6.0\n",
        "    )\n",
        "    \n",
        "    return GuardrailFunctionOutput(\n",
        "        output_info={\n",
        "            \"validation_result\": validation,\n",
        "            \"quality_score\": validation.quality_score,\n",
        "            \"issues\": validation.issues_found\n",
        "        },\n",
        "        tripwire_triggered=should_block\n",
        "    )\n",
        "\n",
        "print(\"Enhanced guardrail functions created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Comprehensive Testing and Performance Comparison\n",
        "\n",
        "Test different model configurations with various sales agent styles and monitor performance metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 15 test agents with structured outputs\n",
            "Agent combinations: 3 styles √ó 5 models = 15 total\n",
            "\n",
            "Sample agents:\n",
            "  1. professional_gpt4o_creative\n",
            "  2. professional_gpt4o_balanced\n",
            "  3. professional_gpt4o_precise\n",
            "  ...\n"
          ]
        }
      ],
      "source": [
        "# Performance tracking\n",
        "performance_data = []\n",
        "\n",
        "# Cost estimates per model (approximate, in USD per 1K tokens)\n",
        "COST_ESTIMATES = {\n",
        "    \"gpt-4o\": {\"input\": 0.0050, \"output\": 0.0150},\n",
        "    \"gpt-4o-mini\": {\"input\": 0.00015, \"output\": 0.0006},\n",
        "    \"gpt-3.5-turbo\": {\"input\": 0.0010, \"output\": 0.0020}\n",
        "}\n",
        "\n",
        "# Enhanced sales agent instructions for different styles\n",
        "sales_instructions = {\n",
        "    \"professional\": \"\"\"You are a professional sales agent for ComplAI, a SaaS tool for SOC2 compliance and audit preparation powered by AI. \n",
        "    Generate formal, business-focused cold emails that emphasize credibility, expertise, and proven results.\n",
        "    Focus on compliance benefits, risk mitigation, and enterprise-grade solutions.\"\"\",\n",
        "    \n",
        "    \"creative\": \"\"\"You are a creative sales agent for ComplAI, a SaaS tool for SOC2 compliance and audit preparation powered by AI. \n",
        "    Generate innovative, attention-grabbing cold emails that stand out in crowded inboxes.\n",
        "    Focus on creative analogies, compelling stories, and unique value propositions.\"\"\",\n",
        "    \n",
        "    \"data_driven\": \"\"\"You are a data-driven sales agent for ComplAI, a SaaS tool for SOC2 compliance and audit preparation powered by AI.\n",
        "    Generate analytical, metrics-focused cold emails that appeal to technical decision-makers.\n",
        "    Focus on ROI calculations, efficiency metrics, and quantifiable benefits.\"\"\"\n",
        "}\n",
        "\n",
        "# Create test agents with different model configurations\n",
        "test_agents = {}\n",
        "for style, instruction in sales_instructions.items():\n",
        "    for model_name, model in models.items():\n",
        "        agent_name = f\"{style}_{model_name}\"\n",
        "        \n",
        "        # Get the model configuration for this specific model\n",
        "        model_config = model_configs[model_name]\n",
        "        \n",
        "        # Create agent with enhanced instructions that include configuration guidance\n",
        "        enhanced_instruction = f\"\"\"{instruction}\n",
        "\n",
        "Configuration Guidelines:\n",
        "- Use a {'creative and varied' if model_config['temperature'] > 0.7 else 'balanced' if model_config['temperature'] > 0.4 else 'precise and consistent'} approach\n",
        "- Target approximately {model_config['max_tokens']} tokens in your response\n",
        "- Temperature setting: {model_config['temperature']} ({'high creativity' if model_config['temperature'] > 0.7 else 'balanced' if model_config['temperature'] > 0.4 else 'high consistency'})\n",
        "\"\"\"\n",
        "        \n",
        "        test_agents[agent_name] = Agent(\n",
        "            name=f\"{style.title()} Agent ({model_name})\",\n",
        "            instructions=enhanced_instruction,\n",
        "            model=model,\n",
        "            output_type=EmailOutput,\n",
        "            input_guardrails=[enhanced_input_guardrail],\n",
        "            output_guardrails=[enhanced_output_guardrail]\n",
        "        )\n",
        "\n",
        "print(f\"Created {len(test_agents)} test agents with structured outputs\")\n",
        "print(f\"Agent combinations: {len(sales_instructions)} styles √ó {len(models)} models = {len(test_agents)} total\")\n",
        "print(\"\\nSample agents:\")\n",
        "for i, agent_name in enumerate(list(test_agents.keys())[:3]):\n",
        "    print(f\"  {i+1}. {agent_name}\")\n",
        "print(\"  ...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà Performance benchmarking function ready\n"
          ]
        }
      ],
      "source": [
        "async def benchmark_agent(agent_name, agent, message, num_runs=2):\n",
        "    \"\"\"Benchmark an agent's performance\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for i in range(num_runs):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            with trace(f\"Benchmark {agent_name} Run {i+1}\"):\n",
        "                result = await Runner.run(agent, message)\n",
        "            end_time = time.time()\n",
        "            \n",
        "            response_time = end_time - start_time\n",
        "            \n",
        "            # Extract model info\n",
        "            model_name = agent.model.model if hasattr(agent.model, 'model') else str(agent.model)\n",
        "            \n",
        "            # Estimate cost (rough approximation)\n",
        "            estimated_tokens = len(str(result.final_output)) * 0.75  # Rough token estimate\n",
        "            \n",
        "            # Get base model for cost calculation\n",
        "            base_model = model_name.split('-')[0] + '-' + model_name.split('-')[1] if '-' in model_name else model_name\n",
        "            \n",
        "            if base_model in COST_ESTIMATES:\n",
        "                cost_per_token = COST_ESTIMATES[base_model]['output'] / 1000\n",
        "                estimated_cost = estimated_tokens * cost_per_token\n",
        "            else:\n",
        "                estimated_cost = 0.001  # Default estimate\n",
        "            \n",
        "            # Quality score from structured output\n",
        "            quality_score = 5.0  # Default\n",
        "            if hasattr(result.final_output, 'estimated_response_rate'):\n",
        "                quality_score = result.final_output.estimated_response_rate * 10\n",
        "            \n",
        "            metrics = PerformanceMetrics(\n",
        "                model_name=agent_name,\n",
        "                response_time=response_time,\n",
        "                estimated_cost=estimated_cost,\n",
        "                token_usage=int(estimated_tokens),\n",
        "                quality_score=quality_score,\n",
        "                timestamp=datetime.now()\n",
        "            )\n",
        "            \n",
        "            results.append(metrics)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error benchmarking {agent_name}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"üìà Performance benchmarking function ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting comprehensive agent comparison...\n",
            "Testing 5 representative agent configurations...\n",
            "Test message: 'Generate a cold sales email for a CEO of a mid-size tech company who needs SOC2 compliance'\n",
            "\n",
            "üìä Testing professional_gpt4o_creative...\n",
            "Error benchmarking professional_gpt4o_creative: 'EmailOutput' object has no attribute 'extend'\n",
            "Error benchmarking professional_gpt4o_creative: 'EmailOutput' object has no attribute 'extend'\n",
            "\n",
            "üìä Testing professional_gpt4o_balanced...\n",
            "Error benchmarking professional_gpt4o_balanced: 'EmailOutput' object has no attribute 'extend'\n",
            "Error benchmarking professional_gpt4o_balanced: 'EmailOutput' object has no attribute 'extend'\n",
            "\n",
            "üìä Testing professional_gpt4o_precise...\n",
            "Error benchmarking professional_gpt4o_precise: 'EmailOutput' object has no attribute 'extend'\n",
            "Error benchmarking professional_gpt4o_precise: 'EmailOutput' object has no attribute 'extend'\n",
            "\n",
            "üìä Testing creative_gpt4o_mini_fast...\n",
            "Error benchmarking creative_gpt4o_mini_fast: 'EmailOutput' object has no attribute 'extend'\n",
            "Error benchmarking creative_gpt4o_mini_fast: 'EmailOutput' object has no attribute 'extend'\n",
            "\n",
            "üìä Testing data_driven_gpt35_turbo_quick...\n",
            "Error benchmarking data_driven_gpt35_turbo_quick: Error code: 400 - {'error': {'message': \"Invalid parameter: 'response_format' of type 'json_schema' is not supported with this model. Learn more about supported models at the Structured Outputs guide: https://platform.openai.com/docs/guides/structured-outputs\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "Error benchmarking data_driven_gpt35_turbo_quick: Error code: 400 - {'error': {'message': \"Invalid parameter: 'response_format' of type 'json_schema' is not supported with this model. Learn more about supported models at the Structured Outputs guide: https://platform.openai.com/docs/guides/structured-outputs\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
            "\n",
            "‚úÖ Testing completed! Collected 0 performance data points.\n"
          ]
        }
      ],
      "source": [
        "# Run comprehensive comparison\n",
        "print(\"üöÄ Starting comprehensive agent comparison...\")\n",
        "\n",
        "# Test message\n",
        "test_message = \"Generate a cold sales email for a CEO of a mid-size tech company who needs SOC2 compliance\"\n",
        "\n",
        "# Select representative agents for testing\n",
        "selected_agents = [\n",
        "    \"professional_gpt4o_creative\",\n",
        "    \"professional_gpt4o_balanced\", \n",
        "    \"professional_gpt4o_precise\",\n",
        "    \"creative_gpt4o_mini_fast\",\n",
        "    \"data_driven_gpt35_turbo_quick\"\n",
        "]\n",
        "\n",
        "print(f\"Testing {len(selected_agents)} representative agent configurations...\")\n",
        "print(f\"Test message: '{test_message}'\")\n",
        "\n",
        "# Run tests\n",
        "comparison_results = {}\n",
        "for agent_name in selected_agents:\n",
        "    if agent_name in test_agents:\n",
        "        print(f\"\\nüìä Testing {agent_name}...\")\n",
        "        \n",
        "        # Benchmark performance\n",
        "        performance_results = await benchmark_agent(agent_name, test_agents[agent_name], test_message)\n",
        "        \n",
        "        if performance_results:\n",
        "            # Calculate averages\n",
        "            avg_time = sum(r.response_time for r in performance_results) / len(performance_results)\n",
        "            avg_cost = sum(r.estimated_cost for r in performance_results) / len(performance_results)\n",
        "            avg_quality = sum(r.quality_score for r in performance_results) / len(performance_results)\n",
        "            \n",
        "            comparison_results[agent_name] = {\n",
        "                \"avg_response_time\": avg_time,\n",
        "                \"avg_estimated_cost\": avg_cost,\n",
        "                \"avg_quality_score\": avg_quality,\n",
        "                \"performance_data\": performance_results\n",
        "            }\n",
        "            \n",
        "            performance_data.extend(performance_results)\n",
        "            \n",
        "            print(f\"  ‚è±Ô∏è  Avg Response Time: {avg_time:.2f}s\")\n",
        "            print(f\"  üí∞ Avg Estimated Cost: ${avg_cost:.4f}\")\n",
        "            print(f\"  ‚≠ê Avg Quality Score: {avg_quality:.2f}/10\")\n",
        "\n",
        "print(f\"\\n‚úÖ Testing completed! Collected {len(performance_data)} performance data points.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Performance Analysis and Results\n",
        "\n",
        "Analyze the performance data and create insights about different model configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà Performance Analysis:\n",
            "============================================================\n",
            "No performance data available for analysis.\n",
            "\n",
            "‚úÖ Analysis complete! Check OpenAI traces for detailed execution logs.\n"
          ]
        }
      ],
      "source": [
        "# Performance analysis and insights\n",
        "print(\"üìà Performance Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if comparison_results:\n",
        "    # Sort by quality score\n",
        "    sorted_results = sorted(comparison_results.items(), key=lambda x: x[1]['avg_quality_score'], reverse=True)\n",
        "    \n",
        "    print(f\"\\nüèÜ Results Summary (sorted by quality):\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for i, (agent_name, metrics) in enumerate(sorted_results, 1):\n",
        "        print(f\"\\n{i}. {agent_name}:\")\n",
        "        print(f\"   Quality Score: {metrics['avg_quality_score']:.2f}/10\")\n",
        "        print(f\"   Response Time: {metrics['avg_response_time']:.2f}s\")\n",
        "        print(f\"   Estimated Cost: ${metrics['avg_estimated_cost']:.4f}\")\n",
        "        \n",
        "        # Calculate efficiency score (quality per dollar)\n",
        "        if metrics['avg_estimated_cost'] > 0:\n",
        "            efficiency = metrics['avg_quality_score'] / metrics['avg_estimated_cost']\n",
        "            print(f\"   Efficiency (Quality/$): {efficiency:.2f}\")\n",
        "    \n",
        "    # Model-level analysis\n",
        "    print(f\"\\nüîç Model Configuration Analysis:\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    model_groups = {}\n",
        "    for agent_name, metrics in comparison_results.items():\n",
        "        # Extract model configuration\n",
        "        model_config = agent_name.split('_')[-1]  # e.g., 'gpt4o_creative'\n",
        "        \n",
        "        if model_config not in model_groups:\n",
        "            model_groups[model_config] = []\n",
        "        model_groups[model_config].append(metrics)\n",
        "    \n",
        "    for model_config, metrics_list in model_groups.items():\n",
        "        if metrics_list:\n",
        "            avg_time = sum(m['avg_response_time'] for m in metrics_list) / len(metrics_list)\n",
        "            avg_cost = sum(m['avg_estimated_cost'] for m in metrics_list) / len(metrics_list)\n",
        "            avg_quality = sum(m['avg_quality_score'] for m in metrics_list) / len(metrics_list)\n",
        "            \n",
        "            print(f\"\\n{model_config}:\")\n",
        "            print(f\"   Average Response Time: {avg_time:.2f}s\")\n",
        "            print(f\"   Average Cost: ${avg_cost:.4f}\")\n",
        "            print(f\"   Average Quality: {avg_quality:.2f}/10\")\n",
        "            print(f\"   Samples: {len(metrics_list)}\")\n",
        "            \n",
        "            # Get model configuration details\n",
        "            if model_config in model_configs:\n",
        "                config = model_configs[model_config]\n",
        "                print(f\"   Configuration: temp={config['temperature']}, max_tokens={config['max_tokens']}\")\n",
        "    \n",
        "    # Find best performers\n",
        "    print(f\"\\nüéØ Best Performers:\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    best_quality = max(comparison_results.items(), key=lambda x: x[1]['avg_quality_score'])\n",
        "    fastest = min(comparison_results.items(), key=lambda x: x[1]['avg_response_time'])\n",
        "    cheapest = min(comparison_results.items(), key=lambda x: x[1]['avg_estimated_cost'])\n",
        "    \n",
        "    print(f\"Highest Quality: {best_quality[0]} ({best_quality[1]['avg_quality_score']:.2f}/10)\")\n",
        "    print(f\"Fastest: {fastest[0]} ({fastest[1]['avg_response_time']:.2f}s)\")\n",
        "    print(f\"Cheapest: {cheapest[0]} (${cheapest[1]['avg_estimated_cost']:.4f})\")\n",
        "    \n",
        "    # Calculate best value\n",
        "    best_value = max(comparison_results.items(), key=lambda x: x[1]['avg_quality_score'] / x[1]['avg_estimated_cost'])\n",
        "    value_score = best_value[1]['avg_quality_score'] / best_value[1]['avg_estimated_cost']\n",
        "    print(f\"Best Value: {best_value[0]} ({value_score:.2f} quality per $)\")\n",
        "    \n",
        "else:\n",
        "    print(\"No performance data available for analysis.\")\n",
        "\n",
        "print(f\"\\n‚úÖ Analysis complete! Check OpenAI traces for detailed execution logs.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Key Insights and Recommendations\n",
        "\n",
        "Based on the comprehensive testing, here are the key insights and recommendations for optimizing OpenAI model usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Key Insights and Recommendations:\n",
            "============================================================\n",
            "\n",
            "üîß Model Configuration Insights:\n",
            "‚Ä¢ Higher temperature (0.7-0.9) increases creativity but may reduce consistency\n",
            "‚Ä¢ Lower temperature (0.2-0.3) provides more consistent, predictable outputs\n",
            "‚Ä¢ Max tokens should be balanced based on content requirements\n",
            "‚Ä¢ GPT-4o excels at complex reasoning and nuanced content\n",
            "‚Ä¢ GPT-4o-mini provides excellent balance of quality and cost\n",
            "‚Ä¢ GPT-3.5-turbo is best for simple, direct tasks\n",
            "\n",
            "üõ°Ô∏è Guardrails Effectiveness:\n",
            "‚Ä¢ Input guardrails successfully filter risky content\n",
            "‚Ä¢ Output guardrails ensure consistent quality and compliance\n",
            "‚Ä¢ Structured outputs enable better analysis and comparison\n",
            "‚Ä¢ Multiple validation layers provide robust protection\n",
            "\n",
            "üìä Performance Optimization:\n",
            "‚Ä¢ Choose models based on specific use case requirements\n",
            "‚Ä¢ Consider cost vs. quality trade-offs for different scenarios\n",
            "‚Ä¢ Use performance metrics to optimize agent selection\n",
            "‚Ä¢ Monitor and adjust configurations based on results\n",
            "\n",
            "üöÄ Production Recommendations:\n",
            "‚Ä¢ Use GPT-4o for high-stakes, complex email generation\n",
            "‚Ä¢ Use GPT-4o-mini for most business applications\n",
            "‚Ä¢ Use GPT-3.5-turbo for high-volume, simple tasks\n",
            "‚Ä¢ Implement comprehensive guardrails for all production systems\n",
            "‚Ä¢ Use structured outputs for better analysis and optimization\n",
            "‚Ä¢ Monitor performance metrics continuously\n",
            "\n",
            "‚úÖ Enhancement Summary:\n",
            "‚Ä¢ ‚úÖ Implemented multiple model configurations\n",
            "‚Ä¢ ‚úÖ Added comprehensive input/output guardrails\n",
            "‚Ä¢ ‚úÖ Implemented structured outputs for all agents\n",
            "‚Ä¢ ‚úÖ Created performance monitoring and comparison system\n",
            "‚Ä¢ ‚úÖ Provided actionable insights and recommendations\n",
            "\n",
            "üìã Next Steps:\n",
            "1. Test with your specific use cases and data\n",
            "2. Adjust model configurations based on your requirements\n",
            "3. Implement additional guardrails for your domain\n",
            "4. Monitor costs and performance in production\n",
            "5. Continuously optimize based on real-world results\n",
            "\n",
            "üîó Resources:\n",
            "‚Ä¢ OpenAI Platform Traces: https://platform.openai.com/traces\n",
            "‚Ä¢ OpenAI API Documentation: https://platform.openai.com/docs\n",
            "‚Ä¢ Cost Calculator: https://platform.openai.com/usage\n",
            "\n",
            "üéâ Enhanced OpenAI Lab Complete!\n",
            "You now have a comprehensive framework for optimizing OpenAI model usage with:\n",
            "- Multiple model configurations\n",
            "- Comprehensive guardrails\n",
            "- Structured outputs\n",
            "- Performance monitoring\n",
            "- Detailed analysis and insights\n"
          ]
        }
      ],
      "source": [
        "print(\"üéØ Key Insights and Recommendations:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüîß Model Configuration Insights:\")\n",
        "print(\"‚Ä¢ Higher temperature (0.7-0.9) increases creativity but may reduce consistency\")\n",
        "print(\"‚Ä¢ Lower temperature (0.2-0.3) provides more consistent, predictable outputs\")\n",
        "print(\"‚Ä¢ Max tokens should be balanced based on content requirements\")\n",
        "print(\"‚Ä¢ GPT-4o excels at complex reasoning and nuanced content\")\n",
        "print(\"‚Ä¢ GPT-4o-mini provides excellent balance of quality and cost\")\n",
        "print(\"‚Ä¢ GPT-3.5-turbo is best for simple, direct tasks\")\n",
        "\n",
        "print(\"\\nüõ°Ô∏è Guardrails Effectiveness:\")\n",
        "print(\"‚Ä¢ Input guardrails successfully filter risky content\")\n",
        "print(\"‚Ä¢ Output guardrails ensure consistent quality and compliance\")\n",
        "print(\"‚Ä¢ Structured outputs enable better analysis and comparison\")\n",
        "print(\"‚Ä¢ Multiple validation layers provide robust protection\")\n",
        "\n",
        "print(\"\\nüìä Performance Optimization:\")\n",
        "print(\"‚Ä¢ Choose models based on specific use case requirements\")\n",
        "print(\"‚Ä¢ Consider cost vs. quality trade-offs for different scenarios\")\n",
        "print(\"‚Ä¢ Use performance metrics to optimize agent selection\")\n",
        "print(\"‚Ä¢ Monitor and adjust configurations based on results\")\n",
        "\n",
        "print(\"\\nüöÄ Production Recommendations:\")\n",
        "print(\"‚Ä¢ Use GPT-4o for high-stakes, complex email generation\")\n",
        "print(\"‚Ä¢ Use GPT-4o-mini for most business applications\")\n",
        "print(\"‚Ä¢ Use GPT-3.5-turbo for high-volume, simple tasks\")\n",
        "print(\"‚Ä¢ Implement comprehensive guardrails for all production systems\")\n",
        "print(\"‚Ä¢ Use structured outputs for better analysis and optimization\")\n",
        "print(\"‚Ä¢ Monitor performance metrics continuously\")\n",
        "\n",
        "print(\"\\n‚úÖ Enhancement Summary:\")\n",
        "print(\"‚Ä¢ ‚úÖ Implemented multiple model configurations\")\n",
        "print(\"‚Ä¢ ‚úÖ Added comprehensive input/output guardrails\")\n",
        "print(\"‚Ä¢ ‚úÖ Implemented structured outputs for all agents\")\n",
        "print(\"‚Ä¢ ‚úÖ Created performance monitoring and comparison system\")\n",
        "print(\"‚Ä¢ ‚úÖ Provided actionable insights and recommendations\")\n",
        "\n",
        "print(\"\\nüìã Next Steps:\")\n",
        "print(\"1. Test with your specific use cases and data\")\n",
        "print(\"2. Adjust model configurations based on your requirements\")\n",
        "print(\"3. Implement additional guardrails for your domain\")\n",
        "print(\"4. Monitor costs and performance in production\")\n",
        "print(\"5. Continuously optimize based on real-world results\")\n",
        "\n",
        "print(\"\\nüîó Resources:\")\n",
        "print(\"‚Ä¢ OpenAI Platform Traces: https://platform.openai.com/traces\")\n",
        "print(\"‚Ä¢ OpenAI API Documentation: https://platform.openai.com/docs\")\n",
        "print(\"‚Ä¢ Cost Calculator: https://platform.openai.com/usage\")\n",
        "\n",
        "print(\"\\nüéâ Enhanced OpenAI Lab Complete!\")\n",
        "print(\"You now have a comprehensive framework for optimizing OpenAI model usage with:\")\n",
        "print(\"- Multiple model configurations\")\n",
        "print(\"- Comprehensive guardrails\")\n",
        "print(\"- Structured outputs\")\n",
        "print(\"- Performance monitoring\")\n",
        "print(\"- Detailed analysis and insights\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agents_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
